{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jkitchin/s26-06642/blob/main/dsmles/06-linear-regression/linear-regression.ipynb)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "! pip install -q pycse\nfrom pycse.colab import pdf",
   "metadata": {
    "tags": [
     "skip-execution",
     "remove-cell"
    ]
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 05: Linear Regression\n",
    "\n",
    "Building predictive models with linear regression.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand the linear regression model\n",
    "2. Fit models using scikit-learn\n",
    "3. Interpret regression coefficients\n",
    "4. Evaluate model performance with metrics\n",
    "5. Use train/test splits for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} linear regression, coefficients\n```\n\n\n## The Linear Regression Model: Simple Yet Powerful\n\nLinear regression is often the first modeling tool we reach for, and for good reason. Despite its simplicity, it's:\n\n- **Interpretable**: Coefficients have clear physical meaning\n- **Fast**: Solutions are analytic, no iteration needed\n- **Robust**: Well-understood statistical properties\n- **A baseline**: If linear works, why use something complex?\n\n### The Mathematical Form\n\n$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p + \\epsilon$$\n\nEach coefficient $\\beta_i$ tells you: *\"When $x_i$ increases by 1 unit (holding other features constant), y changes by $\\beta_i$ units.\"*\n\n### When Does Linear Regression Work?\n\nLinear regression assumes the relationship between features and target is **approximately linear**. This works well when:\n- Effects are additive (no strong interactions)\n- Relationships are monotonic (increasing or decreasing)\n- You're working in a limited range where curves look straight\n\n### When It Fails\n\nLinear regression will struggle with:\n- Saturation effects (Michaelis-Menten kinetics, Langmuir isotherms)\n- Exponential relationships (Arrhenius law without log-transform)\n- Strong interactions (catalyst × temperature effects)\n- Threshold effects (phase transitions)\n\nWe'll learn nonlinear methods later. For now, remember: **start with linear, add complexity only when needed.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load reactor experiment dataset\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/jkitchin/s26-06642/main/dsmles/data/linear_regression_data.csv\"\ndf = pd.read_csv(url)\n\nprint(f\"Dataset shape: {df.shape}\")\ndf.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look at the data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "Let's start with a single feature to understand the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear regression: conversion vs temperature\n",
    "X_simple = df[['temperature']].values\n",
    "y = df['conversion'].values\n",
    "\n",
    "# Fit the model\n",
    "model_simple = LinearRegression()\n",
    "model_simple.fit(X_simple, y)\n",
    "\n",
    "print(f\"Intercept: {model_simple.intercept_:.4f}\")\n",
    "print(f\"Coefficient (temperature): {model_simple.coef_[0]:.6f}\")\n",
    "print(f\"R² score: {model_simple.score(X_simple, y):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "What do these numbers tell us?\n\n- **Intercept ≈ -0.15**: The baseline conversion when temperature is 0 K (not physically meaningful—this is extrapolation!)\n- **Coefficient ≈ 0.0005**: For each 1 K increase in temperature, conversion increases by ~0.05%. Or equivalently, a 100 K increase gives ~5% higher conversion.\n- **R² ≈ 0.40**: Temperature alone explains only 40% of the variance in conversion. We're missing important information (the other variables)!\n\nThe low R² is expected—we know conversion depends on pressure, catalyst, and time too. Let's add those features.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the fit\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.scatter(df['temperature'], df['conversion'], alpha=0.6, label='Data')\n",
    "\n",
    "# Regression line\n",
    "temp_range = np.linspace(300, 500, 100).reshape(-1, 1)\n",
    "plt.plot(temp_range, model_simple.predict(temp_range), 'r-', \n",
    "         linewidth=2, label='Linear fit')\n",
    "\n",
    "plt.xlabel('Temperature (K)')\n",
    "plt.ylabel('Conversion')\n",
    "plt.title('Simple Linear Regression: Conversion vs Temperature')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Now let's use all features."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "Now R² jumped to ~0.96! Adding the other three features dramatically improved our predictions. But what do the coefficients tell us?\n\n**Raw coefficient interpretation** (holding other variables constant):\n- Increasing temperature by 1 K → conversion increases by 0.0005 (0.05%)\n- Increasing pressure by 1 atm → conversion increases by 0.05 (5%)\n- Increasing catalyst loading by 1 wt% → conversion increases by 0.08 (8%)\n- Increasing residence time by 1 min → conversion increases by 0.01 (1%)\n\nThe problem: these coefficients aren't directly comparable because the features have different scales. Is pressure \"more important\" than temperature? We can't tell yet—we need standardized coefficients.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple linear regression with all features\n",
    "feature_names = ['temperature', 'pressure', 'catalyst_loading', 'residence_time']\n",
    "X = df[feature_names].values\n",
    "y = df['conversion'].values\n",
    "\n",
    "# Fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "print(f\"Intercept: {model.intercept_:.4f}\")\n",
    "print(\"\\nCoefficients:\")\n",
    "for name, coef in zip(feature_names, model.coef_):\n",
    "    print(f\"  {name}: {coef:.6f}\")\n",
    "\n",
    "print(f\"\\nR² score: {model.score(X, y):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "Now we can rank feature importance fairly:\n\n1. **Pressure** (0.13): Most influential—a 1 std increase in pressure has the largest effect\n2. **Catalyst loading** (0.11): Second most important\n3. **Temperature** (0.09): Moderate effect\n4. **Residence time** (0.07): Smallest effect\n\nThis ranking makes chemical sense:\n- Pressure directly affects concentrations and equilibrium\n- More catalyst means more active sites for reaction\n- Temperature affects rate but may also affect selectivity\n- Time matters but longer isn't always better (side reactions)\n\n**Key insight**: When optimizing your process, focus first on the variables with largest standardized coefficients. A small change in pressure may have more impact than a large change in residence time.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} standardized coefficients, feature importance\n```\n\n\n## Interpreting Coefficients: The Hidden Complexity\n\nCoefficient interpretation seems straightforward, but there are important subtleties:\n\n### The Scale Problem\n\nRaw coefficients depend on feature scales. If temperature is in Kelvin (300-500) and pressure is in bar (1-10), their coefficients aren't directly comparable. A coefficient of 0.001 for temperature might be more important than 0.1 for pressure!\n\n**Solution**: Standardize features first, then coefficients are comparable.\n\n### Standardized Coefficients\n\nAfter standardizing (mean=0, std=1), coefficients answer: \"When this feature increases by 1 standard deviation, how many standard deviations does the target change?\"\n\nThis lets you rank feature importance fairly."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardized coefficients for comparison\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "model_scaled = LinearRegression()\n",
    "model_scaled.fit(X_scaled, y)\n",
    "\n",
    "print(\"Standardized coefficients (relative importance):\")\n",
    "for name, coef in sorted(zip(feature_names, model_scaled.coef_), \n",
    "                         key=lambda x: abs(x[1]), reverse=True):\n",
    "    print(f\"  {name}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coefficient importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': model_scaled.coef_\n",
    "})\n",
    "coef_df = coef_df.sort_values('Coefficient')\n",
    "\n",
    "colors = ['red' if x < 0 else 'green' for x in coef_df['Coefficient']]\n",
    "plt.barh(coef_df['Feature'], coef_df['Coefficient'], color=colors, edgecolor='black')\n",
    "plt.xlabel('Standardized Coefficient')\n",
    "plt.title('Feature Importance (Standardized Coefficients)')\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "Excellent! Training and test R² are nearly identical (~0.96). This tells us:\n\n1. **No overfitting**: The model isn't just memorizing training data\n2. **Good generalization**: Predictions on new data should be reliable\n3. **Appropriate complexity**: Linear model is well-suited to this problem\n\nIf we saw Train R² = 0.99 but Test R² = 0.70, that would be overfitting. The model would be fitting noise in the training data that doesn't appear in test data. We'd need to simplify the model or add regularization.\n\nThe similar train/test performance here suggests linear regression is a good choice for this problem—no need to try more complex methods.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} train-test split, overfitting\n```\n\n\n## Train/Test Split: The Most Important Concept in ML\n\nThis is perhaps the most important concept in machine learning. **Never evaluate a model on the data it was trained on.**\n\n### Why?\n\nThe goal isn't to explain the data you have—it's to predict data you haven't seen. A model that memorizes training data (overfitting) looks great on training metrics but fails on new data.\n\n### The Simple Solution: Hold-Out Validation\n\n1. **Split** data into training (typically 70-80%) and test (20-30%) sets\n2. **Train** only on training data\n3. **Evaluate** on test data\n\nThe test set simulates \"new, unseen data.\" If training and test performance are similar, your model generalizes well.\n\n### What the Gap Tells You\n\n- **Train R² ≈ Test R²**: Good! Model generalizes\n- **Train R² >> Test R²**: Overfitting! Model is too complex\n- **Train R² << Test R²**: Unusual (possibly data leakage or very small test set)\n- **Both R² are low**: Underfitting! Model is too simple or features aren't predictive"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on training data only\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on both sets\n",
    "train_score = model.score(X_train, y_train)\n",
    "test_score = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training R²: {train_score:.4f}\")\n",
    "print(f\"Test R²: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} R-squared, RMSE, MAE, evaluation metrics\n```\n\n\n## Model Evaluation Metrics: Choosing the Right One\n\nDifferent metrics answer different questions. Choosing the right one depends on what you care about.\n\n### R² (Coefficient of Determination)\n\n$$R^2 = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}$$\n\n**Interpretation**: Proportion of variance explained by the model\n- R² = 1: Perfect predictions\n- R² = 0: Model is as good as predicting the mean\n- R² < 0: Model is worse than predicting the mean (possible with test data!)\n\n**When to use**: General model comparison, communicating performance\n\n### RMSE (Root Mean Squared Error)\n\n$$RMSE = \\sqrt{\\frac{1}{n}\\sum(y_i - \\hat{y}_i)^2}$$\n\n**Interpretation**: Average prediction error in the same units as y\n- RMSE = 5% means predictions are off by ~5% on average\n\n**When to use**: When you need error in original units, when large errors are especially bad\n\n### MAE (Mean Absolute Error)\n\n$$MAE = \\frac{1}{n}\\sum|y_i - \\hat{y}_i|$$\n\n**Interpretation**: Average absolute error\n- Less sensitive to outliers than RMSE\n\n**When to use**: When outliers shouldn't dominate the metric"
  },
  {
   "cell_type": "markdown",
   "source": "The residual plots look healthy:\n\n**Left plot (Residuals vs Predicted)**:\n- Random scatter around zero ✓\n- No curved pattern → relationship is approximately linear ✓\n- Roughly constant spread → homoscedasticity ✓\n- No obvious outliers ✓\n\n**Right plot (Histogram)**:\n- Roughly symmetric and bell-shaped ✓\n- Centered near zero (mean ≈ 0) ✓\n- No severe skew or heavy tails ✓\n\n**Interpretation**: Our linear model assumptions are satisfied. If we saw:\n- A curved pattern → need polynomial terms or nonlinear model\n- A funnel shape → need weighted regression or log-transform\n- Severe outliers → investigate those data points\n- Non-normal residuals → may affect confidence intervals, but predictions still valid",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"  R²:   {r2_score(y_train, y_train_pred):.4f}\")\n",
    "print(f\"  MSE:  {mean_squared_error(y_train, y_train_pred):.6f}\")\n",
    "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_train, y_train_pred)):.4f}\")\n",
    "print(f\"  MAE:  {mean_absolute_error(y_train, y_train_pred):.4f}\")\n",
    "\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(f\"  R²:   {r2_score(y_test, y_test_pred):.4f}\")\n",
    "print(f\"  MSE:  {mean_squared_error(y_test, y_test_pred):.6f}\")\n",
    "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_test, y_test_pred)):.4f}\")\n",
    "print(f\"  MAE:  {mean_absolute_error(y_test, y_test_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs Actual plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training set\n",
    "axes[0].scatter(y_train, y_train_pred, alpha=0.6)\n",
    "axes[0].plot([y.min(), y.max()], [y.min(), y.max()], 'r--', linewidth=2)\n",
    "axes[0].set_xlabel('Actual Conversion')\n",
    "axes[0].set_ylabel('Predicted Conversion')\n",
    "axes[0].set_title(f'Training Set (R² = {r2_score(y_train, y_train_pred):.3f})')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test set\n",
    "axes[1].scatter(y_test, y_test_pred, alpha=0.6, color='orange')\n",
    "axes[1].plot([y.min(), y.max()], [y.min(), y.max()], 'r--', linewidth=2)\n",
    "axes[1].set_xlabel('Actual Conversion')\n",
    "axes[1].set_ylabel('Predicted Conversion')\n",
    "axes[1].set_title(f'Test Set (R² = {r2_score(y_test, y_test_pred):.3f})')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} residuals, residual analysis\n```\n\n\n## Residual Analysis: Diagnosing Model Problems\n\nResiduals (actual - predicted) are your model's mistakes. Analyzing them reveals problems that R² alone can't detect.\n\n### What Good Residuals Look Like\n\n- **Random scatter** around zero: No systematic patterns\n- **Constant variance**: Spread doesn't change with predicted value\n- **Normally distributed**: Bell-shaped histogram\n\n### Red Flags to Watch For\n\n| Pattern | What It Means | Possible Fix |\n|---------|---------------|--------------|\n| Curved pattern | Nonlinear relationship | Add polynomial terms or use nonlinear model |\n| Fan shape (increasing spread) | Heteroscedasticity | Log-transform y, use weighted regression |\n| Clusters | Distinct groups in data | Add categorical features, consider separate models |\n| Outliers | Unusual data points | Investigate, possibly remove or use robust methods |\n\n### The Residual Plot Recipe\n\n1. Plot residuals vs predicted values (should be random cloud)\n2. Plot histogram of residuals (should be roughly normal)\n3. Plot residuals vs each feature (look for patterns)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plots\n",
    "residuals = y_test - y_test_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Residuals vs predicted\n",
    "axes[0].scatter(y_test_pred, residuals, alpha=0.6)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0].set_xlabel('Predicted Conversion')\n",
    "axes[0].set_ylabel('Residual')\n",
    "axes[0].set_title('Residuals vs Predicted Values')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram of residuals\n",
    "axes[1].hist(residuals, bins=15, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=0, color='r', linestyle='--')\n",
    "axes[1].set_xlabel('Residual')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Residuals')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean residual: {residuals.mean():.6f}\")\n",
    "print(f\"Std of residuals: {residuals.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "Once trained, use the model to predict outcomes for new conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict conversion for new conditions\n",
    "new_conditions = pd.DataFrame({\n",
    "    'temperature': [350, 400, 450],\n",
    "    'pressure': [5, 5, 5],\n",
    "    'catalyst_loading': [2.5, 2.5, 2.5],\n",
    "    'residence_time': [15, 15, 15]\n",
    "})\n",
    "\n",
    "predictions = model.predict(new_conditions[feature_names].values)\n",
    "\n",
    "new_conditions['predicted_conversion'] = predictions\n",
    "print(\"Predictions for new conditions:\")\n",
    "new_conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Features\n",
    "\n",
    "If relationships are nonlinear, we can add polynomial terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create polynomial features (degree 2)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "print(f\"Original features: {X.shape[1]}\")\n",
    "print(f\"Polynomial features: {X_poly.shape[1]}\")\n",
    "print(f\"\\nFeature names: {poly.get_feature_names_out(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare linear vs polynomial\n",
    "X_train_poly, X_test_poly, y_train, y_test = train_test_split(\n",
    "    X_poly, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model_poly = LinearRegression()\n",
    "model_poly.fit(X_train_poly, y_train)\n",
    "\n",
    "print(f\"Linear model test R²: {model.score(X_test, y_test):.4f}\")\n",
    "print(f\"Polynomial model test R²: {model_poly.score(X_test_poly, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} multicollinearity, overfitting, extrapolation\n```\n\n\n## Common Pitfalls\n\n1. **Overfitting**: Model fits training data too well, poor generalization\n   - Solution: Use train/test split, regularization\n\n2. **Multicollinearity**: Features are highly correlated\n   - Can make coefficients unstable\n   - Solution: Remove redundant features, use PCA, or regularization\n\n3. **Extrapolation**: Predicting outside the range of training data\n   - Linear models may give unrealistic predictions\n   - Be cautious about predictions far from training data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for multicollinearity\n",
    "correlation_matrix = df[feature_names].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(correlation_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "plt.colorbar(label='Correlation')\n",
    "plt.xticks(range(len(feature_names)), feature_names, rotation=45, ha='right')\n",
    "plt.yticks(range(len(feature_names)), feature_names)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "\n",
    "# Add correlation values\n",
    "for i in range(len(feature_names)):\n",
    "    for j in range(len(feature_names)):\n",
    "        plt.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}', \n",
    "                 ha='center', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Self-Assessment Quiz\n\nTest your understanding of linear regression concepts:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "! pip install -q jupyterquiz\nfrom jupyterquiz import display_quiz\n\ndisplay_quiz(\"https://raw.githubusercontent.com/jkitchin/s26-06642/main/dsmles/06-linear-regression/quizzes/linear-regression-quiz.json\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Recommended Reading\n\nThese resources deepen understanding of linear regression and its applications:\n\n1. **[Scikit-learn Linear Models](https://scikit-learn.org/stable/modules/linear_model.html)** - Official documentation covering ordinary least squares, Ridge, Lasso, and other linear models. Includes guidance on when to use each variant.\n\n2. **[An Introduction to Statistical Learning, Chapter 3](https://www.statlearning.com/)** - Comprehensive treatment of linear regression including model assumptions, interpretation of coefficients, and diagnostics. Free PDF available.\n\n3. **[Linear Regression (Penn State STAT 501)](https://online.stat.psu.edu/stat501/lesson/1)** - A complete online course on regression with detailed coverage of residual analysis, multicollinearity, and model diagnostics.\n\n4. **[Regression and Other Stories (Gelman, Hill, Vehtari)](https://avehtari.github.io/ROS-Examples/)** - Modern treatment of regression focusing on interpretation and application. Excellent coverage of when regression works and when it doesn't.\n\n5. **[Common Pitfalls in Machine Learning (Domingos)](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)** - Discusses overfitting, the curse of dimensionality, and why evaluating on training data gives overly optimistic results.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary: The Linear Regression Workflow\n\nLinear regression is your starting point for predictive modeling. Here's the workflow:\n\n### The Process\n\n1. **Explore data** → Scatter plots, correlations\n2. **Split data** → Train/test (never skip this!)\n3. **Fit model** → `model.fit(X_train, y_train)`\n4. **Evaluate** → R², RMSE on test set\n5. **Diagnose** → Check residuals for problems\n6. **Interpret** → What do coefficients mean?\n\n### Key Decisions\n\n| Decision | Guidance |\n|----------|----------|\n| Feature scaling? | Yes, if you want to compare coefficient importance |\n| Test set size? | 20-30% is typical; smaller datasets might use cross-validation |\n| Which metric? | R² for overall fit, RMSE for error in original units |\n| Add polynomial terms? | Only if residuals show curvature |\n\n### Common Mistakes to Avoid\n\n1. **Evaluating on training data**: Always use a held-out test set\n2. **Ignoring multicollinearity**: Correlated features make coefficients unstable\n3. **Extrapolating**: Be cautious predicting outside the training data range\n4. **Confusing correlation with causation**: Coefficients show association, not causation\n\n### When to Move Beyond Linear Regression\n\n- Residuals show clear nonlinear patterns\n- R² is too low despite good features\n- Domain knowledge suggests nonlinear relationships\n- You need feature selection (→ Lasso, next module)\n\n## Next Steps\n\nIn the next module, we'll learn about regularization (Ridge, Lasso) to prevent overfitting and cross-validation for more reliable model selection."
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## The Catalyst Crisis: Chapter 6 - \"The Line That Explains (Almost) Everything\"\n\n*A story about linear regression and understanding model limitations*\n\n---\n\n\"R-squared of 0.78,\" Alex reported to the team. \"The model explains most of the variance in yield.\"\n\nShe'd built a proper regression model now, with domain-informed features, proper train-test splits, and cross-validation. It was a real predictive model—feed it operating conditions, get back an expected yield.\n\nBut Val Validation—the actual person, one of Professor Pipeline's teaching assistants—wasn't satisfied. She was reviewing the team's work for the weekly critique.\n\n\"Show me the residuals.\"\n\nAlex pulled up the residual plot. The errors should have been random noise, scattered evenly around zero. Instead, they showed a clear pattern—larger errors at the edges, systematic under-prediction for certain conditions.\n\n\"Your model is biased,\" Val said flatly. \"It's missing something.\"\n\n\"The R-squared is 0.78—\"\n\n\"R-squared lies.\" Val pointed at the screen. \"Look at *where* it fails. The worst errors are all on batches with high catalyst age. Your model doesn't know that old catalyst behaves differently.\"\n\nAlex stared at the pattern she'd somehow missed. Of course. The linear model assumed catalyst age had a constant effect. But the t-SNE clustering had already shown her that old catalyst pushed batches into a different regime entirely.\n\n\"I need an interaction term,\" she said slowly. \"Or a nonlinear model.\"\n\nVal almost smiled. \"Now you're asking the right question.\" She stood to leave. \"A model that honestly shows its limitations is more valuable than one that hides them. The residuals told you where to look next. That's a feature, not a bug.\"\n\nAfter Val left, Jordan spoke up. \"I thought our model was good.\"\n\n\"It is good. For certain conditions.\" Alex marked the high-error region on the plot. \"But we're extrapolating into territory where linear assumptions break down.\"\n\n\"So what do we do?\"\n\nAlex thought about it. The honest answer: she didn't know yet. The model was good enough to be useful but not complete. That felt uncomfortable.\n\n\"We document exactly where it works and where it doesn't,\" she said finally. \"And we keep investigating the catalyst.\"\n\nShe added to the mystery board: **Linear model works, except for old catalyst. Nonlinear relationship suspected.**\n\n---\n\n*Continue to the next lecture to see how the team tackles classification problems...*",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}