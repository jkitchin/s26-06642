{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jkitchin/s26-06642/blob/main/dsmles/06-linear-regression/linear-regression.ipynb)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 05: Linear Regression\n",
    "\n",
    "Building predictive models with linear regression.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand the linear regression model\n",
    "2. Fit models using scikit-learn\n",
    "3. Interpret regression coefficients\n",
    "4. Evaluate model performance with metrics\n",
    "5. Use train/test splits for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## The Linear Regression Model: Simple Yet Powerful\n\nLinear regression is often the first modeling tool we reach for, and for good reason. Despite its simplicity, it's:\n\n- **Interpretable**: Coefficients have clear physical meaning\n- **Fast**: Solutions are analytic, no iteration needed\n- **Robust**: Well-understood statistical properties\n- **A baseline**: If linear works, why use something complex?\n\n### The Mathematical Form\n\n$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p + \\epsilon$$\n\nEach coefficient $\\beta_i$ tells you: *\"When $x_i$ increases by 1 unit (holding other features constant), y changes by $\\beta_i$ units.\"*\n\n### When Does Linear Regression Work?\n\nLinear regression assumes the relationship between features and target is **approximately linear**. This works well when:\n- Effects are additive (no strong interactions)\n- Relationships are monotonic (increasing or decreasing)\n- You're working in a limited range where curves look straight\n\n### When It Fails\n\nLinear regression will struggle with:\n- Saturation effects (Michaelis-Menten kinetics, Langmuir isotherms)\n- Exponential relationships (Arrhenius law without log-transform)\n- Strong interactions (catalyst × temperature effects)\n- Threshold effects (phase transitions)\n\nWe'll learn nonlinear methods later. For now, remember: **start with linear, add complexity only when needed.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reactor experiment dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "# Features: temperature, pressure, catalyst loading, residence time\n",
    "temperature = np.random.uniform(300, 500, n_samples)  # K\n",
    "pressure = np.random.uniform(1, 10, n_samples)  # atm\n",
    "catalyst_loading = np.random.uniform(0.5, 5, n_samples)  # wt%\n",
    "residence_time = np.random.uniform(1, 30, n_samples)  # min\n",
    "\n",
    "# True relationship (with some noise)\n",
    "# Conversion increases with temp, pressure, catalyst, and time\n",
    "conversion = (\n",
    "    0.1 * (temperature - 300) / 200 +  # Temperature effect\n",
    "    0.05 * pressure +                   # Pressure effect\n",
    "    0.08 * catalyst_loading +           # Catalyst effect\n",
    "    0.01 * residence_time +             # Time effect\n",
    "    np.random.normal(0, 0.05, n_samples)  # Noise\n",
    ")\n",
    "conversion = np.clip(conversion, 0, 1)  # Keep between 0 and 1\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'temperature': temperature,\n",
    "    'pressure': pressure,\n",
    "    'catalyst_loading': catalyst_loading,\n",
    "    'residence_time': residence_time,\n",
    "    'conversion': conversion\n",
    "})\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look at the data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "Let's start with a single feature to understand the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear regression: conversion vs temperature\n",
    "X_simple = df[['temperature']].values\n",
    "y = df['conversion'].values\n",
    "\n",
    "# Fit the model\n",
    "model_simple = LinearRegression()\n",
    "model_simple.fit(X_simple, y)\n",
    "\n",
    "print(f\"Intercept: {model_simple.intercept_:.4f}\")\n",
    "print(f\"Coefficient (temperature): {model_simple.coef_[0]:.6f}\")\n",
    "print(f\"R² score: {model_simple.score(X_simple, y):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the fit\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.scatter(df['temperature'], df['conversion'], alpha=0.6, label='Data')\n",
    "\n",
    "# Regression line\n",
    "temp_range = np.linspace(300, 500, 100).reshape(-1, 1)\n",
    "plt.plot(temp_range, model_simple.predict(temp_range), 'r-', \n",
    "         linewidth=2, label='Linear fit')\n",
    "\n",
    "plt.xlabel('Temperature (K)')\n",
    "plt.ylabel('Conversion')\n",
    "plt.title('Simple Linear Regression: Conversion vs Temperature')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Now let's use all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple linear regression with all features\n",
    "feature_names = ['temperature', 'pressure', 'catalyst_loading', 'residence_time']\n",
    "X = df[feature_names].values\n",
    "y = df['conversion'].values\n",
    "\n",
    "# Fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "print(f\"Intercept: {model.intercept_:.4f}\")\n",
    "print(\"\\nCoefficients:\")\n",
    "for name, coef in zip(feature_names, model.coef_):\n",
    "    print(f\"  {name}: {coef:.6f}\")\n",
    "\n",
    "print(f\"\\nR² score: {model.score(X, y):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Interpreting Coefficients: The Hidden Complexity\n\nCoefficient interpretation seems straightforward, but there are important subtleties:\n\n### The Scale Problem\n\nRaw coefficients depend on feature scales. If temperature is in Kelvin (300-500) and pressure is in bar (1-10), their coefficients aren't directly comparable. A coefficient of 0.001 for temperature might be more important than 0.1 for pressure!\n\n**Solution**: Standardize features first, then coefficients are comparable.\n\n### Standardized Coefficients\n\nAfter standardizing (mean=0, std=1), coefficients answer: \"When this feature increases by 1 standard deviation, how many standard deviations does the target change?\"\n\nThis lets you rank feature importance fairly."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardized coefficients for comparison\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "model_scaled = LinearRegression()\n",
    "model_scaled.fit(X_scaled, y)\n",
    "\n",
    "print(\"Standardized coefficients (relative importance):\")\n",
    "for name, coef in sorted(zip(feature_names, model_scaled.coef_), \n",
    "                         key=lambda x: abs(x[1]), reverse=True):\n",
    "    print(f\"  {name}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coefficient importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': model_scaled.coef_\n",
    "})\n",
    "coef_df = coef_df.sort_values('Coefficient')\n",
    "\n",
    "colors = ['red' if x < 0 else 'green' for x in coef_df['Coefficient']]\n",
    "plt.barh(coef_df['Feature'], coef_df['Coefficient'], color=colors, edgecolor='black')\n",
    "plt.xlabel('Standardized Coefficient')\n",
    "plt.title('Feature Importance (Standardized Coefficients)')\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Train/Test Split: The Most Important Concept in ML\n\nThis is perhaps the most important concept in machine learning. **Never evaluate a model on the data it was trained on.**\n\n### Why?\n\nThe goal isn't to explain the data you have—it's to predict data you haven't seen. A model that memorizes training data (overfitting) looks great on training metrics but fails on new data.\n\n### The Simple Solution: Hold-Out Validation\n\n1. **Split** data into training (typically 70-80%) and test (20-30%) sets\n2. **Train** only on training data\n3. **Evaluate** on test data\n\nThe test set simulates \"new, unseen data.\" If training and test performance are similar, your model generalizes well.\n\n### What the Gap Tells You\n\n- **Train R² ≈ Test R²**: Good! Model generalizes\n- **Train R² >> Test R²**: Overfitting! Model is too complex\n- **Train R² << Test R²**: Unusual (possibly data leakage or very small test set)\n- **Both R² are low**: Underfitting! Model is too simple or features aren't predictive"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on training data only\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on both sets\n",
    "train_score = model.score(X_train, y_train)\n",
    "test_score = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training R²: {train_score:.4f}\")\n",
    "print(f\"Test R²: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Model Evaluation Metrics: Choosing the Right One\n\nDifferent metrics answer different questions. Choosing the right one depends on what you care about.\n\n### R² (Coefficient of Determination)\n\n$$R^2 = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}$$\n\n**Interpretation**: Proportion of variance explained by the model\n- R² = 1: Perfect predictions\n- R² = 0: Model is as good as predicting the mean\n- R² < 0: Model is worse than predicting the mean (possible with test data!)\n\n**When to use**: General model comparison, communicating performance\n\n### RMSE (Root Mean Squared Error)\n\n$$RMSE = \\sqrt{\\frac{1}{n}\\sum(y_i - \\hat{y}_i)^2}$$\n\n**Interpretation**: Average prediction error in the same units as y\n- RMSE = 5% means predictions are off by ~5% on average\n\n**When to use**: When you need error in original units, when large errors are especially bad\n\n### MAE (Mean Absolute Error)\n\n$$MAE = \\frac{1}{n}\\sum|y_i - \\hat{y}_i|$$\n\n**Interpretation**: Average absolute error\n- Less sensitive to outliers than RMSE\n\n**When to use**: When outliers shouldn't dominate the metric"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"  R²:   {r2_score(y_train, y_train_pred):.4f}\")\n",
    "print(f\"  MSE:  {mean_squared_error(y_train, y_train_pred):.6f}\")\n",
    "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_train, y_train_pred)):.4f}\")\n",
    "print(f\"  MAE:  {mean_absolute_error(y_train, y_train_pred):.4f}\")\n",
    "\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(f\"  R²:   {r2_score(y_test, y_test_pred):.4f}\")\n",
    "print(f\"  MSE:  {mean_squared_error(y_test, y_test_pred):.6f}\")\n",
    "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_test, y_test_pred)):.4f}\")\n",
    "print(f\"  MAE:  {mean_absolute_error(y_test, y_test_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs Actual plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training set\n",
    "axes[0].scatter(y_train, y_train_pred, alpha=0.6)\n",
    "axes[0].plot([y.min(), y.max()], [y.min(), y.max()], 'r--', linewidth=2)\n",
    "axes[0].set_xlabel('Actual Conversion')\n",
    "axes[0].set_ylabel('Predicted Conversion')\n",
    "axes[0].set_title(f'Training Set (R² = {r2_score(y_train, y_train_pred):.3f})')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test set\n",
    "axes[1].scatter(y_test, y_test_pred, alpha=0.6, color='orange')\n",
    "axes[1].plot([y.min(), y.max()], [y.min(), y.max()], 'r--', linewidth=2)\n",
    "axes[1].set_xlabel('Actual Conversion')\n",
    "axes[1].set_ylabel('Predicted Conversion')\n",
    "axes[1].set_title(f'Test Set (R² = {r2_score(y_test, y_test_pred):.3f})')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Residual Analysis: Diagnosing Model Problems\n\nResiduals (actual - predicted) are your model's mistakes. Analyzing them reveals problems that R² alone can't detect.\n\n### What Good Residuals Look Like\n\n- **Random scatter** around zero: No systematic patterns\n- **Constant variance**: Spread doesn't change with predicted value\n- **Normally distributed**: Bell-shaped histogram\n\n### Red Flags to Watch For\n\n| Pattern | What It Means | Possible Fix |\n|---------|---------------|--------------|\n| Curved pattern | Nonlinear relationship | Add polynomial terms or use nonlinear model |\n| Fan shape (increasing spread) | Heteroscedasticity | Log-transform y, use weighted regression |\n| Clusters | Distinct groups in data | Add categorical features, consider separate models |\n| Outliers | Unusual data points | Investigate, possibly remove or use robust methods |\n\n### The Residual Plot Recipe\n\n1. Plot residuals vs predicted values (should be random cloud)\n2. Plot histogram of residuals (should be roughly normal)\n3. Plot residuals vs each feature (look for patterns)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plots\n",
    "residuals = y_test - y_test_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Residuals vs predicted\n",
    "axes[0].scatter(y_test_pred, residuals, alpha=0.6)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0].set_xlabel('Predicted Conversion')\n",
    "axes[0].set_ylabel('Residual')\n",
    "axes[0].set_title('Residuals vs Predicted Values')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram of residuals\n",
    "axes[1].hist(residuals, bins=15, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=0, color='r', linestyle='--')\n",
    "axes[1].set_xlabel('Residual')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Residuals')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean residual: {residuals.mean():.6f}\")\n",
    "print(f\"Std of residuals: {residuals.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "Once trained, use the model to predict outcomes for new conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict conversion for new conditions\n",
    "new_conditions = pd.DataFrame({\n",
    "    'temperature': [350, 400, 450],\n",
    "    'pressure': [5, 5, 5],\n",
    "    'catalyst_loading': [2.5, 2.5, 2.5],\n",
    "    'residence_time': [15, 15, 15]\n",
    "})\n",
    "\n",
    "predictions = model.predict(new_conditions[feature_names].values)\n",
    "\n",
    "new_conditions['predicted_conversion'] = predictions\n",
    "print(\"Predictions for new conditions:\")\n",
    "new_conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Features\n",
    "\n",
    "If relationships are nonlinear, we can add polynomial terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create polynomial features (degree 2)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "print(f\"Original features: {X.shape[1]}\")\n",
    "print(f\"Polynomial features: {X_poly.shape[1]}\")\n",
    "print(f\"\\nFeature names: {poly.get_feature_names_out(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare linear vs polynomial\n",
    "X_train_poly, X_test_poly, y_train, y_test = train_test_split(\n",
    "    X_poly, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model_poly = LinearRegression()\n",
    "model_poly.fit(X_train_poly, y_train)\n",
    "\n",
    "print(f\"Linear model test R²: {model.score(X_test, y_test):.4f}\")\n",
    "print(f\"Polynomial model test R²: {model_poly.score(X_test_poly, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Pitfalls\n",
    "\n",
    "1. **Overfitting**: Model fits training data too well, poor generalization\n",
    "   - Solution: Use train/test split, regularization\n",
    "\n",
    "2. **Multicollinearity**: Features are highly correlated\n",
    "   - Can make coefficients unstable\n",
    "   - Solution: Remove redundant features, use PCA, or regularization\n",
    "\n",
    "3. **Extrapolation**: Predicting outside the range of training data\n",
    "   - Linear models may give unrealistic predictions\n",
    "   - Be cautious about predictions far from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for multicollinearity\n",
    "correlation_matrix = df[feature_names].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(correlation_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "plt.colorbar(label='Correlation')\n",
    "plt.xticks(range(len(feature_names)), feature_names, rotation=45, ha='right')\n",
    "plt.yticks(range(len(feature_names)), feature_names)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "\n",
    "# Add correlation values\n",
    "for i in range(len(feature_names)):\n",
    "    for j in range(len(feature_names)):\n",
    "        plt.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}', \n",
    "                 ha='center', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary: The Linear Regression Workflow\n\nLinear regression is your starting point for predictive modeling. Here's the workflow:\n\n### The Process\n\n1. **Explore data** → Scatter plots, correlations\n2. **Split data** → Train/test (never skip this!)\n3. **Fit model** → `model.fit(X_train, y_train)`\n4. **Evaluate** → R², RMSE on test set\n5. **Diagnose** → Check residuals for problems\n6. **Interpret** → What do coefficients mean?\n\n### Key Decisions\n\n| Decision | Guidance |\n|----------|----------|\n| Feature scaling? | Yes, if you want to compare coefficient importance |\n| Test set size? | 20-30% is typical; smaller datasets might use cross-validation |\n| Which metric? | R² for overall fit, RMSE for error in original units |\n| Add polynomial terms? | Only if residuals show curvature |\n\n### Common Mistakes to Avoid\n\n1. **Evaluating on training data**: Always use a held-out test set\n2. **Ignoring multicollinearity**: Correlated features make coefficients unstable\n3. **Extrapolating**: Be cautious predicting outside the training data range\n4. **Confusing correlation with causation**: Coefficients show association, not causation\n\n### When to Move Beyond Linear Regression\n\n- Residuals show clear nonlinear patterns\n- R² is too low despite good features\n- Domain knowledge suggests nonlinear relationships\n- You need feature selection (→ Lasso, next module)\n\n## Next Steps\n\nIn the next module, we'll learn about regularization (Ridge, Lasso) to prevent overfitting and cross-validation for more reliable model selection."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}