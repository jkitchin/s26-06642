{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jkitchin/s26-06642/blob/main/dsmles/08-regularization-model-selection/regularization-model-selection.ipynb)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "! curl -LsSf https://astral.sh/uv/install.sh | sh && \\\n  uv pip install -q --system \"s26-06642 @ git+https://github.com/jkitchin/s26-06642.git\"\nfrom pycse.colab import pdf",
   "metadata": {
    "tags": [
     "skip-execution",
     "remove-cell"
    ]
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Module 08: Regularization and Model Selection\n\nPreventing overfitting and choosing the best model.\n\n## Learning Objectives\n\n1. Understand overfitting and the bias-variance tradeoff\n2. Apply Ridge, Lasso, and ElasticNet regularization\n3. Use cross-validation for model selection\n4. Tune hyperparameters systematically\n5. Compare models fairly"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} overfitting, underfitting, bias-variance tradeoff\n```\n\n\n## The Overfitting Problem: Why Complex Models Fail\n\nOverfitting is the central challenge in machine learning. Understanding it is crucial.\n\n### What Is Overfitting?\n\nA model that **memorizes the training data** instead of learning generalizable patterns. It fits the noise, not the signal.\n\n### The Bias-Variance Tradeoff\n\nEvery model makes two types of errors:\n\n**Bias** (underfitting): Error from oversimplifying. A linear model can't capture nonlinear relationships, no matter how much data you have.\n\n**Variance** (overfitting): Error from being too sensitive to training data. A very flexible model will fit different training sets very differently.\n\n| Model Complexity | Bias | Variance | Result |\n|------------------|------|----------|--------|\n| Too simple | High | Low | Underfitting: misses patterns |\n| Just right | Medium | Medium | Generalizes well |\n| Too complex | Low | High | Overfitting: memorizes noise |\n\n### When Overfitting Happens\n\nYou're at risk when:\n- **Many features, few samples**: More parameters than data points to constrain them\n- **Features are correlated**: Multiple ways to explain the same variance\n- **Model is very flexible**: High-degree polynomials, deep trees, etc.\n- **No regularization**: Nothing preventing the model from fitting noise"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load sparse regression dataset\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nurl = \"https://raw.githubusercontent.com/jkitchin/s26-06642/main/dsmles/data/sparse_regression.csv\"\ndf = pd.read_csv(url)\n\n# Extract features and target\nfeature_cols = [col for col in df.columns if col != 'target']\nX = df[feature_cols].values\nn_features = X.shape[1]\ny = df['target'].values\n\n# Load true coefficients for reference\nurl_coef = \"https://raw.githubusercontent.com/jkitchin/s26-06642/main/dsmles/data/sparse_regression_true_coef.csv\"\ntrue_coef_df = pd.read_csv(url_coef)\ntrue_coef = true_coef_df['true_coef'].values\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nprint(f\"Dataset: {X.shape[0]} samples, {X.shape[1]} features\")\nprint(f\"Training: {X_train.shape[0]} samples, Test: {X_test.shape[0]} samples\")\nprint(f\"True non-zero coefficients at indices: {np.where(true_coef != 0)[0]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinary Least Squares (OLS) - no regularization\n",
    "ols = LinearRegression()\n",
    "ols.fit(X_train, y_train)\n",
    "\n",
    "print(\"OLS (No Regularization):\")\n",
    "print(f\"  Training R²: {ols.score(X_train, y_train):.4f}\")\n",
    "print(f\"  Test R²: {ols.score(X_test, y_test):.4f}\")\n",
    "print(f\"  Gap: {ols.score(X_train, y_train) - ols.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare OLS coefficients to true coefficients\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# True coefficients\n",
    "axes[0].bar(range(n_features), true_coef, edgecolor='black')\n",
    "axes[0].set_xlabel('Feature Index')\n",
    "axes[0].set_ylabel('Coefficient')\n",
    "axes[0].set_title('True Coefficients (Only 5 are non-zero)')\n",
    "axes[0].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# OLS coefficients\n",
    "axes[1].bar(range(n_features), ols.coef_, edgecolor='black')\n",
    "axes[1].set_xlabel('Feature Index')\n",
    "axes[1].set_ylabel('Coefficient')\n",
    "axes[1].set_title('OLS Estimated Coefficients (Many non-zero!)')\n",
    "axes[1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} Ridge regression, L2 regularization\n```\n\n\n## Ridge Regression (L2 Regularization): Shrink, Don't Select\n\nRidge regression adds a penalty on the sum of squared coefficients:\n\n$$\\min_\\beta \\|y - X\\beta\\|^2 + \\alpha \\|\\beta\\|^2$$\n\n### The Key Insight\n\nRidge **shrinks** coefficients toward zero but never exactly to zero. All features stay in the model, just with smaller effects.\n\n### When to Use Ridge\n\n- **Multicollinearity**: Correlated features cause unstable OLS coefficients. Ridge stabilizes them.\n- **Many features**: Even if all features might be relevant, you want smaller, more stable coefficients.\n- **Prediction focus**: You care more about prediction accuracy than identifying which features matter.\n\n### The Alpha Parameter\n\n- **α = 0**: Pure OLS, no regularization\n- **α → ∞**: All coefficients shrink to zero\n- **Optimal α**: Found via cross-validation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "print(\"Ridge Regression (alpha=1.0):\")\n",
    "print(f\"  Training R²: {ridge.score(X_train, y_train):.4f}\")\n",
    "print(f\"  Test R²: {ridge.score(X_test, y_test):.4f}\")\n",
    "print(f\"  Gap: {ridge.score(X_train, y_train) - ridge.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of alpha on coefficients\n",
    "alphas = np.logspace(-2, 4, 50)\n",
    "ridge_coefs = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    ridge_coefs.append(ridge.coef_)\n",
    "\n",
    "ridge_coefs = np.array(ridge_coefs)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(10):  # Plot first 10 features\n",
    "    color = 'red' if i < 5 else 'gray'  # Red for true predictors\n",
    "    alpha_val = 0.8 if i < 5 else 0.3\n",
    "    plt.semilogx(alphas, ridge_coefs[:, i], color=color, alpha=alpha_val)\n",
    "\n",
    "plt.xlabel('Alpha (Regularization Strength)')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Ridge: Coefficients vs Alpha (red = true predictors)')\n",
    "plt.axhline(y=0, color='black', linestyle='--', linewidth=0.5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} Lasso regression, L1 regularization, feature selection\n```\n\n\n## Lasso Regression (L1 Regularization): Shrink AND Select\n\nLasso adds a penalty on the sum of absolute coefficients:\n\n$$\\min_\\beta \\|y - X\\beta\\|^2 + \\alpha \\|\\beta\\|_1$$\n\n### The Key Insight\n\nThe L1 penalty has a remarkable property: it drives some coefficients **exactly to zero**. Lasso performs automatic feature selection!\n\n### Why L1 Creates Sparsity (Intuition)\n\nImagine you're trying to reduce the total \"cost\" of coefficients. With L2 (squared), reducing a large coefficient saves more than eliminating a small one. With L1 (absolute), eliminating a coefficient entirely saves just as much per unit as shrinking a large one. The optimizer often chooses to eliminate.\n\n### When to Use Lasso\n\n- **Feature selection needed**: You suspect many features are irrelevant\n- **Interpretability**: You want a sparse model with only important features\n- **High dimensionality**: When p > n (more features than samples)\n\n### The Tradeoff\n\nLasso can be unstable with correlated features—it might arbitrarily pick one and zero out the others. If you need stable selection of correlated features, consider ElasticNet."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso regression\n",
    "lasso = Lasso(alpha=0.1, max_iter=10000)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "print(\"Lasso Regression (alpha=0.1):\")\n",
    "print(f\"  Training R²: {lasso.score(X_train, y_train):.4f}\")\n",
    "print(f\"  Test R²: {lasso.score(X_test, y_test):.4f}\")\n",
    "print(f\"  Non-zero coefficients: {np.sum(lasso.coef_ != 0)} out of {n_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Lasso to true coefficients\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# True coefficients\n",
    "axes[0].bar(range(n_features), true_coef, edgecolor='black')\n",
    "axes[0].set_xlabel('Feature Index')\n",
    "axes[0].set_ylabel('Coefficient')\n",
    "axes[0].set_title('True Coefficients')\n",
    "\n",
    "# Lasso coefficients\n",
    "axes[1].bar(range(n_features), lasso.coef_, edgecolor='black')\n",
    "axes[1].set_xlabel('Feature Index')\n",
    "axes[1].set_ylabel('Coefficient')\n",
    "axes[1].set_title('Lasso Estimated Coefficients (Sparse!)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} ElasticNet\n```\n\n\n## ElasticNet (Combined L1 + L2)\n\nElasticNet combines both penalties:\n\n$$\\min_\\beta \\|y - X\\beta\\|^2 + \\alpha \\cdot l1\\_ratio \\cdot \\|\\beta\\|_1 + \\alpha \\cdot (1 - l1\\_ratio) \\cdot \\|\\beta\\|^2$$\n\n- `l1_ratio=1`: Pure Lasso\n- `l1_ratio=0`: Pure Ridge\n- `l1_ratio=0.5`: Balanced mix"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet\n",
    "elastic = ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000)\n",
    "elastic.fit(X_train, y_train)\n",
    "\n",
    "print(\"ElasticNet (alpha=0.1, l1_ratio=0.5):\")\n",
    "print(f\"  Training R²: {elastic.score(X_train, y_train):.4f}\")\n",
    "print(f\"  Test R²: {elastic.score(X_test, y_test):.4f}\")\n",
    "print(f\"  Non-zero coefficients: {np.sum(elastic.coef_ != 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} cross-validation, K-fold\n```\n\n\n## Cross-Validation: The Right Way to Evaluate Models\n\nA single train/test split is noisy. The test performance depends on which points happened to be in the test set. Cross-validation gives more reliable estimates.\n\n### How K-Fold Cross-Validation Works\n\n1. Split data into k equal parts (folds)\n2. For each fold:\n   - Train on k-1 folds\n   - Test on the remaining fold\n3. Average the k test scores\n\n### Why It's Better\n\n- Uses all data for both training and testing\n- Provides error bars (standard deviation across folds)\n- Less sensitive to the random split\n\n### Choosing K\n\n| K | Pros | Cons |\n|---|------|------|\n| 5 | Fast, reasonable variance estimate | Some bias if dataset is small |\n| 10 | Good balance, common default | Slower than 5-fold |\n| n (Leave-One-Out) | Minimum bias | High variance, slow, rarely used |\n\n**Rule of thumb**: K=5 or K=10 works well for most problems."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Use full dataset for CV\n",
    "cv_scores_ols = cross_val_score(LinearRegression(), X, y, cv=5, scoring='r2')\n",
    "cv_scores_ridge = cross_val_score(Ridge(alpha=1.0), X, y, cv=5, scoring='r2')\n",
    "cv_scores_lasso = cross_val_score(Lasso(alpha=0.1, max_iter=10000), X, y, cv=5, scoring='r2')\n",
    "\n",
    "print(\"5-Fold Cross-Validation R² Scores:\")\n",
    "print(f\"  OLS:   {cv_scores_ols.mean():.4f} (+/- {cv_scores_ols.std()*2:.4f})\")\n",
    "print(f\"  Ridge: {cv_scores_ridge.mean():.4f} (+/- {cv_scores_ridge.std()*2:.4f})\")\n",
    "print(f\"  Lasso: {cv_scores_lasso.mean():.4f} (+/- {cv_scores_lasso.std()*2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CV process\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 2))\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "    fold_array = np.zeros(len(X))\n",
    "    fold_array[train_idx] = 1  # Training = 1\n",
    "    fold_array[test_idx] = 2   # Test = 2\n",
    "    \n",
    "    axes[i].imshow([fold_array], aspect='auto', cmap='RdYlGn')\n",
    "    axes[i].set_title(f'Fold {i+1}')\n",
    "    axes[i].set_yticks([])\n",
    "    axes[i].set_xlabel('Sample Index')\n",
    "\n",
    "plt.suptitle('5-Fold Cross-Validation (Green=Train, Red=Test)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} hyperparameter tuning\n```\n\n\n## Hyperparameter Tuning with CV\n\nUse cross-validation to find the optimal regularization strength."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RidgeCV: automatically finds best alpha\n",
    "alphas = np.logspace(-4, 4, 50)\n",
    "\n",
    "ridge_cv = RidgeCV(alphas=alphas, cv=5)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Ridge alpha: {ridge_cv.alpha_:.4f}\")\n",
    "print(f\"Test R²: {ridge_cv.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LassoCV: automatically finds best alpha\n",
    "lasso_cv = LassoCV(alphas=np.logspace(-4, 1, 50), cv=5, max_iter=10000)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Lasso alpha: {lasso_cv.alpha_:.4f}\")\n",
    "print(f\"Test R²: {lasso_cv.score(X_test, y_test):.4f}\")\n",
    "print(f\"Non-zero coefficients: {np.sum(lasso_cv.coef_ != 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize alpha selection for Lasso\n",
    "alphas_lasso = np.logspace(-4, 1, 50)\n",
    "mse_path = []\n",
    "\n",
    "for alpha in alphas_lasso:\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    scores = -cross_val_score(lasso, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    mse_path.append(scores.mean())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(alphas_lasso, mse_path, 'o-')\n",
    "plt.axvline(x=lasso_cv.alpha_, color='r', linestyle='--', label=f'Best α = {lasso_cv.alpha_:.4f}')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Cross-Validation MSE')\n",
    "plt.title('Lasso: Alpha Selection via Cross-Validation')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} GridSearchCV\n```\n\n\n## Grid Search for Multiple Hyperparameters\n\nWhen you have multiple hyperparameters, use GridSearchCV."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Grid search for ElasticNet\n",
    "param_grid = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1.0],\n",
    "    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "}\n",
    "\n",
    "elastic = ElasticNet(max_iter=10000)\n",
    "grid_search = GridSearchCV(elastic, param_grid, cv=5, scoring='r2')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Test score: {grid_search.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize grid search results\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "pivot = results.pivot_table(\n",
    "    values='mean_test_score',\n",
    "    index='param_l1_ratio',\n",
    "    columns='param_alpha'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(pivot.values, cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='Mean CV R²')\n",
    "plt.xticks(range(len(pivot.columns)), pivot.columns)\n",
    "plt.yticks(range(len(pivot.index)), pivot.index)\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('L1 Ratio')\n",
    "plt.title('ElasticNet Grid Search Results')\n",
    "\n",
    "# Add values\n",
    "for i in range(len(pivot.index)):\n",
    "    for j in range(len(pivot.columns)):\n",
    "        plt.text(j, i, f'{pivot.values[i, j]:.3f}', ha='center', va='center', \n",
    "                 color='white' if pivot.values[i, j] < 0.7 else 'black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "```{index} Optuna, Bayesian optimization, TPE, hyperparameter optimization\n```\n\n## Bonus: Smarter Hyperparameter Search with Optuna\n\nGridSearchCV is exhaustive—it tries every combination. This is thorough but wasteful. If alpha=0.001 and alpha=0.01 both perform poorly, why try alpha=0.005?\n\n**Optuna** uses Bayesian optimization to learn from previous trials and suggest promising hyperparameters. It's especially powerful for:\n- **Continuous search spaces**: Instead of [0.001, 0.01, 0.1, 1.0], search the full range [0.001, 1.0]\n- **Many hyperparameters**: Scales better than grid search as dimensions increase\n- **Limited budgets**: Often finds good solutions with fewer trials\n\n### How Optuna Works\n\nOptuna uses the **Tree-structured Parzen Estimator (TPE)** sampler:\n1. Try a few random combinations (exploration)\n2. Model which hyperparameters led to good vs bad results\n3. Suggest new hyperparameters likely to improve (exploitation)\n4. Repeat until budget exhausted\n\n### When to Use Optuna vs GridSearchCV\n\n| Use GridSearchCV When | Use Optuna When |\n|----------------------|-----------------|\n| Few discrete choices (e.g., 3-5 alphas) | Continuous ranges or many values |\n| Need to try every combination | Want to find good solutions faster |\n| Search space is small (<50 trials) | Search space is large (>100 trials) |\n| Interpretability matters (exhaustive coverage) | Efficiency matters (limited compute) |\n\n### Installation\n\nOptuna is not in the default dependencies, so install it first:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%pip install -q optuna plotly",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import optuna\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import cross_val_score\n\n# Suppress Optuna's verbose output\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\ndef objective(trial):\n    \"\"\"\n    Objective function for Optuna to maximize.\n    \n    trial.suggest_float() samples from continuous ranges.\n    Returns the mean CV score (higher is better).\n    \"\"\"\n    # Suggest hyperparameters from continuous ranges\n    alpha = trial.suggest_float('alpha', 0.001, 1.0, log=True)\n    l1_ratio = trial.suggest_float('l1_ratio', 0.1, 0.9)\n    \n    # Create model with suggested hyperparameters\n    model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=10000)\n    \n    # Evaluate with 5-fold cross-validation (same as GridSearchCV)\n    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n    \n    # Return mean CV score (Optuna will maximize this)\n    return scores.mean()\n\n# Create study and optimize\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\nstudy.optimize(objective, n_trials=20)  # Same budget as GridSearch\n\nprint(\"Optuna Results:\")\nprint(f\"  Best alpha: {study.best_params['alpha']:.4f}\")\nprint(f\"  Best l1_ratio: {study.best_params['l1_ratio']:.4f}\")\nprint(f\"  Best CV R²: {study.best_value:.4f}\")\n\n# Compare to GridSearchCV results (from Cell 24)\nprint(f\"\\nGridSearchCV best CV R²: {grid_search.best_score_:.4f}\")\nprint(f\"Difference: {study.best_value - grid_search.best_score_:.4f}\")\n\n# Evaluate on test set\nbest_model = ElasticNet(**study.best_params, max_iter=10000)\nbest_model.fit(X_train, y_train)\nprint(f\"\\nOptuna best model test R²: {best_model.score(X_test, y_test):.4f}\")\nprint(f\"GridSearch best model test R²: {grid_search.score(X_test, y_test):.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Visualizing the Optimization Process\n\nOne advantage of Optuna is transparency: we can see how it learns over time.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\n\n# Extract trial data\ntrials = study.trials\ntrial_numbers = [t.number for t in trials]\ntrial_values = [t.value for t in trials]\n\n# Calculate running best (cumulative maximum)\nrunning_best = []\nbest_so_far = -np.inf\nfor value in trial_values:\n    best_so_far = max(best_so_far, value)\n    running_best.append(best_so_far)\n\nplt.figure(figsize=(10, 6))\n\n# Plot all trials\nplt.scatter(trial_numbers, trial_values, alpha=0.6, label='Trial CV R²', zorder=2)\n\n# Plot running best\nplt.plot(trial_numbers, running_best, 'g-', linewidth=2, label='Best So Far', zorder=3)\n\n# GridSearch best as reference\nplt.axhline(y=grid_search.best_score_, color='red', linestyle='--',\n            linewidth=2, label=f'GridSearch Best ({grid_search.best_score_:.4f})', zorder=1)\n\nplt.xlabel('Trial Number')\nplt.ylabel('Cross-Validation R²')\nplt.title('Optuna Optimization History (TPE Sampler)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Find when Optuna matched GridSearch\ntrials_to_match = np.argmax(np.array(running_best) >= grid_search.best_score_) + 1\nprint(f\"Trials needed to match GridSearch: {trials_to_match}/{len(trials)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Understanding the Hyperparameter Landscape\n\nOptuna provides built-in visualizations to understand which parameters matter most.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from optuna.visualization import plot_param_importances, plot_contour\n\n# Parameter importance (based on functional ANOVA)\nfig = plot_param_importances(study)\nfig.update_layout(title='Which Hyperparameter Matters More?')\nfig.show()\n\nprint(\"\\nInterpretation:\")\nprint(\"- Higher importance = larger impact on CV score\")\nprint(\"- In this sparse regression problem, both alpha and l1_ratio matter\")\nprint(\"- Alpha controls overall regularization strength\")\nprint(\"- L1_ratio controls feature selection (higher = more Lasso-like)\")\n\n# Contour plot: alpha vs l1_ratio landscape\nfig = plot_contour(study, params=['alpha', 'l1_ratio'])\nfig.update_layout(title='Hyperparameter Landscape: Alpha vs L1 Ratio')\nfig.show()\n\nprint(\"\\nCompare to GridSearchCV heatmap (Cell 25):\")\nprint(\"- GridSearch samples uniformly (20 fixed grid points)\")\nprint(\"- Optuna samples densely in promising regions (adaptive)\")\nprint(\"- Both identify similar optimal region (low-to-medium alpha, high l1_ratio)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Advanced: Automatic Model Selection\n\nGridSearchCV can't easily compare different model types. Optuna can suggest categorical choices, enabling **model architecture search**.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def multi_model_objective(trial):\n    \"\"\"\n    Objective that selects model type AND tunes its hyperparameters.\n    \n    Demonstrates categorical parameters and conditional logic.\n    \"\"\"\n    # Suggest model type\n    model_type = trial.suggest_categorical('model', ['Ridge', 'Lasso', 'ElasticNet'])\n    \n    # Shared hyperparameter\n    alpha = trial.suggest_float('alpha', 0.001, 10.0, log=True)\n    \n    # Model-specific hyperparameters\n    if model_type == 'Ridge':\n        from sklearn.linear_model import Ridge\n        model = Ridge(alpha=alpha)\n    elif model_type == 'Lasso':\n        from sklearn.linear_model import Lasso\n        model = Lasso(alpha=alpha, max_iter=10000)\n    else:  # ElasticNet\n        l1_ratio = trial.suggest_float('l1_ratio', 0.1, 0.9)\n        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=10000)\n    \n    # Evaluate\n    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n    return scores.mean()\n\n# Optimize with larger budget (3 models to explore)\nmulti_study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\nmulti_study.optimize(multi_model_objective, n_trials=30)\n\nprint(\"Best Configuration:\")\nprint(f\"  Model: {multi_study.best_params['model']}\")\nprint(f\"  Alpha: {multi_study.best_params['alpha']:.4f}\")\nif 'l1_ratio' in multi_study.best_params:\n    print(f\"  L1 Ratio: {multi_study.best_params['l1_ratio']:.4f}\")\nprint(f\"  CV R²: {multi_study.best_value:.4f}\")\n\n# Count model selections\nmodel_counts = {}\nfor trial in multi_study.trials:\n    model = trial.params['model']\n    model_counts[model] = model_counts.get(model, 0) + 1\n\nprint(f\"\\nModel Selection Frequency:\")\nfor model, count in sorted(model_counts.items(), key=lambda x: x[1], reverse=True):\n    print(f\"  {model}: {count}/{len(multi_study.trials)} trials ({100*count/len(multi_study.trials):.1f}%)\")\n\nprint(\"\\nNote: This is difficult/impossible with GridSearchCV!\")\nprint(\"You'd need separate grid searches for each model type.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Optuna + Pipelines: Production Pattern\n\nCombine Optuna with scikit-learn pipelines for a complete, leak-free workflow.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\ndef pipeline_objective(trial):\n    \"\"\"\n    Tune preprocessing AND model hyperparameters together.\n    \n    Demonstrates that Optuna works seamlessly with Pipeline objects.\n    \"\"\"\n    # Preprocessing choices\n    use_scaling = trial.suggest_categorical('use_scaling', [True, False])\n    \n    # Model hyperparameters\n    alpha = trial.suggest_float('alpha', 0.001, 1.0, log=True)\n    l1_ratio = trial.suggest_float('l1_ratio', 0.1, 0.9)\n    \n    # Build pipeline\n    steps = []\n    if use_scaling:\n        steps.append(('scaler', StandardScaler()))\n    steps.append(('model', ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=10000)))\n    \n    pipeline = Pipeline(steps)\n    \n    # Evaluate (cross_val_score handles train/val splits correctly, no leakage)\n    scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='r2')\n    return scores.mean()\n\n# Optimize\npipeline_study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\npipeline_study.optimize(pipeline_objective, n_trials=20)\n\nprint(\"Best Pipeline Configuration:\")\nprint(f\"  Use Scaling: {pipeline_study.best_params['use_scaling']}\")\nprint(f\"  Alpha: {pipeline_study.best_params['alpha']:.4f}\")\nprint(f\"  L1 Ratio: {pipeline_study.best_params['l1_ratio']:.4f}\")\nprint(f\"  CV R²: {pipeline_study.best_value:.4f}\")\n\nprint(\"\\nNote: Regularization typically requires scaling (penalizes by magnitude).\")\nprint(\"Optuna should discover use_scaling=True is better.\")\nprint(\"This demonstrates automatic discovery of preprocessing choices!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Summary: When to Use Optuna\n\n**Advantages:**\n- **Smarter sampling**: TPE learns from past trials, focuses on promising regions\n- **Continuous spaces**: No need to discretize (alpha can be any value in [0.001, 1.0])\n- **Scalability**: Efficient with many hyperparameters (>3 dimensions)\n- **Flexibility**: Categorical choices, conditional parameters, multi-model selection\n- **Transparency**: Built-in visualizations and importance analysis\n\n**Limitations:**\n- **External dependency**: Not in scikit-learn, requires installation\n- **Complexity**: More code than GridSearchCV for simple cases\n- **Randomness**: Non-deterministic (even with seed, results may vary slightly)\n- **Overhead**: For tiny search spaces (<20 trials), GridSearch is simpler\n\n**Best Practices:**\n\n1. **Start with GridSearch** for initial exploration (discrete, interpretable)\n2. **Switch to Optuna** when:\n   - Search space is large (>50 combinations)\n   - Hyperparameters are continuous\n   - You need to compare different model architectures\n3. **Use the same CV strategy** for fair comparison (same folds, same metric)\n4. **Set a seed** for reproducibility (`sampler=TPESampler(seed=42)`)\n5. **Visualize the optimization** to ensure convergence\n6. **Combine with Pipelines** to prevent data leakage\n\n**Alternative Tools:**\n- **scikit-learn's RandomizedSearchCV**: Middle ground (random sampling, no Bayesian optimization)\n- **Hyperopt**: Similar to Optuna, TPE-based (Optuna has cleaner API)\n- **Ray Tune**: For distributed hyperparameter search at scale\n- **Weights & Biases Sweeps**: For deep learning experiment tracking\n\n### Key Takeaway\n\nGridSearchCV is exhaustive and interpretable. Optuna is intelligent and efficient. For most problems, start with GridSearchCV. Upgrade to Optuna when you need more power, flexibility, or efficiency.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines for Reproducible Workflows\n",
    "\n",
    "Combine preprocessing and modeling into a single pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline example\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('lasso', Lasso(alpha=0.1, max_iter=10000))\n",
    "])\n",
    "\n",
    "# Cross-validate the entire pipeline\n",
    "scores = cross_val_score(pipeline, X, y, cv=5, scoring='r2')\n",
    "print(f\"Pipeline CV R²: {scores.mean():.4f} (+/- {scores.std()*2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search with pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', Ridge())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__alpha': np.logspace(-2, 4, 20)\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best alpha: {grid.best_params_['model__alpha']:.4f}\")\n",
    "print(f\"Test R²: {grid.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all methods\n",
    "models = {\n",
    "    'OLS': LinearRegression(),\n",
    "    'Ridge (CV)': RidgeCV(alphas=np.logspace(-4, 4, 50), cv=5),\n",
    "    'Lasso (CV)': LassoCV(alphas=np.logspace(-4, 1, 50), cv=5, max_iter=10000),\n",
    "    'ElasticNet (CV)': ElasticNetCV(l1_ratio=[0.1, 0.5, 0.9], cv=5, max_iter=10000)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    train_score = model.score(X_train, y_train)\n",
    "    test_score = model.score(X_test, y_test)\n",
    "    \n",
    "    # Count non-zero coefficients\n",
    "    n_nonzero = np.sum(model.coef_ != 0)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Train R²': train_score,\n",
    "        'Test R²': test_score,\n",
    "        'Overfit Gap': train_score - test_score,\n",
    "        'Non-zero Coefs': n_nonzero\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Test Your Knowledge",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%pip install -q jupyterquiz\nfrom jupyterquiz import display_quiz\n\ndisplay_quiz(\"https://raw.githubusercontent.com/jkitchin/s26-06642/main/dsmles/08-regularization-model-selection/quizzes/regularization-quiz.json\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Recommended Reading\n\nThese resources cover regularization theory and model selection best practices:\n\n1. **[An Introduction to Statistical Learning, Chapter 6](https://www.statlearning.com/)** - Comprehensive coverage of Ridge, Lasso, and model selection. Includes the geometric intuition for why L1 produces sparsity.\n\n2. **[Scikit-learn Model Selection](https://scikit-learn.org/stable/model_selection.html)** - Official documentation on cross-validation, grid search, and hyperparameter tuning. Essential reference for practical implementation.\n\n3. **[Regularization for Machine Learning (Ng, Stanford CS229)](https://cs229.stanford.edu/notes2022fall/main_notes.pdf)** - Course notes explaining the bias-variance tradeoff and how regularization addresses overfitting. Clear mathematical treatment.\n\n4. **[The Elements of Statistical Learning, Chapter 3](https://hastie.su.domains/ElemStatLearn/)** - Advanced treatment of shrinkage methods including the relationship between Ridge, Lasso, and subset selection. Free PDF available.\n\n5. **[Cross-Validation Pitfalls (Hastie & Efron)](https://web.stanford.edu/~hastie/TALKS/cv.pdf)** - Discusses common mistakes in cross-validation, including data leakage and proper nested CV. Important for avoiding subtle errors.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary: Regularization and Model Selection\n\n### The Regularization Toolkit\n\n| Method | Penalty | Effect | Use When |\n|--------|---------|--------|----------|\n| **Ridge** | L2 (squared) | Shrinks all coefficients | Multicollinearity, stability |\n| **Lasso** | L1 (absolute) | Some coefficients = 0 | Feature selection, sparsity |\n| **ElasticNet** | L1 + L2 | Combines both | Correlated features + selection |\n\n### Key Decisions\n\n| Decision | Guidance |\n|----------|----------|\n| Ridge vs Lasso? | Lasso if you want feature selection; Ridge if all features might matter |\n| How to choose α? | Cross-validation (RidgeCV, LassoCV) |\n| How many folds? | 5-10 is typical |\n| Grid search exhaustive? | Use RandomizedSearchCV for many hyperparameters |\n\n### The Model Selection Workflow\n\n1. **Start with cross-validation** for reliable estimates\n2. **Use *CV variants** (RidgeCV, LassoCV) for automatic hyperparameter tuning\n3. **Use GridSearchCV** when you have multiple hyperparameters\n4. **Use Pipelines** to combine preprocessing + modeling + tuning\n\n### Common Pitfalls\n\n- **Tuning on test data**: Never! Use cross-validation for tuning, keep test set for final evaluation\n- **Forgetting to scale**: Regularization penalizes magnitude—scale features first!\n- **Ignoring ElasticNet**: It's often better than pure Lasso with correlated features\n- **Not using pipelines**: Risk of data leakage when scaling/transforming\n\n## Next Steps\n\nIn the next module, we'll explore nonlinear methods (polynomial features, SVR) for when linear models aren't enough."
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## The Catalyst Crisis: Chapter 8 - \"The Danger of Being Too Clever\"\n\n*A story about overfitting, regularization, and learning from struggle*\n\n---\n\nJordan was still in the lab at 11 PM when Alex arrived.\n\nHe startled at the door. \"I didn't hear you come in.\"\n\n\"Couldn't sleep. Thought I'd review some results.\" Alex noticed the dark circles under Jordan's eyes, the forest of crumpled sticky notes around his laptop. \"How long have you been here?\"\n\n\"I don't know. A while.\" He gestured at his screen. \"I built this model—polynomial features, degree four, all the interactions. R-squared of 0.97 on training data.\"\n\nAlex sat down beside him. \"What about test data?\"\n\nJordan was silent for a moment. \"0.41.\"\n\n\"Ah.\"\n\n\"I don't understand. It fits the training data almost perfectly. But when I give it new data...\" He trailed off.\n\nAlex had made this exact mistake, three weeks ago. It was strangely comforting to see someone else struggling with it too.\n\n\"You're overfitting,\" she said gently. \"The model memorized the training data instead of learning the underlying pattern. It's like studying for a test by memorizing the answers to last year's questions. Works great until the questions change.\"\n\n\"So what do I do?\"\n\n\"Regularization.\" Alex pulled up her own work. \"Ridge and Lasso. They add a penalty for model complexity. The model has to earn each feature—if a variable isn't pulling its weight, it gets shrunk toward zero.\"\n\nThey worked together through the night, rebuilding Jordan's model with L1 regularization. The training R-squared dropped to 0.82—less impressive on paper. But the test R-squared climbed to 0.79. The gap nearly closed.\n\n\"It's... less good on training data,\" Jordan said uncertainly.\n\n\"But more honest. More generalizable.\" Alex pointed at the coefficient plot. \"Look—Lasso zeroed out twenty features. They weren't helping; they were fitting noise.\"\n\n\"How do you know which features to trust?\"\n\n\"The ones that survive regularization. The ones that show up consistently across cross-validation folds. The ones that make physical sense.\" She highlighted the non-zero coefficients. \"Catalyst age. Temperature. Pressure. The basics. Everything else was distraction.\"\n\nJordan leaned back, exhaustion and relief mingling on his face. \"You too? The struggling, I mean?\"\n\n\"Everyone. Maya's been up late debugging. Sam nearly quit after their PCA mistake. We're all figuring it out.\"\n\n\"Nobody talks about it.\"\n\n\"I know. We should.\" Alex stood to leave, then paused. \"Same time tomorrow? I'm working on the classification threshold problem—could use a second set of eyes.\"\n\nJordan nodded. \"Thanks, Alex.\"\n\nShe added to the mystery board: **Simpler models generalize better. Regularization keeps only what matters: catalyst, temperature, pressure.**\n\n---\n\n*Continue to the next lecture to explore nonlinear methods...*",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}