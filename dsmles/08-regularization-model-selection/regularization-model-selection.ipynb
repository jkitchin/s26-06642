{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jkitchin/s26-06642/blob/main/dsmles/08-regularization-model-selection/regularization-model-selection.ipynb)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "! pip install -q pycse\nfrom pycse.colab import pdf",
   "metadata": {
    "tags": [
     "skip-execution",
     "remove-cell"
    ]
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Module 08: Regularization and Model Selection\n\nPreventing overfitting and choosing the best model.\n\n## Learning Objectives\n\n1. Understand overfitting and the bias-variance tradeoff\n2. Apply Ridge, Lasso, and ElasticNet regularization\n3. Use cross-validation for model selection\n4. Tune hyperparameters systematically\n5. Compare models fairly"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} overfitting, underfitting, bias-variance tradeoff\n```\n\n\n## The Overfitting Problem: Why Complex Models Fail\n\nOverfitting is the central challenge in machine learning. Understanding it is crucial.\n\n### What Is Overfitting?\n\nA model that **memorizes the training data** instead of learning generalizable patterns. It fits the noise, not the signal.\n\n### The Bias-Variance Tradeoff\n\nEvery model makes two types of errors:\n\n**Bias** (underfitting): Error from oversimplifying. A linear model can't capture nonlinear relationships, no matter how much data you have.\n\n**Variance** (overfitting): Error from being too sensitive to training data. A very flexible model will fit different training sets very differently.\n\n| Model Complexity | Bias | Variance | Result |\n|------------------|------|----------|--------|\n| Too simple | High | Low | Underfitting: misses patterns |\n| Just right | Medium | Medium | Generalizes well |\n| Too complex | Low | High | Overfitting: memorizes noise |\n\n### When Overfitting Happens\n\nYou're at risk when:\n- **Many features, few samples**: More parameters than data points to constrain them\n- **Features are correlated**: Multiple ways to explain the same variance\n- **Model is very flexible**: High-degree polynomials, deep trees, etc.\n- **No regularization**: Nothing preventing the model from fitting noise"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load sparse regression dataset\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nurl = \"https://raw.githubusercontent.com/jkitchin/s26-06642/main/dsmles/data/sparse_regression.csv\"\ndf = pd.read_csv(url)\n\n# Extract features and target\nfeature_cols = [col for col in df.columns if col != 'target']\nX = df[feature_cols].values\nn_features = X.shape[1]\ny = df['target'].values\n\n# Load true coefficients for reference\nurl_coef = \"https://raw.githubusercontent.com/jkitchin/s26-06642/main/dsmles/data/sparse_regression_true_coef.csv\"\ntrue_coef_df = pd.read_csv(url_coef)\ntrue_coef = true_coef_df['true_coef'].values\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nprint(f\"Dataset: {X.shape[0]} samples, {X.shape[1]} features\")\nprint(f\"Training: {X_train.shape[0]} samples, Test: {X_test.shape[0]} samples\")\nprint(f\"True non-zero coefficients at indices: {np.where(true_coef != 0)[0]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinary Least Squares (OLS) - no regularization\n",
    "ols = LinearRegression()\n",
    "ols.fit(X_train, y_train)\n",
    "\n",
    "print(\"OLS (No Regularization):\")\n",
    "print(f\"  Training R\u00b2: {ols.score(X_train, y_train):.4f}\")\n",
    "print(f\"  Test R\u00b2: {ols.score(X_test, y_test):.4f}\")\n",
    "print(f\"  Gap: {ols.score(X_train, y_train) - ols.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare OLS coefficients to true coefficients\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# True coefficients\n",
    "axes[0].bar(range(n_features), true_coef, edgecolor='black')\n",
    "axes[0].set_xlabel('Feature Index')\n",
    "axes[0].set_ylabel('Coefficient')\n",
    "axes[0].set_title('True Coefficients (Only 5 are non-zero)')\n",
    "axes[0].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# OLS coefficients\n",
    "axes[1].bar(range(n_features), ols.coef_, edgecolor='black')\n",
    "axes[1].set_xlabel('Feature Index')\n",
    "axes[1].set_ylabel('Coefficient')\n",
    "axes[1].set_title('OLS Estimated Coefficients (Many non-zero!)')\n",
    "axes[1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} Ridge regression, L2 regularization\n```\n\n\n## Ridge Regression (L2 Regularization): Shrink, Don't Select\n\nRidge regression adds a penalty on the sum of squared coefficients:\n\n$$\\min_\\beta \\|y - X\\beta\\|^2 + \\alpha \\|\\beta\\|^2$$\n\n### The Key Insight\n\nRidge **shrinks** coefficients toward zero but never exactly to zero. All features stay in the model, just with smaller effects.\n\n### When to Use Ridge\n\n- **Multicollinearity**: Correlated features cause unstable OLS coefficients. Ridge stabilizes them.\n- **Many features**: Even if all features might be relevant, you want smaller, more stable coefficients.\n- **Prediction focus**: You care more about prediction accuracy than identifying which features matter.\n\n### The Alpha Parameter\n\n- **\u03b1 = 0**: Pure OLS, no regularization\n- **\u03b1 \u2192 \u221e**: All coefficients shrink to zero\n- **Optimal \u03b1**: Found via cross-validation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "print(\"Ridge Regression (alpha=1.0):\")\n",
    "print(f\"  Training R\u00b2: {ridge.score(X_train, y_train):.4f}\")\n",
    "print(f\"  Test R\u00b2: {ridge.score(X_test, y_test):.4f}\")\n",
    "print(f\"  Gap: {ridge.score(X_train, y_train) - ridge.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of alpha on coefficients\n",
    "alphas = np.logspace(-2, 4, 50)\n",
    "ridge_coefs = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    ridge_coefs.append(ridge.coef_)\n",
    "\n",
    "ridge_coefs = np.array(ridge_coefs)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(10):  # Plot first 10 features\n",
    "    color = 'red' if i < 5 else 'gray'  # Red for true predictors\n",
    "    alpha_val = 0.8 if i < 5 else 0.3\n",
    "    plt.semilogx(alphas, ridge_coefs[:, i], color=color, alpha=alpha_val)\n",
    "\n",
    "plt.xlabel('Alpha (Regularization Strength)')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Ridge: Coefficients vs Alpha (red = true predictors)')\n",
    "plt.axhline(y=0, color='black', linestyle='--', linewidth=0.5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} Lasso regression, L1 regularization, feature selection\n```\n\n\n## Lasso Regression (L1 Regularization): Shrink AND Select\n\nLasso adds a penalty on the sum of absolute coefficients:\n\n$$\\min_\\beta \\|y - X\\beta\\|^2 + \\alpha \\|\\beta\\|_1$$\n\n### The Key Insight\n\nThe L1 penalty has a remarkable property: it drives some coefficients **exactly to zero**. Lasso performs automatic feature selection!\n\n### Why L1 Creates Sparsity (Intuition)\n\nImagine you're trying to reduce the total \"cost\" of coefficients. With L2 (squared), reducing a large coefficient saves more than eliminating a small one. With L1 (absolute), eliminating a coefficient entirely saves just as much per unit as shrinking a large one. The optimizer often chooses to eliminate.\n\n### When to Use Lasso\n\n- **Feature selection needed**: You suspect many features are irrelevant\n- **Interpretability**: You want a sparse model with only important features\n- **High dimensionality**: When p > n (more features than samples)\n\n### The Tradeoff\n\nLasso can be unstable with correlated features\u2014it might arbitrarily pick one and zero out the others. If you need stable selection of correlated features, consider ElasticNet."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso regression\n",
    "lasso = Lasso(alpha=0.1, max_iter=10000)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "print(\"Lasso Regression (alpha=0.1):\")\n",
    "print(f\"  Training R\u00b2: {lasso.score(X_train, y_train):.4f}\")\n",
    "print(f\"  Test R\u00b2: {lasso.score(X_test, y_test):.4f}\")\n",
    "print(f\"  Non-zero coefficients: {np.sum(lasso.coef_ != 0)} out of {n_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Lasso to true coefficients\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# True coefficients\n",
    "axes[0].bar(range(n_features), true_coef, edgecolor='black')\n",
    "axes[0].set_xlabel('Feature Index')\n",
    "axes[0].set_ylabel('Coefficient')\n",
    "axes[0].set_title('True Coefficients')\n",
    "\n",
    "# Lasso coefficients\n",
    "axes[1].bar(range(n_features), lasso.coef_, edgecolor='black')\n",
    "axes[1].set_xlabel('Feature Index')\n",
    "axes[1].set_ylabel('Coefficient')\n",
    "axes[1].set_title('Lasso Estimated Coefficients (Sparse!)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} ElasticNet\n```\n\n\n## ElasticNet (Combined L1 + L2)\n\nElasticNet combines both penalties:\n\n$$\\min_\\beta \\|y - X\\beta\\|^2 + \\alpha \\cdot l1\\_ratio \\cdot \\|\\beta\\|_1 + \\alpha \\cdot (1 - l1\\_ratio) \\cdot \\|\\beta\\|^2$$\n\n- `l1_ratio=1`: Pure Lasso\n- `l1_ratio=0`: Pure Ridge\n- `l1_ratio=0.5`: Balanced mix"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet\n",
    "elastic = ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000)\n",
    "elastic.fit(X_train, y_train)\n",
    "\n",
    "print(\"ElasticNet (alpha=0.1, l1_ratio=0.5):\")\n",
    "print(f\"  Training R\u00b2: {elastic.score(X_train, y_train):.4f}\")\n",
    "print(f\"  Test R\u00b2: {elastic.score(X_test, y_test):.4f}\")\n",
    "print(f\"  Non-zero coefficients: {np.sum(elastic.coef_ != 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} cross-validation, K-fold\n```\n\n\n## Cross-Validation: The Right Way to Evaluate Models\n\nA single train/test split is noisy. The test performance depends on which points happened to be in the test set. Cross-validation gives more reliable estimates.\n\n### How K-Fold Cross-Validation Works\n\n1. Split data into k equal parts (folds)\n2. For each fold:\n   - Train on k-1 folds\n   - Test on the remaining fold\n3. Average the k test scores\n\n### Why It's Better\n\n- Uses all data for both training and testing\n- Provides error bars (standard deviation across folds)\n- Less sensitive to the random split\n\n### Choosing K\n\n| K | Pros | Cons |\n|---|------|------|\n| 5 | Fast, reasonable variance estimate | Some bias if dataset is small |\n| 10 | Good balance, common default | Slower than 5-fold |\n| n (Leave-One-Out) | Minimum bias | High variance, slow, rarely used |\n\n**Rule of thumb**: K=5 or K=10 works well for most problems."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Use full dataset for CV\n",
    "cv_scores_ols = cross_val_score(LinearRegression(), X, y, cv=5, scoring='r2')\n",
    "cv_scores_ridge = cross_val_score(Ridge(alpha=1.0), X, y, cv=5, scoring='r2')\n",
    "cv_scores_lasso = cross_val_score(Lasso(alpha=0.1, max_iter=10000), X, y, cv=5, scoring='r2')\n",
    "\n",
    "print(\"5-Fold Cross-Validation R\u00b2 Scores:\")\n",
    "print(f\"  OLS:   {cv_scores_ols.mean():.4f} (+/- {cv_scores_ols.std()*2:.4f})\")\n",
    "print(f\"  Ridge: {cv_scores_ridge.mean():.4f} (+/- {cv_scores_ridge.std()*2:.4f})\")\n",
    "print(f\"  Lasso: {cv_scores_lasso.mean():.4f} (+/- {cv_scores_lasso.std()*2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CV process\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 2))\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "    fold_array = np.zeros(len(X))\n",
    "    fold_array[train_idx] = 1  # Training = 1\n",
    "    fold_array[test_idx] = 2   # Test = 2\n",
    "    \n",
    "    axes[i].imshow([fold_array], aspect='auto', cmap='RdYlGn')\n",
    "    axes[i].set_title(f'Fold {i+1}')\n",
    "    axes[i].set_yticks([])\n",
    "    axes[i].set_xlabel('Sample Index')\n",
    "\n",
    "plt.suptitle('5-Fold Cross-Validation (Green=Train, Red=Test)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} hyperparameter tuning\n```\n\n\n## Hyperparameter Tuning with CV\n\nUse cross-validation to find the optimal regularization strength."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RidgeCV: automatically finds best alpha\n",
    "alphas = np.logspace(-4, 4, 50)\n",
    "\n",
    "ridge_cv = RidgeCV(alphas=alphas, cv=5)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Ridge alpha: {ridge_cv.alpha_:.4f}\")\n",
    "print(f\"Test R\u00b2: {ridge_cv.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LassoCV: automatically finds best alpha\n",
    "lasso_cv = LassoCV(alphas=np.logspace(-4, 1, 50), cv=5, max_iter=10000)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Lasso alpha: {lasso_cv.alpha_:.4f}\")\n",
    "print(f\"Test R\u00b2: {lasso_cv.score(X_test, y_test):.4f}\")\n",
    "print(f\"Non-zero coefficients: {np.sum(lasso_cv.coef_ != 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize alpha selection for Lasso\n",
    "alphas_lasso = np.logspace(-4, 1, 50)\n",
    "mse_path = []\n",
    "\n",
    "for alpha in alphas_lasso:\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    scores = -cross_val_score(lasso, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    mse_path.append(scores.mean())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(alphas_lasso, mse_path, 'o-')\n",
    "plt.axvline(x=lasso_cv.alpha_, color='r', linestyle='--', label=f'Best \u03b1 = {lasso_cv.alpha_:.4f}')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Cross-Validation MSE')\n",
    "plt.title('Lasso: Alpha Selection via Cross-Validation')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} GridSearchCV\n```\n\n\n## Grid Search for Multiple Hyperparameters\n\nWhen you have multiple hyperparameters, use GridSearchCV."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Grid search for ElasticNet\n",
    "param_grid = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1.0],\n",
    "    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "}\n",
    "\n",
    "elastic = ElasticNet(max_iter=10000)\n",
    "grid_search = GridSearchCV(elastic, param_grid, cv=5, scoring='r2')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Test score: {grid_search.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize grid search results\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "pivot = results.pivot_table(\n",
    "    values='mean_test_score',\n",
    "    index='param_l1_ratio',\n",
    "    columns='param_alpha'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(pivot.values, cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='Mean CV R\u00b2')\n",
    "plt.xticks(range(len(pivot.columns)), pivot.columns)\n",
    "plt.yticks(range(len(pivot.index)), pivot.index)\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('L1 Ratio')\n",
    "plt.title('ElasticNet Grid Search Results')\n",
    "\n",
    "# Add values\n",
    "for i in range(len(pivot.index)):\n",
    "    for j in range(len(pivot.columns)):\n",
    "        plt.text(j, i, f'{pivot.values[i, j]:.3f}', ha='center', va='center', \n",
    "                 color='white' if pivot.values[i, j] < 0.7 else 'black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines for Reproducible Workflows\n",
    "\n",
    "Combine preprocessing and modeling into a single pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline example\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('lasso', Lasso(alpha=0.1, max_iter=10000))\n",
    "])\n",
    "\n",
    "# Cross-validate the entire pipeline\n",
    "scores = cross_val_score(pipeline, X, y, cv=5, scoring='r2')\n",
    "print(f\"Pipeline CV R\u00b2: {scores.mean():.4f} (+/- {scores.std()*2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search with pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', Ridge())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__alpha': np.logspace(-2, 4, 20)\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best alpha: {grid.best_params_['model__alpha']:.4f}\")\n",
    "print(f\"Test R\u00b2: {grid.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all methods\n",
    "models = {\n",
    "    'OLS': LinearRegression(),\n",
    "    'Ridge (CV)': RidgeCV(alphas=np.logspace(-4, 4, 50), cv=5),\n",
    "    'Lasso (CV)': LassoCV(alphas=np.logspace(-4, 1, 50), cv=5, max_iter=10000),\n",
    "    'ElasticNet (CV)': ElasticNetCV(l1_ratio=[0.1, 0.5, 0.9], cv=5, max_iter=10000)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    train_score = model.score(X_train, y_train)\n",
    "    test_score = model.score(X_test, y_test)\n",
    "    \n",
    "    # Count non-zero coefficients\n",
    "    n_nonzero = np.sum(model.coef_ != 0)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Train R\u00b2': train_score,\n",
    "        'Test R\u00b2': test_score,\n",
    "        'Overfit Gap': train_score - test_score,\n",
    "        'Non-zero Coefs': n_nonzero\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Test Your Knowledge",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "! pip install -q jupyterquiz\nfrom jupyterquiz import display_quiz\n\ndisplay_quiz(\"https://raw.githubusercontent.com/jkitchin/s26-06642/main/dsmles/08-regularization-model-selection/quizzes/regularization-quiz.json\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Recommended Reading\n\nThese resources cover regularization theory and model selection best practices:\n\n1. **[An Introduction to Statistical Learning, Chapter 6](https://www.statlearning.com/)** - Comprehensive coverage of Ridge, Lasso, and model selection. Includes the geometric intuition for why L1 produces sparsity.\n\n2. **[Scikit-learn Model Selection](https://scikit-learn.org/stable/model_selection.html)** - Official documentation on cross-validation, grid search, and hyperparameter tuning. Essential reference for practical implementation.\n\n3. **[Regularization for Machine Learning (Ng, Stanford CS229)](https://cs229.stanford.edu/notes2022fall/main_notes.pdf)** - Course notes explaining the bias-variance tradeoff and how regularization addresses overfitting. Clear mathematical treatment.\n\n4. **[The Elements of Statistical Learning, Chapter 3](https://hastie.su.domains/ElemStatLearn/)** - Advanced treatment of shrinkage methods including the relationship between Ridge, Lasso, and subset selection. Free PDF available.\n\n5. **[Cross-Validation Pitfalls (Hastie & Efron)](https://web.stanford.edu/~hastie/TALKS/cv.pdf)** - Discusses common mistakes in cross-validation, including data leakage and proper nested CV. Important for avoiding subtle errors.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary: Regularization and Model Selection\n\n### The Regularization Toolkit\n\n| Method | Penalty | Effect | Use When |\n|--------|---------|--------|----------|\n| **Ridge** | L2 (squared) | Shrinks all coefficients | Multicollinearity, stability |\n| **Lasso** | L1 (absolute) | Some coefficients = 0 | Feature selection, sparsity |\n| **ElasticNet** | L1 + L2 | Combines both | Correlated features + selection |\n\n### Key Decisions\n\n| Decision | Guidance |\n|----------|----------|\n| Ridge vs Lasso? | Lasso if you want feature selection; Ridge if all features might matter |\n| How to choose \u03b1? | Cross-validation (RidgeCV, LassoCV) |\n| How many folds? | 5-10 is typical |\n| Grid search exhaustive? | Use RandomizedSearchCV for many hyperparameters |\n\n### The Model Selection Workflow\n\n1. **Start with cross-validation** for reliable estimates\n2. **Use *CV variants** (RidgeCV, LassoCV) for automatic hyperparameter tuning\n3. **Use GridSearchCV** when you have multiple hyperparameters\n4. **Use Pipelines** to combine preprocessing + modeling + tuning\n\n### Common Pitfalls\n\n- **Tuning on test data**: Never! Use cross-validation for tuning, keep test set for final evaluation\n- **Forgetting to scale**: Regularization penalizes magnitude\u2014scale features first!\n- **Ignoring ElasticNet**: It's often better than pure Lasso with correlated features\n- **Not using pipelines**: Risk of data leakage when scaling/transforming\n\n## Next Steps\n\nIn the next module, we'll explore nonlinear methods (polynomial features, SVR) for when linear models aren't enough."
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## The Catalyst Crisis: Chapter 8 - \"The Danger of Being Too Clever\"\n\n*A story about overfitting, regularization, and learning from struggle*\n\n---\n\nJordan was still in the lab at 11 PM when Alex arrived.\n\nHe startled at the door. \"I didn't hear you come in.\"\n\n\"Couldn't sleep. Thought I'd review some results.\" Alex noticed the dark circles under Jordan's eyes, the forest of crumpled sticky notes around his laptop. \"How long have you been here?\"\n\n\"I don't know. A while.\" He gestured at his screen. \"I built this model\u2014polynomial features, degree four, all the interactions. R-squared of 0.97 on training data.\"\n\nAlex sat down beside him. \"What about test data?\"\n\nJordan was silent for a moment. \"0.41.\"\n\n\"Ah.\"\n\n\"I don't understand. It fits the training data almost perfectly. But when I give it new data...\" He trailed off.\n\nAlex had made this exact mistake, three weeks ago. It was strangely comforting to see someone else struggling with it too.\n\n\"You're overfitting,\" she said gently. \"The model memorized the training data instead of learning the underlying pattern. It's like studying for a test by memorizing the answers to last year's questions. Works great until the questions change.\"\n\n\"So what do I do?\"\n\n\"Regularization.\" Alex pulled up her own work. \"Ridge and Lasso. They add a penalty for model complexity. The model has to earn each feature\u2014if a variable isn't pulling its weight, it gets shrunk toward zero.\"\n\nThey worked together through the night, rebuilding Jordan's model with L1 regularization. The training R-squared dropped to 0.82\u2014less impressive on paper. But the test R-squared climbed to 0.79. The gap nearly closed.\n\n\"It's... less good on training data,\" Jordan said uncertainly.\n\n\"But more honest. More generalizable.\" Alex pointed at the coefficient plot. \"Look\u2014Lasso zeroed out twenty features. They weren't helping; they were fitting noise.\"\n\n\"How do you know which features to trust?\"\n\n\"The ones that survive regularization. The ones that show up consistently across cross-validation folds. The ones that make physical sense.\" She highlighted the non-zero coefficients. \"Catalyst age. Temperature. Pressure. The basics. Everything else was distraction.\"\n\nJordan leaned back, exhaustion and relief mingling on his face. \"You too? The struggling, I mean?\"\n\n\"Everyone. Maya's been up late debugging. Sam nearly quit after their PCA mistake. We're all figuring it out.\"\n\n\"Nobody talks about it.\"\n\n\"I know. We should.\" Alex stood to leave, then paused. \"Same time tomorrow? I'm working on the classification threshold problem\u2014could use a second set of eyes.\"\n\nJordan nodded. \"Thanks, Alex.\"\n\nShe added to the mystery board: **Simpler models generalize better. Regularization keeps only what matters: catalyst, temperature, pressure.**\n\n---\n\n*Continue to the next lecture to explore nonlinear methods...*",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}