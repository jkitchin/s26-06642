{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jkitchin/s26-06642/blob/main/dsmles/08-regularization-model-selection/regularization-model-selection.ipynb)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 06: Regularization and Model Selection\n",
    "\n",
    "Preventing overfitting and choosing the best model.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand overfitting and the bias-variance tradeoff\n",
    "2. Apply Ridge, Lasso, and ElasticNet regularization\n",
    "3. Use cross-validation for model selection\n",
    "4. Tune hyperparameters systematically\n",
    "5. Compare models fairly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## The Overfitting Problem: Why Complex Models Fail\n\nOverfitting is the central challenge in machine learning. Understanding it is crucial.\n\n### What Is Overfitting?\n\nA model that **memorizes the training data** instead of learning generalizable patterns. It fits the noise, not the signal.\n\n### The Bias-Variance Tradeoff\n\nEvery model makes two types of errors:\n\n**Bias** (underfitting): Error from oversimplifying. A linear model can't capture nonlinear relationships, no matter how much data you have.\n\n**Variance** (overfitting): Error from being too sensitive to training data. A very flexible model will fit different training sets very differently.\n\n| Model Complexity | Bias | Variance | Result |\n|------------------|------|----------|--------|\n| Too simple | High | Low | Underfitting: misses patterns |\n| Just right | Medium | Medium | Generalizes well |\n| Too complex | Low | High | Overfitting: memorizes noise |\n\n### When Overfitting Happens\n\nYou're at risk when:\n- **Many features, few samples**: More parameters than data points to constrain them\n- **Features are correlated**: Multiple ways to explain the same variance\n- **Model is very flexible**: High-degree polynomials, deep trees, etc.\n- **No regularization**: Nothing preventing the model from fitting noise"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with many features (some irrelevant)\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "n_features = 50  # Many features, some irrelevant\n",
    "\n",
    "# Only first 5 features are truly predictive\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "true_coef = np.zeros(n_features)\n",
    "true_coef[:5] = [2.0, -1.5, 1.0, 0.5, -0.8]  # Only 5 non-zero\n",
    "\n",
    "y = X @ true_coef + np.random.normal(0, 0.5, n_samples)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Number of features: {n_features}\")\n",
    "print(f\"Number of truly predictive features: 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinary Least Squares (OLS) - no regularization\n",
    "ols = LinearRegression()\n",
    "ols.fit(X_train, y_train)\n",
    "\n",
    "print(\"OLS (No Regularization):\")\n",
    "print(f\"  Training R²: {ols.score(X_train, y_train):.4f}\")\n",
    "print(f\"  Test R²: {ols.score(X_test, y_test):.4f}\")\n",
    "print(f\"  Gap: {ols.score(X_train, y_train) - ols.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare OLS coefficients to true coefficients\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# True coefficients\n",
    "axes[0].bar(range(n_features), true_coef, edgecolor='black')\n",
    "axes[0].set_xlabel('Feature Index')\n",
    "axes[0].set_ylabel('Coefficient')\n",
    "axes[0].set_title('True Coefficients (Only 5 are non-zero)')\n",
    "axes[0].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# OLS coefficients\n",
    "axes[1].bar(range(n_features), ols.coef_, edgecolor='black')\n",
    "axes[1].set_xlabel('Feature Index')\n",
    "axes[1].set_ylabel('Coefficient')\n",
    "axes[1].set_title('OLS Estimated Coefficients (Many non-zero!)')\n",
    "axes[1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Ridge Regression (L2 Regularization): Shrink, Don't Select\n\nRidge regression adds a penalty on the sum of squared coefficients:\n\n$$\\min_\\beta \\|y - X\\beta\\|^2 + \\alpha \\|\\beta\\|^2$$\n\n### The Key Insight\n\nRidge **shrinks** coefficients toward zero but never exactly to zero. All features stay in the model, just with smaller effects.\n\n### When to Use Ridge\n\n- **Multicollinearity**: Correlated features cause unstable OLS coefficients. Ridge stabilizes them.\n- **Many features**: Even if all features might be relevant, you want smaller, more stable coefficients.\n- **Prediction focus**: You care more about prediction accuracy than identifying which features matter.\n\n### The Alpha Parameter\n\n- **α = 0**: Pure OLS, no regularization\n- **α → ∞**: All coefficients shrink to zero\n- **Optimal α**: Found via cross-validation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "print(\"Ridge Regression (alpha=1.0):\")\n",
    "print(f\"  Training R²: {ridge.score(X_train, y_train):.4f}\")\n",
    "print(f\"  Test R²: {ridge.score(X_test, y_test):.4f}\")\n",
    "print(f\"  Gap: {ridge.score(X_train, y_train) - ridge.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of alpha on coefficients\n",
    "alphas = np.logspace(-2, 4, 50)\n",
    "ridge_coefs = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    ridge_coefs.append(ridge.coef_)\n",
    "\n",
    "ridge_coefs = np.array(ridge_coefs)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(10):  # Plot first 10 features\n",
    "    color = 'red' if i < 5 else 'gray'  # Red for true predictors\n",
    "    alpha_val = 0.8 if i < 5 else 0.3\n",
    "    plt.semilogx(alphas, ridge_coefs[:, i], color=color, alpha=alpha_val)\n",
    "\n",
    "plt.xlabel('Alpha (Regularization Strength)')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Ridge: Coefficients vs Alpha (red = true predictors)')\n",
    "plt.axhline(y=0, color='black', linestyle='--', linewidth=0.5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Lasso Regression (L1 Regularization): Shrink AND Select\n\nLasso adds a penalty on the sum of absolute coefficients:\n\n$$\\min_\\beta \\|y - X\\beta\\|^2 + \\alpha \\|\\beta\\|_1$$\n\n### The Key Insight\n\nThe L1 penalty has a remarkable property: it drives some coefficients **exactly to zero**. Lasso performs automatic feature selection!\n\n### Why L1 Creates Sparsity (Intuition)\n\nImagine you're trying to reduce the total \"cost\" of coefficients. With L2 (squared), reducing a large coefficient saves more than eliminating a small one. With L1 (absolute), eliminating a coefficient entirely saves just as much per unit as shrinking a large one. The optimizer often chooses to eliminate.\n\n### When to Use Lasso\n\n- **Feature selection needed**: You suspect many features are irrelevant\n- **Interpretability**: You want a sparse model with only important features\n- **High dimensionality**: When p > n (more features than samples)\n\n### The Tradeoff\n\nLasso can be unstable with correlated features—it might arbitrarily pick one and zero out the others. If you need stable selection of correlated features, consider ElasticNet."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso regression\n",
    "lasso = Lasso(alpha=0.1, max_iter=10000)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "print(\"Lasso Regression (alpha=0.1):\")\n",
    "print(f\"  Training R²: {lasso.score(X_train, y_train):.4f}\")\n",
    "print(f\"  Test R²: {lasso.score(X_test, y_test):.4f}\")\n",
    "print(f\"  Non-zero coefficients: {np.sum(lasso.coef_ != 0)} out of {n_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Lasso to true coefficients\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# True coefficients\n",
    "axes[0].bar(range(n_features), true_coef, edgecolor='black')\n",
    "axes[0].set_xlabel('Feature Index')\n",
    "axes[0].set_ylabel('Coefficient')\n",
    "axes[0].set_title('True Coefficients')\n",
    "\n",
    "# Lasso coefficients\n",
    "axes[1].bar(range(n_features), lasso.coef_, edgecolor='black')\n",
    "axes[1].set_xlabel('Feature Index')\n",
    "axes[1].set_ylabel('Coefficient')\n",
    "axes[1].set_title('Lasso Estimated Coefficients (Sparse!)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElasticNet (Combined L1 + L2)\n",
    "\n",
    "ElasticNet combines both penalties:\n",
    "\n",
    "$$\\min_\\beta \\|y - X\\beta\\|^2 + \\alpha \\cdot l1\\_ratio \\cdot \\|\\beta\\|_1 + \\alpha \\cdot (1 - l1\\_ratio) \\cdot \\|\\beta\\|^2$$\n",
    "\n",
    "- `l1_ratio=1`: Pure Lasso\n",
    "- `l1_ratio=0`: Pure Ridge\n",
    "- `l1_ratio=0.5`: Balanced mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet\n",
    "elastic = ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000)\n",
    "elastic.fit(X_train, y_train)\n",
    "\n",
    "print(\"ElasticNet (alpha=0.1, l1_ratio=0.5):\")\n",
    "print(f\"  Training R²: {elastic.score(X_train, y_train):.4f}\")\n",
    "print(f\"  Test R²: {elastic.score(X_test, y_test):.4f}\")\n",
    "print(f\"  Non-zero coefficients: {np.sum(elastic.coef_ != 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cross-Validation: The Right Way to Evaluate Models\n\nA single train/test split is noisy. The test performance depends on which points happened to be in the test set. Cross-validation gives more reliable estimates.\n\n### How K-Fold Cross-Validation Works\n\n1. Split data into k equal parts (folds)\n2. For each fold:\n   - Train on k-1 folds\n   - Test on the remaining fold\n3. Average the k test scores\n\n### Why It's Better\n\n- Uses all data for both training and testing\n- Provides error bars (standard deviation across folds)\n- Less sensitive to the random split\n\n### Choosing K\n\n| K | Pros | Cons |\n|---|------|------|\n| 5 | Fast, reasonable variance estimate | Some bias if dataset is small |\n| 10 | Good balance, common default | Slower than 5-fold |\n| n (Leave-One-Out) | Minimum bias | High variance, slow, rarely used |\n\n**Rule of thumb**: K=5 or K=10 works well for most problems."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Use full dataset for CV\n",
    "cv_scores_ols = cross_val_score(LinearRegression(), X, y, cv=5, scoring='r2')\n",
    "cv_scores_ridge = cross_val_score(Ridge(alpha=1.0), X, y, cv=5, scoring='r2')\n",
    "cv_scores_lasso = cross_val_score(Lasso(alpha=0.1, max_iter=10000), X, y, cv=5, scoring='r2')\n",
    "\n",
    "print(\"5-Fold Cross-Validation R² Scores:\")\n",
    "print(f\"  OLS:   {cv_scores_ols.mean():.4f} (+/- {cv_scores_ols.std()*2:.4f})\")\n",
    "print(f\"  Ridge: {cv_scores_ridge.mean():.4f} (+/- {cv_scores_ridge.std()*2:.4f})\")\n",
    "print(f\"  Lasso: {cv_scores_lasso.mean():.4f} (+/- {cv_scores_lasso.std()*2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CV process\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 2))\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "    fold_array = np.zeros(len(X))\n",
    "    fold_array[train_idx] = 1  # Training = 1\n",
    "    fold_array[test_idx] = 2   # Test = 2\n",
    "    \n",
    "    axes[i].imshow([fold_array], aspect='auto', cmap='RdYlGn')\n",
    "    axes[i].set_title(f'Fold {i+1}')\n",
    "    axes[i].set_yticks([])\n",
    "    axes[i].set_xlabel('Sample Index')\n",
    "\n",
    "plt.suptitle('5-Fold Cross-Validation (Green=Train, Red=Test)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with CV\n",
    "\n",
    "Use cross-validation to find the optimal regularization strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RidgeCV: automatically finds best alpha\n",
    "alphas = np.logspace(-4, 4, 50)\n",
    "\n",
    "ridge_cv = RidgeCV(alphas=alphas, cv=5)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Ridge alpha: {ridge_cv.alpha_:.4f}\")\n",
    "print(f\"Test R²: {ridge_cv.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LassoCV: automatically finds best alpha\n",
    "lasso_cv = LassoCV(alphas=np.logspace(-4, 1, 50), cv=5, max_iter=10000)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Lasso alpha: {lasso_cv.alpha_:.4f}\")\n",
    "print(f\"Test R²: {lasso_cv.score(X_test, y_test):.4f}\")\n",
    "print(f\"Non-zero coefficients: {np.sum(lasso_cv.coef_ != 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize alpha selection for Lasso\n",
    "alphas_lasso = np.logspace(-4, 1, 50)\n",
    "mse_path = []\n",
    "\n",
    "for alpha in alphas_lasso:\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    scores = -cross_val_score(lasso, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    mse_path.append(scores.mean())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(alphas_lasso, mse_path, 'o-')\n",
    "plt.axvline(x=lasso_cv.alpha_, color='r', linestyle='--', label=f'Best α = {lasso_cv.alpha_:.4f}')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Cross-Validation MSE')\n",
    "plt.title('Lasso: Alpha Selection via Cross-Validation')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search for Multiple Hyperparameters\n",
    "\n",
    "When you have multiple hyperparameters, use GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Grid search for ElasticNet\n",
    "param_grid = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1.0],\n",
    "    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "}\n",
    "\n",
    "elastic = ElasticNet(max_iter=10000)\n",
    "grid_search = GridSearchCV(elastic, param_grid, cv=5, scoring='r2')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Test score: {grid_search.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize grid search results\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "pivot = results.pivot_table(\n",
    "    values='mean_test_score',\n",
    "    index='param_l1_ratio',\n",
    "    columns='param_alpha'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(pivot.values, cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='Mean CV R²')\n",
    "plt.xticks(range(len(pivot.columns)), pivot.columns)\n",
    "plt.yticks(range(len(pivot.index)), pivot.index)\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('L1 Ratio')\n",
    "plt.title('ElasticNet Grid Search Results')\n",
    "\n",
    "# Add values\n",
    "for i in range(len(pivot.index)):\n",
    "    for j in range(len(pivot.columns)):\n",
    "        plt.text(j, i, f'{pivot.values[i, j]:.3f}', ha='center', va='center', \n",
    "                 color='white' if pivot.values[i, j] < 0.7 else 'black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines for Reproducible Workflows\n",
    "\n",
    "Combine preprocessing and modeling into a single pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline example\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('lasso', Lasso(alpha=0.1, max_iter=10000))\n",
    "])\n",
    "\n",
    "# Cross-validate the entire pipeline\n",
    "scores = cross_val_score(pipeline, X, y, cv=5, scoring='r2')\n",
    "print(f\"Pipeline CV R²: {scores.mean():.4f} (+/- {scores.std()*2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search with pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', Ridge())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__alpha': np.logspace(-2, 4, 20)\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best alpha: {grid.best_params_['model__alpha']:.4f}\")\n",
    "print(f\"Test R²: {grid.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all methods\n",
    "models = {\n",
    "    'OLS': LinearRegression(),\n",
    "    'Ridge (CV)': RidgeCV(alphas=np.logspace(-4, 4, 50), cv=5),\n",
    "    'Lasso (CV)': LassoCV(alphas=np.logspace(-4, 1, 50), cv=5, max_iter=10000),\n",
    "    'ElasticNet (CV)': ElasticNetCV(l1_ratio=[0.1, 0.5, 0.9], cv=5, max_iter=10000)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    train_score = model.score(X_train, y_train)\n",
    "    test_score = model.score(X_test, y_test)\n",
    "    \n",
    "    # Count non-zero coefficients\n",
    "    n_nonzero = np.sum(model.coef_ != 0)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Train R²': train_score,\n",
    "        'Test R²': test_score,\n",
    "        'Overfit Gap': train_score - test_score,\n",
    "        'Non-zero Coefs': n_nonzero\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary: Regularization and Model Selection\n\n### The Regularization Toolkit\n\n| Method | Penalty | Effect | Use When |\n|--------|---------|--------|----------|\n| **Ridge** | L2 (squared) | Shrinks all coefficients | Multicollinearity, stability |\n| **Lasso** | L1 (absolute) | Some coefficients = 0 | Feature selection, sparsity |\n| **ElasticNet** | L1 + L2 | Combines both | Correlated features + selection |\n\n### Key Decisions\n\n| Decision | Guidance |\n|----------|----------|\n| Ridge vs Lasso? | Lasso if you want feature selection; Ridge if all features might matter |\n| How to choose α? | Cross-validation (RidgeCV, LassoCV) |\n| How many folds? | 5-10 is typical |\n| Grid search exhaustive? | Use RandomizedSearchCV for many hyperparameters |\n\n### The Model Selection Workflow\n\n1. **Start with cross-validation** for reliable estimates\n2. **Use *CV variants** (RidgeCV, LassoCV) for automatic hyperparameter tuning\n3. **Use GridSearchCV** when you have multiple hyperparameters\n4. **Use Pipelines** to combine preprocessing + modeling + tuning\n\n### Common Pitfalls\n\n- **Tuning on test data**: Never! Use cross-validation for tuning, keep test set for final evaluation\n- **Forgetting to scale**: Regularization penalizes magnitude—scale features first!\n- **Ignoring ElasticNet**: It's often better than pure Lasso with correlated features\n- **Not using pipelines**: Risk of data leakage when scaling/transforming\n\n## Next Steps\n\nIn the next module, we'll explore nonlinear methods (polynomial features, SVR) for when linear models aren't enough."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}