{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jkitchin/s26-06642/blob/main/dsmles/11-clustering/clustering.ipynb)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "! pip install -q pycse\nfrom pycse.colab import pdf",
   "metadata": {
    "tags": [
     "skip-execution",
     "remove-cell"
    ]
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "{index} clustering, unsupervised learning\n\n# Module 09: Clustering\n\nUnsupervised learning to discover patterns in data.\n\n## Learning Objectives\n\n1. Understand clustering as unsupervised learning\n2. Apply k-means clustering\n3. Use hierarchical clustering with dendrograms\n4. Apply DBSCAN for density-based clustering\n5. Evaluate and compare clustering results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Unsupervised Learning: Finding Hidden Structure\n\nEverything we've done so far has been **supervised learning**: we had labels (y) to train against. Clustering is different—it's **unsupervised**. We only have features (X) and want to discover natural groups.\n\n### Why Clustering Matters in Chemical Engineering\n\n| Application | What You're Clustering | Why |\n|-------------|----------------------|-----|\n| Material discovery | Material properties | Find families of similar materials |\n| Process monitoring | Sensor readings | Identify operating regimes |\n| Fault detection | Process variables | Distinguish normal vs abnormal states |\n| Customer segmentation | Usage patterns | Group similar use cases |\n| Literature mining | Document features | Find related papers |\n\n### The Fundamental Challenge\n\nWithout labels, how do you know if clustering worked? This is the deepest question in unsupervised learning:\n\n- **No ground truth**: You can't calculate accuracy\n- **Multiple valid solutions**: Different algorithms find different structure\n- **Subjective evaluation**: What counts as a \"good\" cluster?\n\n### Key Insight\n\nClustering doesn't find \"the\" structure—it finds **a** structure. The algorithm's assumptions determine what it finds. Choose algorithms based on what kind of clusters you expect."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create material property dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "# Three types of polymers with different properties\n",
    "n_per_type = 50\n",
    "\n",
    "# Type 1: High strength, low flexibility\n",
    "type1 = np.column_stack([\n",
    "    np.random.normal(100, 10, n_per_type),  # Tensile strength (MPa)\n",
    "    np.random.normal(20, 5, n_per_type),    # Elongation (%)\n",
    "    np.random.normal(3.5, 0.3, n_per_type), # Density (g/cm³)\n",
    "    np.random.normal(150, 15, n_per_type),  # Melting point (°C)\n",
    "    np.random.normal(80, 10, n_per_type)    # Hardness\n",
    "])\n",
    "\n",
    "# Type 2: Low strength, high flexibility (rubber-like)\n",
    "type2 = np.column_stack([\n",
    "    np.random.normal(30, 8, n_per_type),\n",
    "    np.random.normal(300, 50, n_per_type),\n",
    "    np.random.normal(1.2, 0.1, n_per_type),\n",
    "    np.random.normal(80, 10, n_per_type),\n",
    "    np.random.normal(40, 8, n_per_type)\n",
    "])\n",
    "\n",
    "# Type 3: Medium properties\n",
    "type3 = np.column_stack([\n",
    "    np.random.normal(60, 12, n_per_type),\n",
    "    np.random.normal(100, 20, n_per_type),\n",
    "    np.random.normal(2.0, 0.2, n_per_type),\n",
    "    np.random.normal(120, 12, n_per_type),\n",
    "    np.random.normal(60, 10, n_per_type)\n",
    "])\n",
    "\n",
    "X = np.vstack([type1, type2, type3])\n",
    "true_labels = np.array([0]*n_per_type + [1]*n_per_type + [2]*n_per_type)\n",
    "\n",
    "feature_names = ['tensile_strength', 'elongation', 'density', 'melting_point', 'hardness']\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data (important for clustering)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Visualize with PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=true_labels, cmap='viridis', alpha=0.6)\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "plt.title('Material Properties (True Labels)')\n",
    "plt.colorbar(label='True Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "{index} K-means, centroid\n\n## K-Means: The Workhorse Algorithm\n\nK-means is the most widely used clustering algorithm. It's fast, simple, and often works well.\n\n### How It Works\n\n1. Pick k random points as initial centroids\n2. Assign each point to the nearest centroid\n3. Move each centroid to the mean of its assigned points\n4. Repeat until centroids stop moving\n\n### The Key Assumption: Spherical Clusters\n\nK-means assumes clusters are:\n- **Spherical**: Similar extent in all directions\n- **Similar size**: Roughly equal number of points\n- **Well-separated**: Clear gaps between clusters\n\nWhen these assumptions hold, k-means is excellent. When they don't, consider alternatives.\n\n### The k Problem\n\nYou must specify k (number of clusters) in advance. This is a fundamental limitation—how do you know how many clusters exist?\n\nCommon approaches:\n- **Domain knowledge**: You know there are 3 catalyst types\n- **Elbow method**: Plot inertia vs k, look for diminishing returns\n- **Silhouette score**: Measure cluster quality for each k"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means with k=3\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_labels, cmap='viridis', alpha=0.6)\n",
    "\n",
    "# Plot centroids\n",
    "centroids_pca = pca.transform(kmeans.cluster_centers_)\n",
    "plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1], c='red', marker='X', \n",
    "            s=200, edgecolors='black', linewidths=2, label='Centroids')\n",
    "\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('K-Means Clustering (k=3)')\n",
    "plt.legend()\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "{index} elbow method, silhouette score\n\n## Choosing the Number of Clusters\n\nTwo common methods:\n1. **Elbow method**: Plot inertia vs k, look for \"elbow\"\n2. **Silhouette score**: Measure how well points fit their cluster"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method\n",
    "k_range = range(1, 11)\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    if k > 1:\n",
    "        silhouettes.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Elbow plot\n",
    "axes[0].plot(k_range, inertias, 'o-')\n",
    "axes[0].set_xlabel('Number of Clusters (k)')\n",
    "axes[0].set_ylabel('Inertia')\n",
    "axes[0].set_title('Elbow Method')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Silhouette plot\n",
    "axes[1].plot(range(2, 11), silhouettes, 'o-')\n",
    "axes[1].set_xlabel('Number of Clusters (k)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('Silhouette Analysis')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best k by silhouette: {np.argmax(silhouettes) + 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**Interpreting these plots:**\n\n**Elbow plot (left)**: Shows inertia (within-cluster sum of squares) vs k. We're looking for an \"elbow\"—where adding more clusters gives diminishing returns. Here, the elbow is around k=3, after which the curve flattens. This suggests 3 clusters is a natural choice.\n\n**Silhouette plot (right)**: Silhouette score measures how well each point fits its cluster vs neighboring clusters. Values range from -1 (wrong cluster) to +1 (perfect fit). The peak at k=3 confirms our choice.\n\n**Why k=3?** We created data with 3 polymer types, so finding 3 clusters validates that:\n1. Our synthetic data has the structure we intended\n2. K-means can recover that structure\n3. Both selection methods agree\n\nIn real data, you won't know the \"true\" k. Use these plots to guide your choice, but remember: the best k depends on your application, not just the statistics.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "{index} hierarchical clustering, dendrogram\n\n## Hierarchical Clustering: See the Full Tree\n\nHierarchical clustering builds a tree (dendrogram) showing how clusters merge at different scales. This gives you more information than a single partition.\n\n### Two Approaches\n\n| Type | Process | Advantage |\n|------|---------|-----------|\n| **Agglomerative** | Start with n clusters, merge pairs | More common, flexible |\n| **Divisive** | Start with 1 cluster, split | Less common |\n\n### The Dendrogram: A Roadmap of Clusters\n\nThe dendrogram shows:\n- **Y-axis**: Distance at which clusters merge\n- **X-axis**: Individual samples (leaves)\n- **Horizontal lines**: Where clusters join\n\nCut the dendrogram at different heights to get different numbers of clusters. A big gap suggests a natural clustering.\n\n### When to Use Hierarchical Clustering\n\n- Small to medium datasets (slow for large n)\n- You want to see cluster structure at multiple scales\n- You don't know k in advance\n- You want interpretable cluster relationships"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute linkage for dendrogram\n",
    "linkage_matrix = linkage(X_scaled, method='ward')\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "dendrogram(linkage_matrix, truncate_mode='lastp', p=30)\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.axhline(y=15, color='r', linestyle='--', label='Cut at 3 clusters')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agglomerative clustering with 3 clusters\n",
    "agg = AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
    "agg_labels = agg.fit_predict(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=agg_labels, cmap='viridis', alpha=0.6)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Agglomerative Clustering')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "{index} DBSCAN, eps, min_samples\n\n## DBSCAN: Clusters of Arbitrary Shape\n\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) takes a completely different approach. Instead of assuming spherical clusters, it finds regions of high density.\n\n### The Key Idea\n\nA cluster is a dense region separated from other dense regions by sparse regions. Points in sparse regions are \"noise.\"\n\n### How It Works\n\n1. A point is a **core point** if it has ≥ min_samples neighbors within radius eps\n2. Points within eps of a core point belong to its cluster\n3. Core points within eps of each other are in the same cluster\n4. Non-core points with no nearby core points are **noise** (labeled -1)\n\n### Strengths of DBSCAN\n\n| K-means Limitation | DBSCAN Solution |\n|--------------------|-----------------|\n| Must specify k | Finds k automatically |\n| Spherical clusters only | Any shape works |\n| Sensitive to outliers | Identifies outliers as noise |\n| All points assigned | Points can be noise |\n\n### The Parameters\n\n- **eps**: Maximum distance between neighbors. Too small → too many noise points. Too large → clusters merge.\n- **min_samples**: Minimum points to form a core. Higher = fewer, denser clusters."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN\n",
    "dbscan = DBSCAN(eps=0.8, min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_noise = list(dbscan_labels).count(-1)\n",
    "\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "print(f\"Number of noise points: {n_noise}\")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=dbscan_labels, cmap='viridis', alpha=0.6)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title(f'DBSCAN Clustering (eps=0.8, {n_clusters} clusters, {n_noise} noise)')\n",
    "plt.colorbar(scatter, label='Cluster (-1 = noise)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**The eps parameter controls cluster granularity:**\n\n- **eps=0.5** (too small): Many tiny clusters or noise. The algorithm sees the data as too sparse—points don't have enough nearby neighbors to form clusters.\n\n- **eps=0.8** (about right): Recovers the 3 clusters with minimal noise. The radius is large enough to connect points within true clusters but small enough to keep clusters separate.\n\n- **eps=1.5** (too large): Clusters start merging. Points from different true groups are now considered neighbors.\n\n**How to choose eps in practice?**\n1. Plot the k-distance graph (distance to kth nearest neighbor for each point)\n2. Look for an \"elbow\" where distances jump\n3. The elbow distance is a good starting point for eps\n\n**The advantage over k-means**: DBSCAN found the same 3 clusters *without us specifying k=3*. It discovered the number of clusters from the data structure itself.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of eps parameter\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for ax, eps in zip(axes, [0.5, 0.8, 1.5]):\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=5)\n",
    "    labels = dbscan.fit_predict(X_scaled)\n",
    "    \n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = list(labels).count(-1)\n",
    "    \n",
    "    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', alpha=0.6)\n",
    "    ax.set_xlabel('PC1')\n",
    "    ax.set_ylabel('PC2')\n",
    "    ax.set_title(f'eps={eps}\\n{n_clusters} clusters, {n_noise} noise')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Clustering Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare methods\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# True labels\n",
    "axes[0, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=true_labels, cmap='viridis', alpha=0.6)\n",
    "axes[0, 0].set_title('True Labels')\n",
    "\n",
    "# K-means\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans.fit_predict(X_scaled)\n",
    "axes[0, 1].scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_labels, cmap='viridis', alpha=0.6)\n",
    "axes[0, 1].set_title(f'K-Means (Silhouette: {silhouette_score(X_scaled, kmeans_labels):.3f})')\n",
    "\n",
    "# Agglomerative\n",
    "agg = AgglomerativeClustering(n_clusters=3)\n",
    "agg_labels = agg.fit_predict(X_scaled)\n",
    "axes[1, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=agg_labels, cmap='viridis', alpha=0.6)\n",
    "axes[1, 0].set_title(f'Agglomerative (Silhouette: {silhouette_score(X_scaled, agg_labels):.3f})')\n",
    "\n",
    "# DBSCAN\n",
    "dbscan = DBSCAN(eps=0.8, min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(X_scaled)\n",
    "axes[1, 1].scatter(X_pca[:, 0], X_pca[:, 1], c=dbscan_labels, cmap='viridis', alpha=0.6)\n",
    "# Calculate silhouette only for non-noise points\n",
    "mask = dbscan_labels != -1\n",
    "if mask.sum() > 1 and len(set(dbscan_labels[mask])) > 1:\n",
    "    sil = silhouette_score(X_scaled[mask], dbscan_labels[mask])\n",
    "else:\n",
    "    sil = np.nan\n",
    "axes[1, 1].set_title(f'DBSCAN (Silhouette: {sil:.3f})')\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlabel('PC1')\n",
    "    ax.set_ylabel('PC2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to true labels with Adjusted Rand Index\n",
    "print(\"Adjusted Rand Index (1.0 = perfect match):\")\n",
    "print(f\"  K-Means: {adjusted_rand_score(true_labels, kmeans_labels):.3f}\")\n",
    "print(f\"  Agglomerative: {adjusted_rand_score(true_labels, agg_labels):.3f}\")\n",
    "\n",
    "# For DBSCAN, only compare non-noise points\n",
    "if mask.sum() > 0:\n",
    "    print(f\"  DBSCAN (non-noise): {adjusted_rand_score(true_labels[mask], dbscan_labels[mask]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Profiling\n",
    "\n",
    "Understanding what makes each cluster different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster labels to dataframe\n",
    "df['cluster'] = kmeans_labels\n",
    "\n",
    "# Profile each cluster\n",
    "cluster_profiles = df.groupby('cluster')[feature_names].mean()\n",
    "print(\"Cluster Profiles (Mean Values):\")\n",
    "print(cluster_profiles.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cluster profiles\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Normalize for visualization\n",
    "profiles_normalized = (cluster_profiles - cluster_profiles.min()) / (cluster_profiles.max() - cluster_profiles.min())\n",
    "\n",
    "x = np.arange(len(feature_names))\n",
    "width = 0.25\n",
    "\n",
    "for i, cluster in enumerate(profiles_normalized.index):\n",
    "    ax.bar(x + i*width, profiles_normalized.loc[cluster], width, \n",
    "           label=f'Cluster {cluster}', edgecolor='black')\n",
    "\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(feature_names, rotation=45, ha='right')\n",
    "ax.set_ylabel('Normalized Value')\n",
    "ax.set_title('Cluster Profiles')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Self-Assessment Quiz\n\nTest your understanding of clustering concepts.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "! pip install -q jupyterquiz\nfrom jupyterquiz import display_quiz\n\ndisplay_quiz(\"https://raw.githubusercontent.com/jkitchin/s26-06642/main/dsmles/11-clustering/quizzes/clustering-quiz.json\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Recommended Reading\n\nThese resources explore clustering algorithms and unsupervised learning:\n\n1. **[Scikit-learn Clustering](https://scikit-learn.org/stable/modules/clustering.html)** - Official documentation comparing all clustering algorithms. Includes guidance on algorithm selection and the clustering comparison chart.\n\n2. **[An Introduction to Statistical Learning, Chapter 12](https://www.statlearning.com/)** - Covers unsupervised learning including K-means, hierarchical clustering, and practical considerations for choosing the number of clusters.\n\n3. **[DBSCAN: A Density-Based Algorithm (Ester et al., KDD 1996)](https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf)** - The original DBSCAN paper. Short and readable, explains the core concepts of density-based clustering.\n\n4. **[Clustering Validation Indices (Halkidi et al., SIGMOD Record 2001)](https://dl.acm.org/doi/10.1145/568574.568575)** - Survey of methods for evaluating clustering quality without ground truth labels. Important for real-world applications.\n\n5. **[How HDBSCAN Works](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html)** - Excellent visual explanation of density-based clustering. While focused on HDBSCAN, it builds intuition applicable to DBSCAN as well.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary: Choosing a Clustering Algorithm\n\n### Decision Guide\n\n```\nDo you know the number of clusters?\n├── Yes, and clusters are roughly spherical\n│   └── K-means (fast, reliable)\n├── No, want to explore different k values\n│   └── Hierarchical (see dendrogram)\n└── No, expect irregular shapes or outliers\n    └── DBSCAN (density-based)\n```\n\n### Algorithm Comparison\n\n| Aspect | K-means | Hierarchical | DBSCAN |\n|--------|---------|--------------|--------|\n| Must specify k | Yes | No (cut dendrogram) | No (finds automatically) |\n| Cluster shapes | Spherical | Depends on linkage | Any |\n| Handles outliers | No | Somewhat | Yes (labels as noise) |\n| Scalability | Excellent | Poor (O(n²) memory) | Good |\n| Deterministic | No (random init) | Yes | Yes |\n\n### Key Takeaways\n\n1. **Always scale your data**: Clustering uses distances; unscaled features dominate\n2. **Visualize with PCA/UMAP first**: See if cluster structure is visible\n3. **Try multiple algorithms**: Different algorithms find different structure\n4. **Validate clusters**: Use silhouette score, but also domain knowledge\n5. **Profile your clusters**: Understand what makes each cluster distinct\n\n### Common Pitfalls\n\n- Forgetting to scale features\n- Choosing k arbitrarily without elbow/silhouette analysis\n- Using k-means for non-spherical clusters\n- Expecting clustering to find \"correct\" groups without ground truth\n- Not profiling clusters to understand what they represent\n\n## Next Steps\n\nIn the next module, we'll learn about uncertainty quantification—understanding how confident we should be in our predictions."
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## The Catalyst Crisis: Chapter 11 - \"Patterns Without Labels\"\n\n*A story about unsupervised learning and actionable intelligence*\n\n---\n\n\"Can you tell us which specific catalyst lots are bad?\"\n\nFrank's question was reasonable. The team had proven that catalyst was the problem. Now ChemCorp needed actionable intelligence—which lots to reject, which to accept.\n\nThe problem: they didn't have labels. No one had systematically tested catalyst quality. The only information was batch outcomes—and those depended on many factors, not just catalyst.\n\n\"We need unsupervised learning,\" Alex said. \"Let the catalyst properties cluster naturally, then map outcomes to clusters.\"\n\nShe built a feature matrix for each catalyst lot: chemical composition from the certificates of analysis, physical properties from incoming QC tests. Forty dimensions of catalyst data.\n\nK-means found three clusters. The mapping to yield was stark:\n\n- Cluster 1: 91% good batches\n- Cluster 2: 67% good batches\n- Cluster 3: 23% good batches\n\n\"Three grades of catalyst,\" Jordan observed. \"And nobody knew.\"\n\nBut which properties separated them? Alex dug into the cluster centers. The answer was subtle—not any single property, but a combination of trace metal ratios and surface area measurements that fell within spec individually but created problems together.\n\n\"They're all within specification,\" she explained to Frank. \"Every lot passes incoming QC. But the *combination* of properties matters, and your specs don't capture that.\"\n\nFrank leaned forward. \"So what do we do?\"\n\n\"You can use this clustering model to screen incoming catalyst. If a lot falls into Cluster 3, reject it or divert it to less sensitive products.\"\n\n\"Will the supplier cooperate?\"\n\n\"They will when you show them the data.\" Alex pulled up the timeline. \"All of Cluster 3 comes from their March-August production run. Something changed in their process during that period. With this analysis, you can have a specific conversation—not just 'your catalyst is bad,' but 'something happened in March that affected these specific properties.'\"\n\nThe meeting ended with Frank promising to contact the supplier with the analysis. For the first time, he sounded almost enthusiastic.\n\n\"I've been running reactors for thirty years,\" he said before signing off. \"Never thought I'd trust a computer to tell me about my catalyst. But this... this I can use.\"\n\nAfter he left, Maya raised an eyebrow. \"Frank's coming around.\"\n\n\"Data he can act on,\" Alex said. \"That's what skeptics need. Not promises—proof.\"\n\nShe updated the mystery board: **Three catalyst clusters identified. Cluster 3 = bad lots, all from March-August supplier production run.**\n\n---\n\n*Continue to the next lecture to learn about uncertainty quantification...*",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}