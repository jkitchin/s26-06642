{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jkitchin/s26-06642/blob/main/dsmles/10-ensemble-methods/ensemble-methods.ipynb)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 08: Ensemble Methods\n",
    "\n",
    "Combining multiple models for better predictions.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand the concept of ensemble learning\n",
    "2. Apply Random Forests for regression and classification\n",
    "3. Use Gradient Boosting methods\n",
    "4. Implement XGBoost for high performance\n",
    "5. Compare and tune ensemble methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Why Ensemble Methods? The Wisdom of Crowds\n\nThe insight behind ensemble methods is simple but powerful: **combining multiple models often beats any single model**.\n\n### Why Does This Work?\n\nConsider weather forecasting. If you ask 10 meteorologists for predictions, averaging their forecasts is often more accurate than any individual forecast. Why?\n\n1. **Different errors cancel out**: Individual models make different mistakes. Averaging reduces the impact of any one model's errors.\n2. **Reduced variance**: A single complex model might overfit. Many simple models averaged together are more stable.\n3. **Better coverage**: Different models might capture different aspects of the relationship.\n\n### The Two Main Strategies\n\n| Strategy | How It Works | When It Helps |\n|----------|--------------|---------------|\n| **Bagging** | Train models on random subsets, average | Reduces variance (overfitting) |\n| **Boosting** | Train models sequentially, each fixes previous errors | Reduces bias (underfitting) |\n\n### Why Ensembles Dominate in Practice\n\nIn machine learning competitions (Kaggle, etc.), ensemble methods win almost every time. In chemical engineering:\n- Process data is often noisy → bagging helps\n- Relationships are complex → boosting captures nonlinearity\n- You care about reliable predictions → ensembles are more stable"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a realistic chemical engineering dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "\n",
    "# Process parameters\n",
    "temperature = np.random.uniform(300, 500, n_samples)  # K\n",
    "pressure = np.random.uniform(1, 20, n_samples)  # bar\n",
    "catalyst_loading = np.random.uniform(0.1, 5, n_samples)  # wt%\n",
    "residence_time = np.random.uniform(1, 60, n_samples)  # min\n",
    "feed_ratio = np.random.uniform(0.5, 3, n_samples)  # mol/mol\n",
    "impurity_level = np.random.uniform(0, 0.1, n_samples)  # fraction\n",
    "\n",
    "# Complex nonlinear relationship\n",
    "conversion = (\n",
    "    0.3 * np.exp(-2000 / temperature) *  # Arrhenius\n",
    "    (1 - np.exp(-0.1 * pressure)) *  # Pressure effect\n",
    "    np.tanh(catalyst_loading) *  # Saturation\n",
    "    (1 - np.exp(-0.05 * residence_time)) *  # Time dependence\n",
    "    (1 / (1 + np.abs(feed_ratio - 1.5))) *  # Optimal ratio\n",
    "    (1 - 2 * impurity_level)  # Impurity inhibition\n",
    ")\n",
    "conversion = conversion + np.random.normal(0, 0.02, n_samples)\n",
    "conversion = np.clip(conversion, 0, 1)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'temperature': temperature,\n",
    "    'pressure': pressure,\n",
    "    'catalyst_loading': catalyst_loading,\n",
    "    'residence_time': residence_time,\n",
    "    'feed_ratio': feed_ratio,\n",
    "    'impurity_level': impurity_level,\n",
    "    'conversion': conversion\n",
    "})\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "feature_names = ['temperature', 'pressure', 'catalyst_loading', \n",
    "                 'residence_time', 'feed_ratio', 'impurity_level']\n",
    "X = df[feature_names].values\n",
    "y = df['conversion'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Decision Trees: The Building Block\n\nBefore understanding ensembles, we need to understand their building block: **decision trees**.\n\n### How Trees Work\n\nA decision tree splits data based on yes/no questions:\n- \"Is temperature > 400K?\" → If yes, go left; if no, go right\n- At each leaf, predict the average of training points that land there\n\n### Trees Are Interpretable but Fragile\n\n**Pros**:\n- Easy to visualize and explain\n- Handle nonlinear relationships naturally\n- No scaling required\n- Handle mixed feature types\n\n**Cons**:\n- High variance: Small changes in data → very different tree\n- Prone to overfitting: A deep tree can memorize training data\n- Greedy splits: May not find globally optimal structure\n\n### The Overfitting Problem\n\nWatch how a deeper tree fits training data perfectly but generalizes poorly:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single decision tree\n",
    "tree = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Single Decision Tree:\")\n",
    "print(f\"  Training R²: {tree.score(X_train, y_train):.4f}\")\n",
    "print(f\"  Test R²: {tree.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: trees can easily overfit\n",
    "tree_deep = DecisionTreeRegressor(max_depth=20, random_state=42)\n",
    "tree_deep.fit(X_train, y_train)\n",
    "\n",
    "print(\"Deep Decision Tree (max_depth=20):\")\n",
    "print(f\"  Training R²: {tree_deep.score(X_train, y_train):.4f}\")\n",
    "print(f\"  Test R²: {tree_deep.score(X_test, y_test):.4f}\")\n",
    "print(f\"  Gap: {tree_deep.score(X_train, y_train) - tree_deep.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**This is overfitting in action.** The deep tree achieves near-perfect training performance (R² ≈ 0.99) but much worse test performance (R² ≈ 0.85). The \"gap\" of ~0.14 tells us the model has memorized training data rather than learning generalizable patterns.\n\nWhat happened? With max_depth=20, the tree created highly specific rules like \"if temperature is between 342.7 and 343.1 AND pressure is between 7.23 and 7.31 AND...\". These rules fit training noise perfectly but don't apply to new data.\n\nThis is exactly why we need ensembles: they combine many imperfect trees into a robust predictor.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Random Forests: Bagging + Random Features\n\nRandom Forest is one of the most successful and widely-used algorithms. It fixes trees' problems through two key ideas:\n\n### 1. Bootstrap Aggregating (Bagging)\n- Create many random samples (with replacement) from training data\n- Train a separate tree on each sample\n- Average all trees' predictions\n\nThis reduces variance because different trees see different data and make different errors.\n\n### 2. Random Feature Selection\n- At each split, only consider a random subset of features\n- This decorrelates the trees—they don't all rely on the same strong features\n\n### Why Random Forests Work\n\n| Problem with Single Tree | How Random Forest Fixes It |\n|-------------------------|---------------------------|\n| High variance | Averaging many trees |\n| Overfitting | Bootstrap samples + feature subsets |\n| Unstable | Many trees → stable predictions |\n\n### The Magic of Averaging\n\nEven if individual trees are mediocre, their average is often excellent. It's like how the average of many guesses at a jar of jellybeans is often close to the true count."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Random Forest (100 trees):\")\n",
    "print(f\"  Training R²: {rf.score(X_train, y_train):.4f}\")\n",
    "print(f\"  Test R²: {rf.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**Key observations:**\n\n1. **Diminishing returns**: Performance improves rapidly from 1-50 trees, then levels off. The curve shows that ~100 trees captures most of the benefit; 200 trees gives marginal improvement.\n\n2. **No overfitting**: Unlike a single deep tree, adding more trees doesn't hurt test performance. This is the magic of bagging—averaging prevents overfitting.\n\n3. **Train-test gap is small**: Unlike the deep decision tree, Random Forest has nearly identical train and test performance.\n\n**Practical implication**: You can safely use many trees (100-500) without worrying about overfitting. The only cost is computation time.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of number of trees\n",
    "n_trees_range = [1, 5, 10, 25, 50, 100, 200]\n",
    "scores = []\n",
    "\n",
    "for n in n_trees_range:\n",
    "    rf = RandomForestRegressor(n_estimators=n, max_depth=10, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    scores.append({\n",
    "        'n_trees': n,\n",
    "        'train': rf.score(X_train, y_train),\n",
    "        'test': rf.score(X_test, y_test)\n",
    "    })\n",
    "\n",
    "scores_df = pd.DataFrame(scores)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(scores_df['n_trees'], scores_df['train'], 'o-', label='Train R²')\n",
    "plt.plot(scores_df['n_trees'], scores_df['test'], 's-', label='Test R²')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('R²')\n",
    "plt.title('Random Forest: Effect of Number of Trees')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values('Importance', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance['Feature'], importance['Importance'], edgecolor='black')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Gradient Boosting: Sequential Error Correction\n\nBoosting takes a different approach than bagging. Instead of averaging independent models, it builds models **sequentially**, with each one correcting the errors of the previous.\n\n### The Key Idea\n\n1. Fit a model to the data\n2. Look at what it got wrong (the residuals)\n3. Fit a new model specifically to predict those errors\n4. Add the new model to the ensemble\n5. Repeat\n\nEach new model focuses on the \"hard cases\" that previous models missed.\n\n### Why It Works\n\n- **Reduces bias**: Each iteration reduces the overall error\n- **Builds complexity gradually**: Starts simple, adds detail where needed\n- **Focuses effort**: Spends modeling capacity on difficult regions\n\n### The Tradeoff: Overfitting Risk\n\nUnlike Random Forests (which rarely overfit), boosting can overfit if you:\n- Use too many iterations\n- Use a learning rate that's too high\n- Don't use regularization\n\nThe **learning_rate** parameter controls how much each tree contributes. Lower values are more conservative but need more trees."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "gb = GradientBoostingRegressor(\n",
    "    n_estimators=100, \n",
    "    max_depth=5, \n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "print(\"Gradient Boosting:\")\n",
    "print(f\"  Training R²: {gb.score(X_train, y_train):.4f}\")\n",
    "print(f\"  Test R²: {gb.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curve: how error decreases with more trees\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "# Get staged predictions\n",
    "for i, y_pred in enumerate(gb.staged_predict(X_train)):\n",
    "    train_scores.append(r2_score(y_train, y_pred))\n",
    "\n",
    "for i, y_pred in enumerate(gb.staged_predict(X_test)):\n",
    "    test_scores.append(r2_score(y_test, y_pred))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_scores)+1), train_scores, label='Train R²')\n",
    "plt.plot(range(1, len(test_scores)+1), test_scores, label='Test R²')\n",
    "plt.xlabel('Number of Boosting Iterations')\n",
    "plt.ylabel('R²')\n",
    "plt.title('Gradient Boosting: Learning Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## XGBoost: Gradient Boosting on Steroids\n\nXGBoost (eXtreme Gradient Boosting) is a highly optimized implementation that dominates machine learning competitions. It adds several improvements:\n\n### Key Innovations\n\n1. **Regularization**: Built-in L1/L2 penalties prevent overfitting\n2. **Parallel processing**: Splits training across CPU cores\n3. **Missing values**: Learns the best direction for missing data\n4. **Pruning**: Grows tree then prunes, rather than stopping early\n5. **Cache optimization**: Faster memory access patterns\n\n### When to Use XGBoost\n\n- You want maximum predictive performance\n- You have structured/tabular data (not images or text)\n- You have time to tune hyperparameters\n- Interpretability is secondary to accuracy\n\n### XGBoost vs Random Forest\n\n| Aspect | Random Forest | XGBoost |\n|--------|---------------|---------|\n| Training | Embarrassingly parallel | Sequential (within boosting) |\n| Overfitting | Rarely | Need to tune carefully |\n| Default performance | Good out of box | Often needs tuning |\n| Speed | Faster training | Can be slower |\n| Best performance | Very good | Often better (tuned) |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"XGBoost:\")\n",
    "print(f\"  Training R²: {xgb_model.score(X_train, y_train):.4f}\")\n",
    "print(f\"  Test R²: {xgb_model.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost feature importance\n",
    "importance_xgb = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_xgb['Feature'], importance_xgb['Importance'], \n",
    "         edgecolor='black', color='coral')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "grid_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Random Forest Best Parameters:\")\n",
    "print(f\"  {grid_rf.best_params_}\")\n",
    "print(f\"  Best CV R²: {grid_rf.best_score_:.4f}\")\n",
    "print(f\"  Test R²: {grid_rf.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for XGBoost\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(random_state=42)\n",
    "grid_xgb = GridSearchCV(xgb_model, param_grid_xgb, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid_xgb.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nXGBoost Best Parameters:\")\n",
    "print(f\"  {grid_xgb.best_params_}\")\n",
    "print(f\"  Best CV R²: {grid_xgb.best_score_:.4f}\")\n",
    "print(f\"  Test R²: {grid_xgb.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all methods\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Decision Tree': DecisionTreeRegressor(max_depth=10, random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42),\n",
    "    'XGBoost': xgb.XGBRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    train_score = model.score(X_train, y_train)\n",
    "    test_score = model.score(X_test, y_test)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Train R²': train_score,\n",
    "        'Test R²': test_score,\n",
    "        'CV R² (mean)': cv_scores.mean(),\n",
    "        'CV R² (std)': cv_scores.std()\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart of scores\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, results_df['Train R²'], width, label='Train', edgecolor='black')\n",
    "axes[0].bar(x + width/2, results_df['Test R²'], width, label='Test', edgecolor='black')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "axes[0].set_ylabel('R²')\n",
    "axes[0].set_title('Model Performance Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# CV scores with error bars\n",
    "axes[1].barh(results_df['Model'], results_df['CV R² (mean)'], \n",
    "             xerr=results_df['CV R² (std)'] * 2, capsize=5, edgecolor='black', color='steelblue')\n",
    "axes[1].set_xlabel('Cross-Validation R²')\n",
    "axes[1].set_title('5-Fold CV Scores (±2 std)')\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs Actual for best model\n",
    "best_model = xgb.XGBRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', linewidth=2)\n",
    "plt.xlabel('Actual Conversion')\n",
    "plt.ylabel('Predicted Conversion')\n",
    "plt.title(f'XGBoost: Predicted vs Actual (R² = {r2_score(y_test, y_pred):.3f})')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Hyperparameters\n",
    "\n",
    "### Random Forest\n",
    "| Parameter | Effect |\n",
    "|-----------|--------|\n",
    "| n_estimators | More trees = better, but slower |\n",
    "| max_depth | Deeper = more complex, can overfit |\n",
    "| min_samples_split | Higher = more regularization |\n",
    "| max_features | Random feature subset size |\n",
    "\n",
    "### Gradient Boosting / XGBoost\n",
    "| Parameter | Effect |\n",
    "|-----------|--------|\n",
    "| n_estimators | More iterations, can overfit |\n",
    "| learning_rate | Smaller = more robust, needs more trees |\n",
    "| max_depth | Shallower trees often work well (3-10) |\n",
    "| subsample | Fraction of samples per tree |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary: Ensemble Methods Decision Guide\n\n### When to Use What\n\n```\nNeed a quick, reliable baseline?\n└── Random Forest (works well out of the box)\n\nNeed maximum performance and have time to tune?\n└── XGBoost or LightGBM\n\nHave very large data (millions of rows)?\n└── LightGBM (fastest) or XGBoost with subsampling\n\nNeed interpretability?\n└── Single tree (if accuracy allows) or use SHAP with ensembles\n```\n\n### Key Hyperparameters to Tune\n\n| Method | Most Important | Secondary |\n|--------|---------------|-----------|\n| Random Forest | n_estimators, max_depth | min_samples_split, max_features |\n| Gradient Boosting | n_estimators, learning_rate | max_depth, subsample |\n| XGBoost | learning_rate, n_estimators | max_depth, reg_alpha, reg_lambda |\n\n### Practical Tips\n\n1. **Start with Random Forest**: It's hard to mess up. Good baseline.\n2. **Don't tune n_estimators alone**: More trees are always better (just slower). Focus on other parameters.\n3. **For boosting, tune learning_rate and n_estimators together**: Lower rate → more trees needed.\n4. **Use early stopping**: Prevents overfitting in boosting automatically.\n5. **Feature importance is useful but imperfect**: Use SHAP for more reliable interpretation.\n\n### Common Pitfalls\n\n- Not enough trees (RF): 100+ is usually good, 500+ is safer\n- Learning rate too high (boosting): Start with 0.01-0.1\n- Not tuning XGBoost: Defaults are often suboptimal\n- Ignoring feature importance: Ensembles can reveal which features matter\n\n## Next Steps\n\nIn the next module, we'll explore clustering for unsupervised learning—finding structure in data without labels."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}