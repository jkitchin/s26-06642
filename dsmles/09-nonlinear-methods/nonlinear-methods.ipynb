{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jkitchin/s26-06642/blob/main/dsmles/09-nonlinear-methods/nonlinear-methods.ipynb)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "! pip install -q pycse\nfrom pycse.colab import pdf",
   "metadata": {
    "tags": [
     "skip-execution",
     "remove-cell"
    ]
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 07: Nonlinear Methods\n",
    "\n",
    "Modeling complex, nonlinear relationships.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Recognize when linear models are insufficient\n",
    "2. Apply polynomial features for nonlinear relationships\n",
    "3. Use Support Vector Machines (SVMs) for regression\n",
    "4. Understand kernel methods\n",
    "5. Choose appropriate nonlinear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## When Linear Models Fail: Recognizing Nonlinearity\n\nLinear models assume the relationship between features and target is a straight line (or hyperplane). But many real-world relationships are nonlinear:\n\n### Chemical Engineering Examples\n\n| System | Relationship | Why It's Nonlinear |\n|--------|--------------|-------------------|\n| Enzyme kinetics | Michaelis-Menten | Saturation at high [S] |\n| Reaction rates | Arrhenius | Exponential temperature dependence |\n| Adsorption | Langmuir isotherm | Saturation of surface sites |\n| Heat transfer | Radiation | T\u2074 dependence |\n| Mixing | Log-mean differences | Logarithmic average |\n\n### How to Detect Nonlinearity\n\n1. **Plot residuals**: If they show a curve, the relationship is nonlinear\n2. **Domain knowledge**: You know the physics suggests a specific form\n3. **Low R\u00b2 despite good features**: Linear model can't capture the pattern\n4. **Scatter plot inspection**: Relationship visibly curves\n\n### Two Approaches to Nonlinearity\n\n1. **Transform the data**: Log, square root, inverse (if you know the form)\n2. **Use nonlinear models**: Polynomial features, SVR, trees (if form is unknown)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a nonlinear dataset: enzyme kinetics (Michaelis-Menten like)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Substrate concentration\n",
    "S = np.linspace(0.1, 10, 100)\n",
    "\n",
    "# Michaelis-Menten: V = Vmax * S / (Km + S)\n",
    "Vmax = 100  # Maximum rate\n",
    "Km = 2      # Michaelis constant\n",
    "\n",
    "V = Vmax * S / (Km + S) + np.random.normal(0, 3, len(S))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(S, V, alpha=0.6, label='Data')\n",
    "plt.xlabel('Substrate Concentration [S]')\n",
    "plt.ylabel('Reaction Rate V')\n",
    "plt.title('Enzyme Kinetics (Michaelis-Menten)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try linear regression\n",
    "X = S.reshape(-1, 1)\n",
    "y = V\n",
    "\n",
    "linear = LinearRegression()\n",
    "linear.fit(X, y)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(S, V, alpha=0.6, label='Data')\n",
    "plt.plot(S, linear.predict(X), 'r-', linewidth=2, label=f'Linear (R\u00b2 = {linear.score(X, y):.3f})')\n",
    "plt.xlabel('Substrate Concentration [S]')\n",
    "plt.ylabel('Reaction Rate V')\n",
    "plt.title('Linear Model Fails on Nonlinear Data')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Polynomial Features: The Simple Extension\n\nThe easiest way to capture nonlinearity is to add polynomial terms. Transform:\n\n$$x \\rightarrow [x, x^2, x^3, \\ldots, x^d]$$\n\nThen apply linear regression on these expanded features. The model is still *linear in parameters* (just fitting coefficients), but *nonlinear in the input*.\n\n### The Tradeoff\n\n| Degree | Flexibility | Risk |\n|--------|-------------|------|\n| 1 | Linear only | Underfitting |\n| 2 | Captures curvature | Usually safe |\n| 3-4 | More complex shapes | Often good balance |\n| 5+ | Very flexible | High overfitting risk |\n\n### The Explosion of Features\n\nWith d features and polynomial degree p, you get:\n$$\\binom{d+p}{p}$$ features\n\nFor d=10 features, degree=3: **286 features!**\n\nThis is why polynomial features are best for:\n- Few original features (1-5)\n- Moderate degrees (2-4)\n- Combined with regularization"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial regression with different degrees\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for ax, degree in zip(axes, [2, 4, 10]):\n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    \n",
    "    # Fit model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y)\n",
    "    \n",
    "    # Plot\n",
    "    ax.scatter(S, V, alpha=0.5, label='Data')\n",
    "    S_smooth = np.linspace(0.1, 10, 200).reshape(-1, 1)\n",
    "    X_smooth_poly = poly.transform(S_smooth)\n",
    "    ax.plot(S_smooth, model.predict(X_smooth_poly), 'r-', linewidth=2)\n",
    "    ax.set_xlabel('[S]')\n",
    "    ax.set_ylabel('V')\n",
    "    ax.set_title(f'Degree {degree} (R\u00b2 = {model.score(X_poly, y):.3f})')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for overfitting with train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "results = []\n",
    "for degree in range(1, 15):\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    \n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train)\n",
    "    \n",
    "    results.append({\n",
    "        'Degree': degree,\n",
    "        'Train R\u00b2': model.score(X_train_poly, y_train),\n",
    "        'Test R\u00b2': model.score(X_test_poly, y_test)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results_df['Degree'], results_df['Train R\u00b2'], 'o-', label='Train R\u00b2')\n",
    "plt.plot(results_df['Degree'], results_df['Test R\u00b2'], 's-', label='Test R\u00b2')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('R\u00b2')\n",
    "plt.title('Overfitting with High-Degree Polynomials')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cross-validation to select best degree\n",
    "cv_scores = []\n",
    "\n",
    "for degree in range(1, 12):\n",
    "    pipeline = Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "        ('model', Ridge(alpha=0.1))  # Add regularization\n",
    "    ])\n",
    "    \n",
    "    scores = cross_val_score(pipeline, X, y, cv=5, scoring='r2')\n",
    "    cv_scores.append({\n",
    "        'Degree': degree,\n",
    "        'Mean CV R\u00b2': scores.mean(),\n",
    "        'Std': scores.std()\n",
    "    })\n",
    "\n",
    "cv_df = pd.DataFrame(cv_scores)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(cv_df['Degree'], cv_df['Mean CV R\u00b2'], \n",
    "             yerr=cv_df['Std'], fmt='o-', capsize=5)\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Cross-Validation R\u00b2')\n",
    "plt.title('Selecting Polynomial Degree with CV')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "best_degree = cv_df.loc[cv_df['Mean CV R\u00b2'].idxmax(), 'Degree']\n",
    "print(f\"Best polynomial degree: {best_degree}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Support Vector Regression (SVR): The Kernel Trick\n\nSVR takes a completely different approach to nonlinearity. Instead of explicitly creating polynomial features, it uses the **kernel trick** to implicitly work in a high-dimensional space.\n\n### The Key Idea\n\n1. Map data to a high-dimensional space where it becomes linearly separable\n2. Fit a linear model in that space\n3. The kernel function computes dot products in the high-dimensional space without ever explicitly computing the mapping\n\n### The \u03b5-Tube\n\nUnlike OLS (which penalizes all errors), SVR fits a \"tube\" of width \u03b5 around the data:\n- Points inside the tube: no penalty\n- Points outside the tube: penalized by distance to the tube\n\nThis makes SVR robust to small noise\u2014it only cares about points that deviate significantly.\n\n### Why SVR Works Well\n\n- Handles nonlinearity without explicit feature engineering\n- Robust to outliers (\u03b5-tube ignores small noise)\n- Works well in high dimensions\n- Doesn't require specifying the functional form\n\n### The Cost\n\n- Less interpretable (no meaningful coefficients)\n- Requires feature scaling\n- Hyperparameters can be tricky to tune\n- Slower than linear models for large datasets"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR with RBF kernel\n",
    "# Important: Scale features for SVR!\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "svr = SVR(kernel='rbf', C=100, epsilon=0.1)\n",
    "svr.fit(X_scaled, y)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(S, V, alpha=0.5, label='Data')\n",
    "S_smooth = np.linspace(0.1, 10, 200).reshape(-1, 1)\n",
    "X_smooth_scaled = scaler.transform(S_smooth)\n",
    "plt.plot(S_smooth, svr.predict(X_smooth_scaled), 'r-', linewidth=2, label='SVR (RBF)')\n",
    "plt.xlabel('[S]')\n",
    "plt.ylabel('V')\n",
    "plt.title(f'SVR with RBF Kernel (R\u00b2 = {svr.score(X_scaled, y):.3f})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR Kernels\n",
    "\n",
    "Different kernels for different relationships:\n",
    "\n",
    "| Kernel | Formula | Use Case |\n",
    "|--------|---------|----------|\n",
    "| Linear | $x \\cdot x'$ | Linear relationships |\n",
    "| RBF | $\\exp(-\\gamma \\|x-x'\\|^2)$ | General nonlinear (most common) |\n",
    "| Polynomial | $(\\gamma x \\cdot x' + r)^d$ | Polynomial-like relationships |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different kernels\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "kernel_params = [\n",
    "    {'kernel': 'linear', 'C': 100},\n",
    "    {'kernel': 'rbf', 'C': 100, 'gamma': 'scale'},\n",
    "    {'kernel': 'poly', 'C': 100, 'degree': 3}\n",
    "]\n",
    "\n",
    "for ax, name, params in zip(axes, kernels, kernel_params):\n",
    "    svr = SVR(**params)\n",
    "    svr.fit(X_scaled, y)\n",
    "    \n",
    "    ax.scatter(S, V, alpha=0.5, label='Data')\n",
    "    ax.plot(S_smooth, svr.predict(X_smooth_scaled), 'r-', linewidth=2)\n",
    "    ax.set_xlabel('[S]')\n",
    "    ax.set_ylabel('V')\n",
    "    ax.set_title(f'{name.upper()} Kernel (R\u00b2 = {svr.score(X_scaled, y):.3f})')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR Hyperparameters\n",
    "\n",
    "- **C**: Regularization (higher = less regularization, tighter fit)\n",
    "- **epsilon**: Width of the tube (points inside aren't penalized)\n",
    "- **gamma** (RBF): Controls kernel width (higher = more local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of C\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for ax, C in zip(axes, [0.1, 10, 1000]):\n",
    "    svr = SVR(kernel='rbf', C=C)\n",
    "    svr.fit(X_scaled, y)\n",
    "    \n",
    "    ax.scatter(S, V, alpha=0.5)\n",
    "    ax.plot(S_smooth, svr.predict(X_smooth_scaled), 'r-', linewidth=2)\n",
    "    ax.set_xlabel('[S]')\n",
    "    ax.set_ylabel('V')\n",
    "    ax.set_title(f'C = {C} (R\u00b2 = {svr.score(X_scaled, y):.3f})')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Effect of C on SVR', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of gamma\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for ax, gamma in zip(axes, [0.1, 1, 10]):\n",
    "    svr = SVR(kernel='rbf', C=100, gamma=gamma)\n",
    "    svr.fit(X_scaled, y)\n",
    "    \n",
    "    ax.scatter(S, V, alpha=0.5)\n",
    "    ax.plot(S_smooth, svr.predict(X_smooth_scaled), 'r-', linewidth=2)\n",
    "    ax.set_xlabel('[S]')\n",
    "    ax.set_ylabel('V')\n",
    "    ax.set_title(f'gamma = {gamma} (R\u00b2 = {svr.score(X_scaled, y):.3f})')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Effect of gamma (RBF width) on SVR', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning SVR with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for SVR hyperparameters\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svr', SVR(kernel='rbf'))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'svr__C': [0.1, 1, 10, 100, 1000],\n",
    "    'svr__gamma': [0.01, 0.1, 1, 10],\n",
    "    'svr__epsilon': [0.01, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV R\u00b2: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(S, V, alpha=0.5, label='Data')\n",
    "plt.plot(S_smooth, best_model.predict(S_smooth), 'r-', linewidth=2, label='Best SVR')\n",
    "plt.xlabel('[S]')\n",
    "plt.ylabel('V')\n",
    "plt.title(f'Optimized SVR (R\u00b2 = {grid_search.best_score_:.3f})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-dimensional Example\n",
    "\n",
    "Nonlinear methods work with multiple features too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D nonlinear dataset: reaction yield\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "temperature = np.random.uniform(300, 500, n_samples)\n",
    "pressure = np.random.uniform(1, 10, n_samples)\n",
    "\n",
    "# Nonlinear relationship with interaction\n",
    "yield_rate = (\n",
    "    0.1 * np.exp(-3000 / temperature) +  # Arrhenius-like\n",
    "    0.05 * np.log(pressure) +             # Log relationship\n",
    "    0.001 * temperature * pressure / 100 +  # Interaction\n",
    "    np.random.normal(0, 0.02, n_samples)\n",
    ")\n",
    "\n",
    "X_2d = np.column_stack([temperature, pressure])\n",
    "y_2d = yield_rate\n",
    "\n",
    "X_train_2d, X_test_2d, y_train_2d, y_test_2d = train_test_split(\n",
    "    X_2d, y_2d, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "models = {\n",
    "    'Linear': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', LinearRegression())\n",
    "    ]),\n",
    "    'Polynomial (deg=3)': Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=3)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', Ridge(alpha=0.1))\n",
    "    ]),\n",
    "    'SVR (RBF)': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', SVR(kernel='rbf', C=100))\n",
    "    ])\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_2d, y_train_2d)\n",
    "    train_score = model.score(X_train_2d, y_train_2d)\n",
    "    test_score = model.score(X_test_2d, y_test_2d)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Train R\u00b2': train_score,\n",
    "        'Test R\u00b2': test_score\n",
    "    })\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for ax, (name, model) in zip(axes, models.items()):\n",
    "    y_pred = model.predict(X_test_2d)\n",
    "    \n",
    "    ax.scatter(y_test_2d, y_pred, alpha=0.6)\n",
    "    ax.plot([y_2d.min(), y_2d.max()], [y_2d.min(), y_2d.max()], 'r--')\n",
    "    ax.set_xlabel('Actual Yield')\n",
    "    ax.set_ylabel('Predicted Yield')\n",
    "    ax.set_title(f'{name}\\nTest R\u00b2 = {r2_score(y_test_2d, y_pred):.3f}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection Guidelines\n",
    "\n",
    "| Situation | Recommended Approach |\n",
    "|-----------|---------------------|\n",
    "| Few features, known polynomial form | Polynomial features |\n",
    "| Many features, unknown relationship | SVR with RBF kernel |\n",
    "| Need interpretability | Polynomial (lower degree) |\n",
    "| Large dataset | Consider alternatives (trees, neural nets) |\n",
    "| Small dataset | SVR often works well |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary: Choosing Nonlinear Methods\n\n### Decision Guide\n\n```\nDo you know the functional form?\n\u251c\u2500\u2500 Yes \u2192 Use that form (log-transform, power law, etc.)\n\u2514\u2500\u2500 No\n    \u251c\u2500\u2500 Few features (1-3)?\n    \u2502   \u2514\u2500\u2500 Polynomial features (degree 2-4 with regularization)\n    \u2514\u2500\u2500 Many features?\n        \u251c\u2500\u2500 Small-medium dataset (<10,000)?\n        \u2502   \u2514\u2500\u2500 SVR with RBF kernel\n        \u2514\u2500\u2500 Large dataset?\n            \u2514\u2500\u2500 Consider tree-based methods (next module)\n```\n\n### Method Comparison\n\n| Method | Best For | Interpretable? | Key Hyperparameter |\n|--------|----------|----------------|-------------------|\n| Polynomial | Known polynomial form, few features | Yes | Degree |\n| SVR (RBF) | Unknown nonlinearity, small/medium data | No | C, \u03b3, \u03b5 |\n| SVR (Poly) | Polynomial-like with many features | No | C, degree |\n\n### Key Takeaways\n\n1. **Start simple**: Try degree-2 polynomials before complex methods\n2. **Always scale for SVR**: Kernels are sensitive to feature scales\n3. **Use cross-validation**: Both polynomial degree and SVR hyperparameters need tuning\n4. **Watch for overfitting**: High-degree polynomials and high-C SVR overfit easily\n5. **RBF is a good default**: When you don't know the form, RBF usually works\n\n### Common Pitfalls\n\n- Forgetting to scale features before SVR\n- Using high-degree polynomials without regularization\n- Not tuning hyperparameters (default C=1 is rarely optimal)\n- Ignoring the explosion of polynomial features with multiple inputs\n\n## Next Steps\n\nIn the next module, we'll learn about ensemble methods (Random Forests, Gradient Boosting) that combine multiple models for even better predictions."
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## The Catalyst Crisis: Chapter 9 - \"When Lines Aren't Enough\"\n\n*A story about recognizing the limits of linear models*\n\n---\n\n\"We've hit a wall.\"\n\nAlex said it flatly in the team meeting. They'd squeezed everything they could from linear models. The plateau was real: 0.80 R-squared, 85% of failures caught. Better than nothing. Not good enough.\n\n\"The relationship isn't linear,\" Maya said. \"We've known that since the t-SNE clustering. The reactor has multiple operating regimes.\"\n\n\"So we need nonlinear models.\" Sam pulled up a notebook. \"Neural networks? SVMs?\"\n\nProfessor Pipeline, sitting in the corner, spoke up. \"Before you reach for the complicated tools\u2014why isn't it linear?\"\n\nAlex thought about it. \"Interactions. The effect of temperature depends on catalyst age. The effect of pressure depends on temperature. Everything affects everything.\"\n\n\"And linear models can capture interactions?\"\n\n\"Only the ones we explicitly code. We'd have to know ahead of time which interactions matter.\"\n\n\"So the problem isn't that you need a nonlinear function. It's that you need to discover which interactions matter.\" He stood up. \"Tomorrow's lecture covers decision trees and ensemble methods. They're nonlinear, yes\u2014but more importantly, they find interactions automatically.\"\n\nAfter he left, Maya turned to Alex. \"Do you understand what he meant?\"\n\nAlex wasn't entirely sure. But she'd learned to trust that clarity would come\u2014not all at once, but incrementally, the way understanding usually did.\n\n\"I think he's saying we've been trying to specify the model form ourselves. Trees let the data decide.\"\n\nThat night, Alex reviewed her notebook\u2014the physical one, full of sketches and dead ends. A pattern emerged from the chaos. Every breakthrough had come not from cleverer algorithms, but from asking better questions. What was missing? Why did outliers exist? Which features mattered?\n\nThe algorithm didn't solve problems. It answered questions. The art was in knowing which questions to ask.\n\nShe added to the mystery board: **Linear models have limits. Next step: let the data find interactions we didn't anticipate.**\n\n---\n\n*Continue to the next lecture to discover the power of ensemble methods...*",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}