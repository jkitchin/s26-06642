{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jkitchin/s26-06642/blob/main/dsmles/13-model-interpretability/model-interpretability.ipynb)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "! pip install -q pycse\nfrom pycse.colab import pdf",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 11: Model Interpretability\n",
    "\n",
    "Understanding why models make their predictions.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand the importance of interpretability\n",
    "2. Use feature importance from tree-based models\n",
    "3. Apply SHAP values for model explanation\n",
    "4. Create partial dependence plots\n",
    "5. Interpret individual predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import xgboost as xgb\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Why Interpretability Matters: The Trust Problem\n\nMachine learning models are increasingly powerful, but power without understanding is dangerous. In chemical engineering, we can't just accept predictions\u2014we need to understand *why*.\n\n### The Interpretability Imperative\n\n| Scenario | Why You Need Interpretability |\n|----------|------------------------------|\n| Safety-critical decisions | Regulators require explanations |\n| Process optimization | Need to know which variables to adjust |\n| Model debugging | Unexpected predictions need diagnosis |\n| Knowledge discovery | ML can reveal unknown relationships |\n| Stakeholder trust | Operators won't use black boxes |\n\n### The Accuracy-Interpretability Tradeoff\n\nHistorically, there was a perceived tradeoff:\n\n| Model Type | Interpretability | Accuracy |\n|------------|-----------------|----------|\n| Linear regression | High | Limited |\n| Decision trees | High | Moderate |\n| Random forests | Low | High |\n| Neural networks | Very low | Very high |\n\n**Modern interpretability tools (SHAP, LIME) break this tradeoff.** We can now explain complex models without sacrificing accuracy.\n\n### Two Types of Explanation\n\n1. **Global**: What features matter overall?\n2. **Local**: Why did the model make *this specific* prediction?\n\nBoth are important. Global explanations validate the model. Local explanations enable debugging and trust."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chemical process dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "\n",
    "# Process variables\n",
    "temperature = np.random.uniform(300, 500, n_samples)\n",
    "pressure = np.random.uniform(1, 20, n_samples)\n",
    "catalyst_loading = np.random.uniform(0.5, 5, n_samples)\n",
    "residence_time = np.random.uniform(5, 60, n_samples)\n",
    "feed_purity = np.random.uniform(0.9, 1.0, n_samples)\n",
    "stirrer_speed = np.random.uniform(100, 500, n_samples)\n",
    "\n",
    "# Target: Product yield (complex nonlinear relationship)\n",
    "yield_product = (\n",
    "    40 * (1 - np.exp(-0.01 * (temperature - 300))) *  # Temperature activation\n",
    "    (1 - np.exp(-0.1 * pressure)) *                    # Pressure effect\n",
    "    np.tanh(0.5 * catalyst_loading) *                  # Catalyst saturation\n",
    "    (1 - np.exp(-0.05 * residence_time)) *             # Time effect\n",
    "    feed_purity *                                       # Linear purity effect\n",
    "    (1 + 0.001 * (stirrer_speed - 300))                # Minor mixing effect\n",
    ")\n",
    "yield_product = yield_product + np.random.normal(0, 2, n_samples)\n",
    "yield_product = np.clip(yield_product, 0, 100)\n",
    "\n",
    "# Create DataFrame\n",
    "feature_names = ['temperature', 'pressure', 'catalyst_loading', \n",
    "                 'residence_time', 'feed_purity', 'stirrer_speed']\n",
    "df = pd.DataFrame({\n",
    "    'temperature': temperature,\n",
    "    'pressure': pressure,\n",
    "    'catalyst_loading': catalyst_loading,\n",
    "    'residence_time': residence_time,\n",
    "    'feed_purity': feed_purity,\n",
    "    'stirrer_speed': stirrer_speed,\n",
    "    'yield': yield_product\n",
    "})\n",
    "\n",
    "X = df[feature_names].values\n",
    "y = df['yield'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Random Forest Test R\u00b2: {rf.score(X_test, y_test):.4f}\")\n",
    "print(f\"XGBoost Test R\u00b2: {xgb_model.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Feature Importance: The First Step\n\nTree-based models (Random Forest, XGBoost) provide built-in feature importance. This is often your first look at what the model learned.\n\n### How It's Calculated\n\n| Method | Measure | Pros | Cons |\n|--------|---------|------|------|\n| **Impurity-based** (RF) | Total reduction in impurity | Fast, built-in | Biased toward high-cardinality features |\n| **Gain-based** (XGBoost) | Average gain when feature is used | Fast, built-in | Can vary between runs |\n| **Permutation** | Accuracy drop when feature is shuffled | Model-agnostic, unbiased | Slower, affected by correlations |\n\n### The Limitation\n\nFeature importance tells you **what** matters, but not **how** it matters. Is temperature positively or negatively correlated with yield? Does the effect saturate? These questions require deeper analysis."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Random Forest\n",
    "importance_rf = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values('Importance', ascending=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Random Forest\n",
    "axes[0].barh(importance_rf['Feature'], importance_rf['Importance'], \n",
    "             edgecolor='black', color='steelblue')\n",
    "axes[0].set_xlabel('Feature Importance')\n",
    "axes[0].set_title('Random Forest Feature Importance')\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# XGBoost\n",
    "importance_xgb = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=True)\n",
    "\n",
    "axes[1].barh(importance_xgb['Feature'], importance_xgb['Importance'], \n",
    "             edgecolor='black', color='coral')\n",
    "axes[1].set_xlabel('Feature Importance')\n",
    "axes[1].set_title('XGBoost Feature Importance')\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## SHAP Values: The Gold Standard for Interpretability\n\nSHAP (SHapley Additive exPlanations) is based on game theory. It answers: **\"How much did each feature contribute to this prediction?\"**\n\n### The Shapley Value Concept\n\nImagine features as players in a game (predicting the target). The Shapley value assigns credit fairly by considering all possible combinations of features and averaging each feature's marginal contribution.\n\n### Why SHAP Is Special\n\n1. **Consistency**: If a feature's contribution increases, its SHAP value won't decrease\n2. **Local accuracy**: SHAP values sum to the difference between prediction and baseline\n3. **Theoretical foundation**: Based on 60+ years of game theory research\n4. **Signs and magnitudes**: Positive SHAP = pushed prediction up; magnitude = how much\n\n### Interpreting SHAP Plots\n\n| Plot Type | What It Shows | Use For |\n|-----------|---------------|---------|\n| Summary | Feature importance + direction + distribution | Global overview |\n| Waterfall | Single prediction breakdown | Explaining individual decisions |\n| Dependence | Feature effect vs feature value | Understanding relationships |\n| Force | Real-time feature contributions | Interactive exploration |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainer\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "print(f\"SHAP values shape: {shap_values.shape}\")\n",
    "print(f\"Each row: SHAP value for each feature for one prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot: global feature importance with direction\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, X_test, feature_names=feature_names, show=False)\n",
    "plt.title('SHAP Summary Plot')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**Reading the SHAP summary plot:**\n\nThis is one of the most information-dense visualizations in ML. Here's how to read it:\n\n- **Each row** is a feature, sorted by importance (most important at top)\n- **Each point** is one sample from the test set\n- **X-axis** shows the SHAP value (contribution to prediction)\n- **Color** shows the feature value (red=high, blue=low)\n\n**What we see:**\n- **temperature**: Red points (high temp) have positive SHAP values \u2192 higher temperature increases predicted yield\n- **catalyst_loading**: Same pattern\u2014more catalyst increases yield\n- **feed_purity**: Red points on the right \u2192 higher purity increases yield\n- **pressure**: Positive relationship, but less spread than temperature\n- **residence_time**: Smaller effect, but still positive\n- **stirrer_speed**: Minimal effect (narrow horizontal spread)\n\n**Physical validation**: All effects are positive (more of each variable increases yield), which matches our synthetic data and general chemical intuition. If we saw negative effects for temperature or catalyst, we'd question the model or our data!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of mean absolute SHAP values\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, X_test, feature_names=feature_names, \n",
    "                  plot_type='bar', show=False)\n",
    "plt.title('Mean |SHAP| Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining Individual Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain a single prediction\n",
    "sample_idx = 0\n",
    "sample = X_test[sample_idx]\n",
    "prediction = xgb_model.predict([sample])[0]\n",
    "actual = y_test[sample_idx]\n",
    "\n",
    "print(f\"Sample features:\")\n",
    "for name, val in zip(feature_names, sample):\n",
    "    print(f\"  {name}: {val:.2f}\")\n",
    "print(f\"\\nPredicted yield: {prediction:.2f}\")\n",
    "print(f\"Actual yield: {actual:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waterfall plot for single prediction\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.plots.waterfall(shap.Explanation(\n",
    "    values=shap_values[sample_idx],\n",
    "    base_values=explainer.expected_value,\n",
    "    data=X_test[sample_idx],\n",
    "    feature_names=feature_names\n",
    "), show=False)\n",
    "plt.title(f'SHAP Waterfall: Sample {sample_idx}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force plot for single prediction\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value, shap_values[sample_idx], \n",
    "                X_test[sample_idx], feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP Dependence Plots\n",
    "\n",
    "How does a feature affect predictions across all samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependence plots for key features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for ax, (idx, name) in zip(axes.flat, enumerate(feature_names)):\n",
    "    shap.dependence_plot(idx, shap_values, X_test, \n",
    "                         feature_names=feature_names, ax=ax, show=False)\n",
    "    ax.set_title(f'{name}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Partial Dependence Plots: The Marginal Effect\n\nPartial Dependence Plots (PDPs) show how a feature affects predictions **on average**, while marginalizing over other features.\n\n### How It Works\n\n1. Pick a feature (e.g., temperature)\n2. Create a grid of values for that feature\n3. For each grid value, replace all samples' temperature with that value\n4. Average the predictions\n5. Plot average prediction vs temperature\n\n### PDPs vs SHAP Dependence\n\n| Aspect | PDP | SHAP Dependence |\n|--------|-----|-----------------|\n| Shows | Marginal effect | Actual contribution |\n| Interactions | Hidden (averaged out) | Visible (colored by interaction) |\n| Interpretation | \"If we set T=400, yield is ~X\" | \"For samples where T=400, T contributed ~X\" |\n\n### 2D PDPs: Interaction Effects\n\nTwo-feature PDPs reveal interactions. If the effect of temperature depends on pressure, the 2D plot will show it\u2014something hidden in 1D plots."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial dependence plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for ax, idx in zip(axes.flat, range(len(feature_names))):\n",
    "    PartialDependenceDisplay.from_estimator(\n",
    "        xgb_model, X_train, [idx], \n",
    "        feature_names=feature_names, ax=ax\n",
    "    )\n",
    "    ax.set_title(feature_names[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D interaction plot: temperature vs pressure\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    xgb_model, X_train, [(0, 1)],  # temperature and pressure\n",
    "    feature_names=feature_names, ax=ax\n",
    ")\n",
    "ax.set_title('Interaction: Temperature \u00d7 Pressure')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Model Interpretations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature importance methods\n",
    "# 1. Impurity-based (RF)\n",
    "# 2. Gain-based (XGBoost)\n",
    "# 3. SHAP-based\n",
    "\n",
    "shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'RF Impurity': rf.feature_importances_,\n",
    "    'XGB Gain': xgb_model.feature_importances_,\n",
    "    'SHAP': shap_importance / shap_importance.sum()  # Normalize\n",
    "})\n",
    "\n",
    "# Normalize all for comparison\n",
    "for col in ['RF Impurity', 'XGB Gain', 'SHAP']:\n",
    "    comparison[col] = comparison[col] / comparison[col].sum()\n",
    "\n",
    "print(\"Feature Importance Comparison (normalized):\")\n",
    "print(comparison.round(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "comparison_melted = comparison.melt(id_vars='Feature', var_name='Method', value_name='Importance')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(feature_names))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, comparison['RF Impurity'], width, label='RF Impurity', edgecolor='black')\n",
    "ax.bar(x, comparison['XGB Gain'], width, label='XGB Gain', edgecolor='black')\n",
    "ax.bar(x + width, comparison['SHAP'], width, label='SHAP', edgecolor='black')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(feature_names, rotation=45, ha='right')\n",
    "ax.set_ylabel('Normalized Importance')\n",
    "ax.set_title('Feature Importance: Method Comparison')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Validation\n",
    "\n",
    "Check if model insights match domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected relationships (from process knowledge):\n",
    "# 1. Temperature: Positive effect (Arrhenius)\n",
    "# 2. Pressure: Positive effect (Le Chatelier)\n",
    "# 3. Catalyst: Positive effect (saturation curve)\n",
    "# 4. Residence time: Positive effect (reaction completion)\n",
    "# 5. Feed purity: Positive effect (less side reactions)\n",
    "# 6. Stirrer speed: Minor positive effect (mixing)\n",
    "\n",
    "# Check SHAP dependence trends\n",
    "print(\"Domain Validation:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for idx, name in enumerate(feature_names):\n",
    "    correlation = np.corrcoef(X_test[:, idx], shap_values[:, idx])[0, 1]\n",
    "    direction = \"Positive\" if correlation > 0 else \"Negative\"\n",
    "    print(f\"{name}: {direction} effect (r = {correlation:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary: The Interpretability Toolkit\n\n### Method Comparison\n\n| Method | Speed | Insight Type | Best For |\n|--------|-------|--------------|----------|\n| Feature importance | Fast | Global ranking | Quick overview |\n| SHAP summary | Moderate | Global + direction | Detailed analysis |\n| SHAP waterfall | Fast | Single prediction | Debugging, explanation |\n| Dependence plots | Fast | Feature relationships | Understanding effects |\n| Partial dependence | Moderate | Marginal effects | Smooth trends |\n| 2D PDP | Slow | Interactions | Discovering dependencies |\n\n### The Interpretation Workflow\n\n1. **Start with feature importance**: Quick overview of what matters\n2. **Use SHAP summary**: See direction and distribution of effects\n3. **Check domain validity**: Do the effects make physical sense?\n4. **Debug with waterfall**: Explain unexpected predictions\n5. **Explore with dependence plots**: Understand nonlinear relationships\n\n### Key Takeaways\n\n1. **Interpretability enables trust**: Stakeholders need to understand why\n2. **SHAP is theoretically grounded**: Use it as your primary tool\n3. **Validate against domain knowledge**: Models should make sense\n4. **Local + global**: You need both types of explanation\n5. **Different methods, different insights**: Use multiple approaches\n\n### Common Pitfalls\n\n- Trusting feature importance without checking direction of effects\n- Ignoring interactions (use 2D PDPs and SHAP dependence)\n- Not validating against domain knowledge\n- Over-interpreting small differences in importance\n- Forgetting that correlation \u2260 causation (even with SHAP)\n\n## Course Conclusion\n\nYou've now learned a complete toolkit for data science in chemical engineering:\n\n| Module | Skills |\n|--------|--------|\n| NumPy & Pandas | Data manipulation |\n| Dimensionality reduction | PCA, t-SNE, UMAP |\n| Regression | Linear, regularized, nonlinear |\n| Ensemble methods | Random Forest, XGBoost |\n| Clustering | K-means, hierarchical, DBSCAN |\n| Uncertainty | pycse, Gaussian Processes |\n| Interpretability | SHAP, partial dependence |\n\n**Now apply these tools to your own problems!** The best way to learn is by doing. Start with a real dataset from your research or industry, and work through the analysis pipeline we've covered."
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## The Catalyst Crisis: Chapter 13 - \"The Reveal\"\n\n*A story about interpretability, communication, and the power of explanation*\n\n---\n\nThe boardroom was larger than Alex expected. Twelve executives around a polished table, the ChemCorp logo glowing on the wall screen. Frank Morrison sat near the end, arms crossed, but with something that might have been pride in his expression.\n\n\"Thank you for your time,\" Professor Pipeline began. \"The Data Academy team has spent a semester investigating the catalyst crisis. Alex Chen will present the findings.\"\n\nAlex took a breath and clicked to the first slide.\n\n\"Eighteen months ago, ChemCorp's flagship reactor started producing inconsistent batches. Good yield, bad yield, no apparent pattern.\" She advanced to the t-SNE plot. \"We found the pattern.\"\n\nShe walked them through the journey. Missing data that wasn't random. An outlier that wasn't an error. Dimensions collapsing from two hundred to five. Models that predicted well but couldn't explain why.\n\n\"The answer came when we stopped asking 'what predicts yield' and started asking 'why do certain batches fail.'\" She pulled up the SHAP values\u2014the interpretability layer that explained individual predictions.\n\n\"This is a single batch that failed.\" The waterfall chart showed features pushing the prediction up and down. \"Temperature contributed positively\u2014it was in a good range. Pressure, neutral. But catalyst age pushed the prediction strongly negative. This batch used catalyst from Lot 7,392\u2014Cluster 3 material, manufactured in April of last year.\"\n\nShe clicked through more examples. The pattern was consistent. Every failed batch had the same story: catalyst from the March-August window, trace metal ratios slightly off, surface area slightly low. Each property within spec. The combination, deadly.\n\n\"We traced it back to your supplier.\" Final slide. \"They changed their calcination process in February. The change was undocumented and unannounced. The new process produces catalyst that meets all specifications individually but performs poorly in your reactor.\"\n\nThe VP of Operations spoke first. \"You're certain?\"\n\n\"We presented this analysis to the supplier yesterday. They confirmed the process change.\" Alex allowed herself a small smile. \"They're implementing corrective action.\"\n\nSilence around the table. Then, slowly, nods. The skepticism she'd sensed when she walked in was dissolving.\n\nFrank Morrison spoke up. \"I've been running reactors for thirty years. I know this plant better than anyone.\" He paused. \"I couldn't have found this. Not without the data analysis. Not without the modeling.\" He looked at Alex. \"I was wrong to be skeptical.\"\n\n\"You were right to be skeptical,\" Alex said. \"Until we could explain why the model was right, skepticism was reasonable. Predictions without explanations are just black boxes. You needed to understand.\"\n\nAs the meeting wrapped up, Professor Pipeline caught Alex's eye. The smallest of nods.\n\nLater, at the Data Academy mystery board, Alex removed the sticky notes one by one. Eighteen clues, accumulated over a semester, now resolved into a single answer.\n\nShe saved the last note for her notebook: **The mystery wasn't in the data. It was in what we assumed we already knew.**\n\n---\n\n## Epilogue\n\nThree weeks after the presentation, Alex sat in the graduate student lounge, coffee in hand. The semester was over. The mystery was solved. And yet...\n\nMaya appeared, dropping into the chair across from her. \"You look thoughtful.\"\n\n\"I keep thinking about what's next.\" Alex turned her cup in her hands. \"This was one problem. One reactor. There are thousands of problems like this\u2014hidden patterns, unexplained failures, data no one's asking the right questions about.\"\n\n\"So you want to keep doing this?\"\n\n\"I want to get better at it.\" Alex thought about the semester\u2014the struggles, the breakthroughs, the late nights and unexpected connections. \"I came back to school because I wanted to understand, not just apply. I think I understand a little more now.\"\n\nJordan and Sam joined them, the team reuniting one last time.\n\n\"Frank Morrison called me,\" Sam said. \"He wants to know if we can consult on another plant.\"\n\nMaya laughed. \"Didn't he hate us at first?\"\n\n\"He hated what he didn't understand.\" Alex smiled. \"Once he understood, he became an advocate. That's usually how it works.\"\n\nShe looked around at her team\u2014the CS expert who learned chemistry, the pharma veteran who found his voice, the star student who discovered humility. They'd started as strangers. They'd become colleagues. Maybe friends.\n\n\"Whatever's next,\" Alex said, \"we should do it together.\"\n\nThe Data Academy had one final update to her profile: *Rank: Data Scientist*\n\nBut that wasn't the real accomplishment. The real accomplishment was simpler: she'd walked into a room full of skeptics, with data and models and uncertainty, and she'd helped them see what they'd been missing.\n\nThat was the work. And it was only beginning.\n\n---\n\n*End of The Catalyst Crisis*\n\n---\n\n**Congratulations on completing Data Science and Machine Learning in Chemical Engineering!** You've learned the same tools Alex and her team used to solve the Catalyst Crisis. Now it's your turn to find the patterns hiding in your own data.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}