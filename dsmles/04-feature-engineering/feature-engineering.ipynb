{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "xw1yolbuhfn",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jkitchin/s26-06642/blob/main/dsmles/04-feature-engineering/feature-engineering.ipynb)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lecture, you will be able to:\n",
    "\n",
    "1. Understand why feature engineering often matters more than algorithm choice\n",
    "2. Create domain-informed features from raw measurements\n",
    "3. Apply common transformations (log, polynomial, interactions)\n",
    "4. Handle categorical variables appropriately\n",
    "5. Scale and normalize features for different algorithms\n",
    "6. Use feature selection to reduce dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivation",
   "metadata": {},
   "source": [
    "## Why Feature Engineering Matters\n",
    "\n",
    "Here's a truth that surprises many beginners: **the features you provide often matter more than the algorithm you choose**.\n",
    "\n",
    "Consider this scenario: you have temperature and pressure data from a reactor, and you want to predict reaction rate. A linear model on raw T and P might give R² = 0.6. But if you know the Arrhenius equation suggests rate depends on exp(-Ea/RT), creating that feature could push R² to 0.95—even with the same simple linear model.\n",
    "\n",
    "### The Feature Engineering Mindset\n",
    "\n",
    "Feature engineering is where **domain knowledge meets machine learning**. It's the process of:\n",
    "\n",
    "1. **Transforming** raw measurements into forms that better capture underlying relationships\n",
    "2. **Creating** new features that encode domain knowledge\n",
    "3. **Selecting** the most informative features and removing noise\n",
    "4. **Encoding** categorical and text data for numerical algorithms\n",
    "\n",
    "### Why Algorithms Can't Do This Automatically\n",
    "\n",
    "You might wonder: if deep learning can learn features automatically, why bother?\n",
    "\n",
    "- **Small data**: Most chemical engineering datasets have hundreds, not millions, of samples. Hand-crafted features help models learn from limited data.\n",
    "- **Interpretability**: Features like \"activation energy\" are meaningful; learned features like \"hidden unit 47\" are not.\n",
    "- **Physical constraints**: You can encode conservation laws, bounds, and symmetries that algorithms would have to learn from scratch.\n",
    "- **Efficiency**: A good feature reduces the hypothesis space, making training faster and more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domain-features-intro",
   "metadata": {},
   "source": [
    "## Domain-Informed Feature Creation\n",
    "\n",
    "The most powerful features come from understanding your system. Let's see this with a chemical kinetics example.\n",
    "\n",
    "### Example: Reaction Rate Prediction\n",
    "\n",
    "We have experimental data: temperature (K), concentration (mol/L), and observed reaction rate. A naive approach treats T and C as independent features. But we know from chemical kinetics:\n",
    "\n",
    "$$r = k \\cdot C^n = A \\cdot e^{-E_a/RT} \\cdot C^n$$\n",
    "\n",
    "This suggests features like:\n",
    "- **1/T**: Appears in Arrhenius equation\n",
    "- **log(r) vs 1/T**: Should be linear if Arrhenius holds\n",
    "- **log(C)**: If reaction order n ≠ 1, log-transform helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kinetics-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate reaction rate data with Arrhenius kinetics\n",
    "# r = A * exp(-Ea/RT) * C^n\n",
    "A = 1e8  # pre-exponential factor\n",
    "Ea = 50000  # activation energy, J/mol\n",
    "R = 8.314  # gas constant\n",
    "n = 1.5  # reaction order\n",
    "\n",
    "# Generate experimental conditions\n",
    "T = np.random.uniform(300, 400, 100)  # Temperature in K\n",
    "C = np.random.uniform(0.1, 2.0, 100)  # Concentration in mol/L\n",
    "\n",
    "# True rate with some measurement noise\n",
    "k = A * np.exp(-Ea / (R * T))\n",
    "rate_true = k * C**n\n",
    "rate_observed = rate_true * np.exp(np.random.normal(0, 0.1, 100))  # Log-normal noise\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'T': T,\n",
    "    'C': C,\n",
    "    'rate': rate_observed\n",
    "})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naive-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive approach: use raw T and C\n",
    "X_naive = df[['T', 'C']]\n",
    "y = df['rate']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_naive, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_naive = LinearRegression()\n",
    "model_naive.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Naive model R² (train): {model_naive.score(X_train, y_train):.3f}\")\n",
    "print(f\"Naive model R² (test):  {model_naive.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p2dsgz3076r",
   "source": "The naive model gives R² ≈ 0.79—not terrible, but it's missing something. The linear model is trying to fit what is fundamentally an *exponential* relationship with a straight line. It does okay because over a limited range, most curves can be approximated linearly. But we can do better by thinking about the physics.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engineered-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain-informed approach: use Arrhenius-inspired features\n",
    "# If r = A * exp(-Ea/RT) * C^n, then:\n",
    "# log(r) = log(A) - Ea/(RT) + n*log(C)\n",
    "# This is LINEAR in 1/T and log(C)!\n",
    "\n",
    "df['inv_T'] = 1 / df['T']\n",
    "df['log_C'] = np.log(df['C'])\n",
    "df['log_rate'] = np.log(df['rate'])\n",
    "\n",
    "X_engineered = df[['inv_T', 'log_C']]\n",
    "y_log = df['log_rate']\n",
    "\n",
    "X_train_e, X_test_e, y_train_e, y_test_e = train_test_split(\n",
    "    X_engineered, y_log, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model_engineered = LinearRegression()\n",
    "model_engineered.fit(X_train_e, y_train_e)\n",
    "\n",
    "print(f\"Engineered model R² (train): {model_engineered.score(X_train_e, y_train_e):.3f}\")\n",
    "print(f\"Engineered model R² (test):  {model_engineered.score(X_test_e, y_test_e):.3f}\")\n",
    "\n",
    "# Extract physical parameters!\n",
    "print(f\"\\nExtracted parameters:\")\n",
    "print(f\"  Reaction order n = {model_engineered.coef_[1]:.2f} (true: {n})\")\n",
    "print(f\"  Ea/R = {-model_engineered.coef_[0]:.0f} K (true: {Ea/R:.0f} K)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4lq85wu30b5",
   "source": "**Dramatic improvement!** R² jumped from 0.79 to 0.99 with the same linear regression algorithm. The only difference: we transformed the features based on physical knowledge.\n\n**What happened?**\n- The Arrhenius equation says: r = A·exp(-Ea/RT)·C^n\n- Taking the log: log(r) = log(A) - Ea/RT + n·log(C)\n- This is **linear** in 1/T and log(C)!\n\nBy transforming to the right feature space, we converted a nonlinear problem into a linear one. The coefficients now have direct physical meaning:\n- The coefficient on log(C) ≈ 1.5 gives us the reaction order (true value: 1.5) ✓\n- The coefficient on 1/T gives us Ea/R ≈ 6000 K, so Ea ≈ 50 kJ/mol (true value: 50 kJ/mol) ✓\n\n**The lesson**: Before reaching for complex nonlinear models, ask \"Is there a transformation that makes this linear?\"",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "domain-features-lesson",
   "metadata": {},
   "source": [
    "### Key Insight\n",
    "\n",
    "The same linear regression algorithm went from poor to excellent performance—not by tuning hyperparameters, but by **encoding domain knowledge into features**.\n",
    "\n",
    "Better yet, the coefficients now have physical meaning! This is feature engineering at its best: improving both accuracy and interpretability.\n",
    "\n",
    "### Common Domain Transformations in Chemical Engineering\n",
    "\n",
    "| Physical Law | Suggested Features |\n",
    "|--------------|--------------------|\n",
    "| Arrhenius kinetics | 1/T, exp(-1/T) |\n",
    "| Power laws | log(x), x^n |\n",
    "| Ideal gas | P*V, P/T |\n",
    "| Mass transfer | Re, Sc, Sh (dimensionless groups) |\n",
    "| Heat transfer | Nu, Pr, Gr |\n",
    "| Reaction equilibrium | Products/Reactants ratios |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scaling-intro",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Many algorithms are sensitive to feature scales. Consider predicting material properties from:\n",
    "- Temperature: 300-400 K\n",
    "- Pressure: 1-100 atm  \n",
    "- Concentration: 0.001-0.1 mol/L\n",
    "\n",
    "Without scaling, algorithms might think pressure is \"more important\" simply because it has larger numbers.\n",
    "\n",
    "### When Scaling Matters\n",
    "\n",
    "| Algorithm | Needs Scaling? | Why |\n",
    "|-----------|---------------|-----|\n",
    "| Linear/Logistic Regression | Yes, for regularization | L1/L2 penalties affected by scale |\n",
    "| SVM, SVR | Yes | Distance-based kernel computations |\n",
    "| K-Means, KNN | Yes | Distance-based |\n",
    "| PCA | Yes | Variance-based |\n",
    "| Decision Trees | No | Split thresholds adapt to scale |\n",
    "| Random Forest, XGBoost | No | Tree-based, scale-invariant |\n",
    "\n",
    "### Two Common Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scaling-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data with different scales\n",
    "data = pd.DataFrame({\n",
    "    'temperature_K': [300, 350, 400, 450, 500],\n",
    "    'pressure_atm': [1, 25, 50, 75, 100],\n",
    "    'conc_mol_L': [0.001, 0.025, 0.050, 0.075, 0.100]\n",
    "})\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "print(f\"\\nStandard deviations: {data.std().values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-scaler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler: zero mean, unit variance\n",
    "# z = (x - mean) / std\n",
    "# Good for: algorithms assuming normally distributed features\n",
    "\n",
    "scaler_standard = StandardScaler()\n",
    "data_standard = pd.DataFrame(\n",
    "    scaler_standard.fit_transform(data),\n",
    "    columns=data.columns\n",
    ")\n",
    "\n",
    "print(\"StandardScaler (z-score normalization):\")\n",
    "print(data_standard.round(3))\n",
    "print(f\"\\nMeans: {data_standard.mean().values.round(10)}\")\n",
    "print(f\"Stds:  {data_standard.std().values.round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minmax-scaler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMaxScaler: scales to [0, 1] range\n",
    "# x_scaled = (x - min) / (max - min)\n",
    "# Good for: bounded features, neural networks with sigmoid activations\n",
    "\n",
    "scaler_minmax = MinMaxScaler()\n",
    "data_minmax = pd.DataFrame(\n",
    "    scaler_minmax.fit_transform(data),\n",
    "    columns=data.columns\n",
    ")\n",
    "\n",
    "print(\"MinMaxScaler (0-1 normalization):\")\n",
    "print(data_minmax.round(3))\n",
    "print(f\"\\nMin: {data_minmax.min().values}\")\n",
    "print(f\"Max: {data_minmax.max().values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scaling-warning",
   "metadata": {},
   "source": [
    "### Critical Warning: Fit on Training Data Only!\n",
    "\n",
    "A common mistake is fitting the scaler on all data, then splitting. This causes **data leakage**—information from the test set contaminates training.\n",
    "\n",
    "```python\n",
    "# WRONG\n",
    "X_scaled = scaler.fit_transform(X)  # Sees all data!\n",
    "X_train, X_test = train_test_split(X_scaled)\n",
    "\n",
    "# CORRECT\n",
    "X_train, X_test = train_test_split(X)\n",
    "scaler.fit(X_train)  # Learn parameters from training only\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Apply same transformation\n",
    "```\n",
    "\n",
    "Use scikit-learn Pipelines to avoid this mistake (covered in the regularization module)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polynomial-intro",
   "metadata": {},
   "source": [
    "## Polynomial and Interaction Features\n",
    "\n",
    "When you don't have domain knowledge suggesting specific transformations, polynomial features provide a general way to capture nonlinearity.\n",
    "\n",
    "### The Tradeoff\n",
    "\n",
    "For features [x₁, x₂] with degree=2:\n",
    "- **Output**: [1, x₁, x₂, x₁², x₁x₂, x₂²]\n",
    "- You get interactions (x₁x₂) and nonlinear terms (x₁²)\n",
    "\n",
    "**The danger**: Feature explosion! With d features and degree p:\n",
    "- Number of terms = C(d+p, p) = (d+p)! / (d! × p!)\n",
    "- 10 features, degree 3 → 286 terms\n",
    "- 50 features, degree 3 → 23,426 terms\n",
    "\n",
    "More features than samples = guaranteed overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polynomial-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate polynomial features\n",
    "X_simple = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "print(\"Original features (x1, x2):\")\n",
    "print(X_simple)\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_simple)\n",
    "\n",
    "print(f\"\\nPolynomial features (degree=2):\")\n",
    "print(f\"Feature names: {poly.get_feature_names_out()}\")\n",
    "print(X_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polynomial-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature explosion demonstration\n",
    "from math import comb\n",
    "\n",
    "print(\"Number of polynomial features:\")\n",
    "print(f\"{'Features':<10} {'Degree 2':<12} {'Degree 3':<12} {'Degree 4':<12}\")\n",
    "print(\"-\" * 46)\n",
    "for d in [5, 10, 20, 50]:\n",
    "    n2 = comb(d + 2, 2) - 1  # exclude bias\n",
    "    n3 = comb(d + 3, 3) - 1\n",
    "    n4 = comb(d + 4, 4) - 1\n",
    "    print(f\"{d:<10} {n2:<12} {n3:<12} {n4:<12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interaction-only",
   "metadata": {},
   "source": [
    "### Interaction-Only Features\n",
    "\n",
    "Sometimes you want interactions (x₁×x₂) but not higher powers (x₁²). This is common when you believe features interact but each has a linear effect individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interaction-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction features only (no x^2 terms)\n",
    "poly_interact = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_interact = poly_interact.fit_transform(X_simple)\n",
    "\n",
    "print(\"Interaction-only features:\")\n",
    "print(f\"Feature names: {poly_interact.get_feature_names_out()}\")\n",
    "print(X_interact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "categorical-intro",
   "metadata": {},
   "source": [
    "## Handling Categorical Variables\n",
    "\n",
    "Many chemical engineering datasets include categorical variables:\n",
    "- Catalyst type (Pt, Pd, Ni, ...)\n",
    "- Solvent (water, ethanol, acetone, ...)\n",
    "- Reactor type (batch, CSTR, PFR)\n",
    "- Material phase (solid, liquid, gas)\n",
    "\n",
    "ML algorithms need numbers, not strings. How you encode categories affects model performance.\n",
    "\n",
    "### Label Encoding vs One-Hot Encoding\n",
    "\n",
    "| Method | Encoding | When to Use |\n",
    "|--------|----------|-------------|\n",
    "| Label | A→0, B→1, C→2 | Ordinal categories (low/medium/high) |\n",
    "| One-Hot | A→[1,0,0], B→[0,1,0], C→[0,0,1] | Nominal categories (no ordering) |\n",
    "\n",
    "**Warning**: Label encoding implies ordering! If you encode catalysts as Pt=0, Pd=1, Ni=2, the model thinks Pd is \"between\" Pt and Ni, which is chemically meaningless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "categorical-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catalyst screening dataset\n",
    "catalyst_data = pd.DataFrame({\n",
    "    'catalyst': ['Pt', 'Pd', 'Ni', 'Pt', 'Pd', 'Ni', 'Pt', 'Pd'],\n",
    "    'temperature': [350, 350, 350, 400, 400, 400, 450, 450],\n",
    "    'conversion': [0.65, 0.58, 0.42, 0.82, 0.75, 0.61, 0.91, 0.88]\n",
    "})\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(catalyst_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "label-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding - WRONG for nominal categories\n",
    "le = LabelEncoder()\n",
    "catalyst_data['catalyst_label'] = le.fit_transform(catalyst_data['catalyst'])\n",
    "\n",
    "print(\"Label encoding (problematic for nominal categories):\")\n",
    "print(f\"Classes: {le.classes_}\")\n",
    "print(catalyst_data[['catalyst', 'catalyst_label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "onehot-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding - CORRECT for nominal categories\n",
    "ohe = OneHotEncoder(sparse_output=False, drop='first')  # drop='first' avoids multicollinearity\n",
    "catalyst_encoded = ohe.fit_transform(catalyst_data[['catalyst']])\n",
    "\n",
    "print(\"One-hot encoding (correct for nominal categories):\")\n",
    "print(f\"Feature names: {ohe.get_feature_names_out()}\")\n",
    "print(catalyst_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "onehot-drop",
   "metadata": {},
   "source": [
    "### Why drop='first'?\n",
    "\n",
    "With 3 catalysts, we only need 2 indicator columns:\n",
    "- Pd=1, Pt=0 → It's Pd\n",
    "- Pd=0, Pt=1 → It's Pt  \n",
    "- Pd=0, Pt=0 → It must be Ni (the dropped category)\n",
    "\n",
    "This avoids **multicollinearity**: if all three columns existed, their sum would always equal 1, creating a linear dependency that causes problems for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pandas-dummies",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas alternative: pd.get_dummies()\n",
    "catalyst_dummies = pd.get_dummies(catalyst_data, columns=['catalyst'], drop_first=True)\n",
    "print(\"Using pd.get_dummies():\")\n",
    "print(catalyst_dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selection-intro",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "More features isn't always better. Problems with too many features:\n",
    "\n",
    "1. **Overfitting**: Model memorizes noise instead of learning patterns\n",
    "2. **Curse of dimensionality**: Data becomes sparse in high dimensions\n",
    "3. **Computational cost**: Training and prediction slow down\n",
    "4. **Interpretability**: Hard to explain a model with 1000 features\n",
    "\n",
    "### Three Approaches to Feature Selection\n",
    "\n",
    "| Approach | Method | Pros | Cons |\n",
    "|----------|--------|------|------|\n",
    "| Filter | Statistical tests (correlation, mutual info) | Fast, model-agnostic | Ignores feature interactions |\n",
    "| Wrapper | Train models with feature subsets | Considers interactions | Slow, overfitting risk |\n",
    "| Embedded | L1 regularization (Lasso) | Efficient, considers interactions | Model-specific |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selection-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with informative and noise features\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "# Informative features\n",
    "X1 = np.random.uniform(0, 10, n_samples)  # Strong predictor\n",
    "X2 = np.random.uniform(0, 5, n_samples)   # Moderate predictor\n",
    "X3 = np.random.uniform(0, 1, n_samples)   # Weak predictor\n",
    "\n",
    "# Noise features (no relationship with y)\n",
    "X_noise = np.random.randn(n_samples, 7)\n",
    "\n",
    "# Target: depends on X1, X2, X3 with different strengths\n",
    "y = 3*X1 + 1.5*X2 + 0.5*X3 + np.random.randn(n_samples)*2\n",
    "\n",
    "# Combine into DataFrame\n",
    "X = np.column_stack([X1, X2, X3, X_noise])\n",
    "feature_names = ['X1_strong', 'X2_moderate', 'X3_weak'] + [f'noise_{i}' for i in range(7)]\n",
    "df_select = pd.DataFrame(X, columns=feature_names)\n",
    "\n",
    "print(f\"Dataset shape: {df_select.shape}\")\n",
    "print(f\"True informative features: X1_strong, X2_moderate, X3_weak\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filter-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter method: SelectKBest with f_regression\n",
    "selector = SelectKBest(score_func=f_regression, k=5)\n",
    "selector.fit(X, y)\n",
    "\n",
    "# Get scores for each feature\n",
    "scores = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'f_score': selector.scores_,\n",
    "    'selected': selector.get_support()\n",
    "}).sort_values('f_score', ascending=False)\n",
    "\n",
    "print(\"Filter method (F-statistic):\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lhsgkv58rt",
   "source": "The F-scores clearly distinguish informative from noise features:\n\n- **X1_strong** (F≈180): Highest score, strongest relationship with y\n- **X2_moderate** (F≈25): Second highest, moderate relationship  \n- **X3_weak** (F≈2): Low score—the true weak predictor is hard to distinguish from noise\n- **Noise features** (F≈0-1): As expected, no significant relationship\n\nNotice that X3_weak barely beats some noise features! With only a coefficient of 0.5 in the true model, this feature contributes little to y. In a real analysis, you might reasonably exclude it—sometimes weak features add more noise than signal.\n\nThe filter method selected the top 5 features, which includes X1, X2, X3, and unfortunately 2 noise features. This illustrates the limitation: with noisy data and weak true effects, perfect selection is impossible.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasso-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedded method: Lasso regression\n",
    "# L1 regularization drives coefficients to exactly zero\n",
    "\n",
    "# Scale features first (important for Lasso)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "lasso = Lasso(alpha=0.5)\n",
    "lasso.fit(X_scaled, y)\n",
    "\n",
    "lasso_coefs = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': lasso.coef_,\n",
    "    'selected': lasso.coef_ != 0\n",
    "}).sort_values('coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"Embedded method (Lasso):\")\n",
    "print(lasso_coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selection-comparison",
   "metadata": {},
   "source": [
    "### Comparing Selection Methods\n",
    "\n",
    "Both methods correctly identified X1 and X2 as the most important features. The filter method is faster but treats each feature independently. Lasso considers features together and can handle correlated features better.\n",
    "\n",
    "**Practical advice**: Start with filter methods for quick exploration, then use Lasso or tree-based importance for final selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-data-intro",
   "metadata": {},
   "source": [
    "## Handling Missing Data\n",
    "\n",
    "Real datasets often have missing values. Before feature engineering, you need a strategy:\n",
    "\n",
    "| Strategy | When to Use | Implementation |\n",
    "|----------|-------------|----------------|\n",
    "| Drop rows | Few missing, random pattern | `df.dropna()` |\n",
    "| Drop columns | >50% missing in a feature | `df.drop(columns=[...])` |\n",
    "| Mean/median imputation | Numerical, random missing | `SimpleImputer(strategy='mean')` |\n",
    "| Mode imputation | Categorical | `SimpleImputer(strategy='most_frequent')` |\n",
    "| Flag + impute | Missingness is informative | Create `is_missing` column |\n",
    "\n",
    "**Warning**: Mean imputation reduces variance and can bias coefficients. For serious analysis, consider multiple imputation or model-based approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Data with missing values\n",
    "df_missing = pd.DataFrame({\n",
    "    'temperature': [300, 350, np.nan, 400, 450],\n",
    "    'pressure': [1, np.nan, 50, 75, 100],\n",
    "    'yield': [0.65, 0.72, 0.78, np.nan, 0.91]\n",
    "})\n",
    "\n",
    "print(\"Data with missing values:\")\n",
    "print(df_missing)\n",
    "print(f\"\\nMissing counts: {df_missing.isna().sum().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(df_missing),\n",
    "    columns=df_missing.columns\n",
    ")\n",
    "\n",
    "print(\"After mean imputation:\")\n",
    "print(df_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "workflow",
   "metadata": {},
   "source": [
    "## Feature Engineering Workflow\n",
    "\n",
    "A typical workflow:\n",
    "\n",
    "1. **Understand your data**: Check types, distributions, missing values\n",
    "2. **Handle missing data**: Impute or drop based on pattern and amount\n",
    "3. **Encode categoricals**: One-hot for nominal, ordinal encoding if ordered\n",
    "4. **Create domain features**: Use physical/chemical knowledge\n",
    "5. **Add polynomial/interaction terms**: If needed and data is sufficient\n",
    "6. **Scale features**: Required for many algorithms\n",
    "7. **Select features**: Remove noise, improve interpretability\n",
    "\n",
    "**Important**: Steps 5-7 should be done within cross-validation to avoid data leakage!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Domain knowledge is your superpower**: Physics-informed features often beat complex algorithms on simple features\n",
    "\n",
    "2. **Scaling matters for many algorithms**: Use StandardScaler or MinMaxScaler, but fit only on training data\n",
    "\n",
    "3. **Polynomial features explode quickly**: Be cautious with high degrees or many input features\n",
    "\n",
    "4. **Encode categories correctly**: One-hot for nominal, label for ordinal only\n",
    "\n",
    "5. **Feature selection improves models**: Filter methods are fast; Lasso is powerful\n",
    "\n",
    "### When in Doubt\n",
    "\n",
    "- Start simple: raw features, linear model\n",
    "- Add complexity only when simple fails\n",
    "- Always validate on held-out data\n",
    "- If you can derive a feature from physical laws, try it!\n",
    "\n",
    "### What's Next\n",
    "\n",
    "In the next module, we'll explore **dimensionality reduction**—another way to handle high-dimensional feature spaces by finding lower-dimensional representations that preserve important structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xnxoprxpydc",
   "source": "---\n\n## The Catalyst Crisis: \"Speaking the Right Language\"\n\n*Continued from Intermediate Pandas...*\n\n---\n\n\"R-squared of 0.52,\" Alex muttered, staring at her model results. \"That's... not great.\"\n\nShe'd been trying to predict batch quality from the sensor data for three days. Linear regression on temperature, pressure, flow rates—all the obvious inputs. The model captured half the variance and missed the other half completely.\n\nProfessor Pipeline appeared at her desk, coffee in hand. \"Stuck?\"\n\n\"The model doesn't work. I'm using all the right features—\"\n\n\"Are you?\" He sat down beside her. \"What do you know about reaction kinetics?\"\n\nAlex almost laughed. \"I did seven years of reaction engineering. I know Arrhenius backwards.\"\n\n\"Then why are you feeding your model raw temperature?\"\n\nShe blinked. \"Because... that's the measurement?\"\n\n\"But the reaction rate doesn't depend on temperature linearly, does it?\" He took a sip of coffee. \"If you know the physics, encode it. Transform temperature to one-over-T. The Arrhenius relationship becomes linear.\"\n\nAlex felt something click—the same feeling she got when a P&ID suddenly made sense. Of course. She knew this. She'd just forgotten to use what she knew.\n\nShe rebuilt the model with transformed features: 1/T for the Arrhenius relationship, log of concentration for reaction order effects, dimensionless groups where they made sense.\n\nR-squared: 0.91.\n\nSame data. Same algorithm. Different features. The model went from useless to useful because she'd finally asked the question in a language the physics understood.\n\n\"Domain knowledge isn't optional,\" Professor Pipeline said, watching her results. \"All those young coders who can implement any algorithm—they still need someone who knows what the numbers mean.\"\n\nLater, Maya found Alex in the lab, still refining her feature transformations. \"How'd you know to use one-over-T?\"\n\n\"Arrhenius equation. The rate constant depends exponentially on the inverse of temperature. If you log both sides—\"\n\nMaya held up her hands. \"Okay, I believe you. I just wouldn't have thought of it.\"\n\n\"And I wouldn't have thought to vectorize my loops. That's why we're on the same team.\"\n\nThe transformed features revealed something else: a strong interaction between temperature and catalyst age. The relationship wasn't just additive—it was multiplicative. Old catalyst at high temperature performed dramatically worse than either factor alone would predict.\n\nAlex added to the mystery board: **Temperature × catalyst age interaction. The combination matters more than either alone.**\n\n*To be continued...*",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}