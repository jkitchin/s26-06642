{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jkitchin/s26-06642/blob/main/dsmles/05-dimensionality-reduction/dimensionality-reduction.ipynb)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "! pip install -q pycse\nfrom pycse.colab import pdf",
   "metadata": {
    "tags": [
     "skip-execution",
     "remove-cell"
    ]
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Module 04: Dimensionality Reduction\n\nTechniques for exploring and visualizing high-dimensional data.\n\n## Learning Objectives\n\n1. Understand why dimensionality reduction is useful\n2. Apply PCA for linear dimensionality reduction\n3. Use t-SNE for visualization\n4. Use UMAP for visualization and preprocessing\n5. Interpret reduced representations\n6. Apply to chemical engineering datasets"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport umap"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} dimensionality reduction, curse of dimensionality\n```\n\n\n## The Curse of Dimensionality\n\nReal datasets in chemical engineering often have many features:\n- **Process data**: A modern chemical plant might have 100-500 sensors recording temperature, pressure, flow rates, compositions, and control signals every second\n- **Spectroscopy**: NIR or Raman spectra contain 1000s of wavelengths, each potentially carrying information about composition\n- **Molecular descriptors**: Cheminformatics representations of molecules can include 100s of computed properties (molecular weight, LogP, topological indices, etc.)\n- **Simulation data**: CFD or molecular dynamics outputs can have millions of degrees of freedom\n\n### Why is high dimensionality a problem?\n\n1. **Visualization**: We can only see in 2D (plots) or 3D. How do you \"look at\" 100-dimensional data?\n\n2. **Sparsity**: In high dimensions, data points become increasingly isolated. With 10 features, you need exponentially more samples to achieve the same \"density\" of coverage as in 2D. This makes it harder to find patterns.\n\n3. **Correlation**: Many features are often redundant. In a reactor, temperature at sensor 1 and sensor 2 may be highly correlated - they're not truly independent pieces of information.\n\n4. **Overfitting**: Models with many features relative to samples can memorize noise rather than learning true patterns.\n\n**Dimensionality reduction** addresses these problems by finding a lower-dimensional representation that preserves the important structure in your data. The key question is: what do we mean by \"important\"? Different methods make different choices here."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load catalyst characterization dataset\nimport pandas as pd\nimport numpy as np\n\nurl = \"https://raw.githubusercontent.com/jkitchin/s26-06642/main/dsmles/data/catalyst_characterization.csv\"\ndf = pd.read_csv(url)\n\n# Extract features and labels\nfeature_names = ['surface_area', 'pore_volume', 'acidity', 'crystallite_size',\n                 'metal_dispersion', 'reduction_temp', 'impurity_level', 'particle_size']\nX = df[feature_names].values\ny = df['catalyst_type'].values\n\nprint(f\"Dataset shape: {X.shape}\")\nprint(f\"Catalyst types: {np.unique(y)}\")\ndf.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing 8 dimensions is hard!\n",
    "# Let's look at pairwise relationships\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "colors = {'Type_A': 'blue', 'Type_B': 'red', 'Type_C': 'green'}\n",
    "\n",
    "pairs = [('surface_area', 'pore_volume'), ('acidity', 'crystallite_size'),\n",
    "         ('metal_dispersion', 'reduction_temp'), ('surface_area', 'acidity'),\n",
    "         ('pore_volume', 'particle_size'), ('impurity_level', 'metal_dispersion')]\n",
    "\n",
    "for ax, (x, y) in zip(axes.flat, pairs):\n",
    "    for cat in ['Type_A', 'Type_B', 'Type_C']:\n",
    "        mask = df['catalyst_type'] == cat\n",
    "        ax.scatter(df.loc[mask, x], df.loc[mask, y], c=colors[cat], label=cat, alpha=0.6)\n",
    "    ax.set_xlabel(x)\n",
    "    ax.set_ylabel(y)\n",
    "\n",
    "axes[0, 0].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### The limits of pairwise visualization\n\nThe plots above show 6 of the possible 28 pairwise combinations of our 8 features. Already we can see some separation between catalyst types, but:\n\n- We're missing information by only looking at 2 features at a time\n- Some pairs show good separation, others don't - which view is \"right\"?\n- With 100 features, we'd have 4,950 pairs - impossible to examine manually\n- Patterns that exist in the *combination* of many features are invisible in 2D slices\n\nThis is exactly why we need dimensionality reduction: to find a single 2D view that captures the most important structure from *all* features simultaneously.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} PCA, principal component analysis, explained variance\n```\n\n\n## Principal Component Analysis (PCA)\n\nPCA is the workhorse of dimensionality reduction. It finds new axes (principal components) that are:\n1. **Orthogonal** to each other (uncorrelated)\n2. **Ordered by variance** - PC1 captures the most variance, PC2 the second most, etc.\n\n### The key idea\nPCA asks: \"If I could only keep one direction in my data, which direction would preserve the most information?\" The answer is the direction of maximum variance. Then it asks the same question for the remaining directions, subject to being perpendicular to the ones already chosen.\n\n### Why variance?\nThis is a **design choice** with consequences. PCA assumes that variance = information. This works well when:\n- Differences between samples are spread across many features\n- Noise is relatively small compared to signal\n- The interesting structure is *linear* (lies along straight lines/planes)\n\nIt works poorly when:\n- The interesting structure is nonlinear (curved manifolds)\n- Important differences are subtle compared to unimportant large variations\n- Outliers dominate the variance"
  },
  {
   "cell_type": "markdown",
   "source": "### The critical choice: to scale or not to scale?\n\nBefore applying PCA, we must decide whether to standardize our features. This choice matters enormously:\n\n**Without scaling**: Features with larger numerical ranges dominate. If surface area ranges from 50-200 m²/g but impurity level ranges from 0.01-0.1 wt%, PCA will essentially ignore impurities - they contribute almost nothing to total variance.\n\n**With scaling** (StandardScaler): Each feature is transformed to have mean=0 and std=1. Now all features contribute equally *by default*, and PCA finds directions of maximum variance in this normalized space.\n\n**Which should you choose?**\n- **Scale** when features have different units or vastly different ranges (most common case)\n- **Don't scale** when features are already comparable and you want to preserve natural importance (e.g., all features are concentrations in the same units)\n\nIn chemical engineering, we almost always scale because we're combining heterogeneous measurements (temperatures, pressures, compositions, etc.).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data: extract features and scale\n",
    "X = df[feature_names].values\n",
    "y = df['catalyst_type'].values\n",
    "\n",
    "# Scaling is important for PCA!\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Original shape: {X.shape}\")\n",
    "print(f\"Mean of scaled data: {X_scaled.mean(axis=0).round(2)}\")\n",
    "print(f\"Std of scaled data: {X_scaled.std(axis=0).round(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Explained variance\n",
    "print(\"Explained variance ratio per component:\")\n",
    "for i, var in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f\"  PC{i+1}: {var:.3f} ({var*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nCumulative variance with 2 components: {pca.explained_variance_ratio_[:2].sum():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scree plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Individual variance\n",
    "axes[0].bar(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "            pca.explained_variance_ratio_, edgecolor='black')\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0].set_title('Scree Plot')\n",
    "\n",
    "# Cumulative variance\n",
    "cumulative = np.cumsum(pca.explained_variance_ratio_)\n",
    "axes[1].plot(range(1, len(cumulative) + 1), cumulative, 'o-')\n",
    "axes[1].axhline(y=0.9, color='r', linestyle='--', label='90% threshold')\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance')\n",
    "axes[1].set_title('Cumulative Variance')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "```{index} scree plot\n```\n\n\n### How many components should you keep?\n\nThis is one of the most common questions in PCA, and there's no single right answer. Here are common strategies:\n\n1. **Explained variance threshold** (e.g., 90% or 95%): Keep enough components to explain a target amount of variance. This is arbitrary but practical.\n\n2. **Scree plot \"elbow\"**: Look for where the explained variance drops off sharply. Components before the elbow capture \"signal\"; those after capture \"noise.\"\n\n3. **Kaiser criterion**: Keep components with eigenvalue > 1 (only for scaled data). This means keeping components that explain more variance than a single original variable.\n\n4. **Cross-validation**: If using PCA for preprocessing before a predictive model, choose the number of components that gives best prediction performance.\n\n**The tradeoff**: More components = more information preserved, but also more complexity and potential noise. Fewer components = simpler, more robust representation, but you may lose important details.\n\nFor visualization, we're usually stuck with 2 or 3 components. For preprocessing, use the cumulative variance plot to guide your choice.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize in 2D\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for cat in ['Type_A', 'Type_B', 'Type_C']:\n",
    "    mask = y == cat\n",
    "    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], label=cat, alpha=0.6, s=50)\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "plt.title('PCA of Catalyst Properties')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret principal components: loadings\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=[f'PC{i+1}' for i in range(len(feature_names))],\n",
    "    index=feature_names\n",
    ")\n",
    "\n",
    "print(\"PC Loadings (contribution of each feature):\")\n",
    "print(loadings[['PC1', 'PC2', 'PC3']].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "```{index} loadings\n```\n\n\n### Interpreting PCA: What do the components mean?\n\nA major advantage of PCA over other methods is **interpretability**. The loadings tell us how each original feature contributes to each principal component.\n\n**Reading the loadings table:**\n- Large positive loading: feature increases as PC increases\n- Large negative loading: feature decreases as PC increases  \n- Near-zero loading: feature doesn't contribute much to this PC\n\n**What to look for:**\n- Features with similar loadings are correlated and \"move together\"\n- Features with opposite signs are anti-correlated\n- PC1 often represents overall \"size\" or \"magnitude\" if many loadings have the same sign\n- Later PCs often capture contrasts between groups of features\n\nThe biplot below shows both samples and loadings together, making it easier to see which features drive the separation between groups.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biplot: visualize samples and loadings together\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Plot samples\n",
    "for cat in ['Type_A', 'Type_B', 'Type_C']:\n",
    "    mask = y == cat\n",
    "    ax.scatter(X_pca[mask, 0], X_pca[mask, 1], label=cat, alpha=0.5, s=30)\n",
    "\n",
    "# Plot loadings as arrows\n",
    "scale = 3  # Scale factor for visibility\n",
    "for i, feature in enumerate(feature_names):\n",
    "    ax.arrow(0, 0, loadings.iloc[i, 0]*scale, loadings.iloc[i, 1]*scale,\n",
    "             head_width=0.1, head_length=0.05, fc='black', ec='black')\n",
    "    ax.text(loadings.iloc[i, 0]*scale*1.1, loadings.iloc[i, 1]*scale*1.1, \n",
    "            feature, fontsize=9)\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "ax.set_title('PCA Biplot')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} t-SNE\n```\n\n\n## t-SNE: When Linear Isn't Enough\n\nPCA has a fundamental limitation: it can only find linear structure. If your data lies on a curved surface (a \"manifold\") in high-dimensional space, PCA will fail to capture that structure.\n\n**t-SNE** (t-distributed Stochastic Neighbor Embedding) addresses this by focusing on a different goal: preserve *local neighborhoods*. Points that are close together in the original space should remain close in the reduced space.\n\n### How t-SNE works (intuition)\n1. Compute pairwise similarities in high-dimensional space (who are each point's neighbors?)\n2. Create a random 2D layout\n3. Iteratively adjust the 2D positions to match the high-dimensional neighborhood relationships\n\n### The key tradeoff\nt-SNE sacrifices **global structure** for **local structure**. This means:\n- Clusters are usually well-separated and visually clear\n- But distances *between* clusters are meaningless\n- The overall arrangement of clusters is arbitrary\n\n**Important**: Never interpret t-SNE distances! Two clusters that appear far apart might actually be similar. Two that appear close might be very different. Only the *within-cluster* structure is meaningful."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for cat in ['Type_A', 'Type_B', 'Type_C']:\n",
    "    mask = y == cat\n",
    "    plt.scatter(X_tsne[mask, 0], X_tsne[mask, 1], label=cat, alpha=0.6, s=50)\n",
    "\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.title('t-SNE of Catalyst Properties')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of perplexity\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for ax, perp in zip(axes, [5, 30, 100]):\n",
    "    tsne = TSNE(n_components=2, perplexity=perp, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X_scaled)\n",
    "    \n",
    "    for cat in ['Type_A', 'Type_B', 'Type_C']:\n",
    "        mask = y == cat\n",
    "        ax.scatter(X_tsne[mask, 0], X_tsne[mask, 1], label=cat, alpha=0.6)\n",
    "    \n",
    "    ax.set_title(f'Perplexity = {perp}')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "```{index} perplexity\n```\n\n\n### Choosing perplexity: the local vs global tradeoff\n\nPerplexity is t-SNE's main hyperparameter. It roughly corresponds to \"how many neighbors should each point consider?\"\n\n- **Low perplexity (5-10)**: Focus on very local structure. You'll see tight, fragmented clusters. Good for finding fine-grained substructure, but may split natural groups.\n\n- **Medium perplexity (30-50)**: The typical default. Balances local and broader structure. Usually a good starting point.\n\n- **High perplexity (100+)**: Consider more neighbors, capturing broader patterns. Clusters may merge. Can approach PCA-like behavior at very high values.\n\n**Practical advice:**\n- Start with perplexity = 30 (or n_samples/3 if you have few samples)\n- If clusters look too fragmented, increase perplexity\n- If distinct groups are merging, decrease perplexity\n- Always try multiple values - t-SNE results can be sensitive to this choice\n- Perplexity must be less than n_samples",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "```{index} UMAP\n```\n\n\n## UMAP: The Best of Both Worlds?\n\nUMAP (Uniform Manifold Approximation and Projection) was developed in 2018 and has quickly become the go-to method for visualization. It addresses many of t-SNE's limitations:\n\n### Why UMAP often beats t-SNE\n\n1. **Speed**: UMAP is significantly faster, especially on large datasets (10,000+ points)\n2. **Global structure**: Unlike t-SNE, UMAP tries to preserve relationships between clusters, not just within them\n3. **Reproducibility**: With a fixed random seed, UMAP gives consistent results\n4. **New data**: UMAP can transform new points using a fitted model (t-SNE cannot)\n5. **Preprocessing**: UMAP embeddings can be used as features for downstream ML models\n\n### The tradeoffs\n\n- **Less interpretable than PCA**: No loadings or explained variance\n- **Hyperparameter sensitive**: Results depend on `n_neighbors` and `min_dist`\n- **Still nonlinear**: Can distort distances, though less than t-SNE\n- **Newer = less proven**: PCA has decades of theoretical backing; UMAP is more empirical\n\n### When to use UMAP\n\nUMAP is often the best default choice for visualization when:\n- You have more than a few hundred samples\n- You want to see both cluster structure and relationships between clusters\n- You might need to project new data later\n- Speed matters",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Fit UMAP\nreducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)\nX_umap = reducer.fit_transform(X_scaled)\n\n# Visualize\nplt.figure(figsize=(10, 8))\n\nfor cat in ['Type_A', 'Type_B', 'Type_C']:\n    mask = y == cat\n    plt.scatter(X_umap[mask, 0], X_umap[mask, 1], label=cat, alpha=0.6, s=50)\n\nplt.xlabel('UMAP 1')\nplt.ylabel('UMAP 2')\nplt.title('UMAP of Catalyst Properties')\nplt.legend()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Effect of n_neighbors parameter\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nfor ax, n_neighbors in zip(axes, [5, 15, 50]):\n    reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=0.1, random_state=42)\n    X_umap = reducer.fit_transform(X_scaled)\n    \n    for cat in ['Type_A', 'Type_B', 'Type_C']:\n        mask = y == cat\n        ax.scatter(X_umap[mask, 0], X_umap[mask, 1], label=cat, alpha=0.6)\n    \n    ax.set_title(f'n_neighbors = {n_neighbors}')\n    ax.legend()\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### UMAP's key parameters\n\nUMAP has two main hyperparameters that control the embedding:\n\n**`n_neighbors`** (similar to t-SNE's perplexity):\n- Controls the balance between local and global structure\n- Low values (5-15): Emphasize local structure, tighter clusters\n- High values (50-200): Emphasize global structure, more spread out\n- Default of 15 works well for most cases\n\n**`min_dist`** (unique to UMAP):\n- Controls how tightly points can be packed together\n- Low values (0.0-0.1): Points can clump tightly, good for seeing cluster density\n- High values (0.5-1.0): Points spread out more uniformly, better for seeing individual points\n- Default of 0.1 is usually good\n\n**Practical advice:**\n- Start with defaults (`n_neighbors=15`, `min_dist=0.1`)\n- If clusters are too fragmented, increase `n_neighbors`\n- If you can't see individual points, increase `min_dist`\n- UMAP is generally less sensitive than t-SNE to parameter choices",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## PCA vs t-SNE vs UMAP\n\n| Aspect | PCA | t-SNE | UMAP |\n|--------|-----|-------|------|\n| Type | Linear | Nonlinear | Nonlinear |\n| Preserves | Global structure | Local structure | Local + global |\n| Interpretable | Yes (loadings) | No | No |\n| Speed | Fast | Slow | Fast |\n| New data | Can project | Cannot project | Can project |\n| Use for | Features, preprocessing | Visualization | Visualization, preprocessing |"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Side-by-side comparison\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# PCA\nfor cat in ['Type_A', 'Type_B', 'Type_C']:\n    mask = y == cat\n    axes[0].scatter(X_pca[mask, 0], X_pca[mask, 1], label=cat, alpha=0.6, s=50)\naxes[0].set_xlabel('PC1')\naxes[0].set_ylabel('PC2')\naxes[0].set_title('PCA')\naxes[0].legend()\n\n# t-SNE\ntsne = TSNE(n_components=2, perplexity=30, random_state=42)\nX_tsne = tsne.fit_transform(X_scaled)\n\nfor cat in ['Type_A', 'Type_B', 'Type_C']:\n    mask = y == cat\n    axes[1].scatter(X_tsne[mask, 0], X_tsne[mask, 1], label=cat, alpha=0.6, s=50)\naxes[1].set_xlabel('t-SNE 1')\naxes[1].set_ylabel('t-SNE 2')\naxes[1].set_title('t-SNE')\naxes[1].legend()\n\n# UMAP\nreducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)\nX_umap = reducer.fit_transform(X_scaled)\n\nfor cat in ['Type_A', 'Type_B', 'Type_C']:\n    mask = y == cat\n    axes[2].scatter(X_umap[mask, 0], X_umap[mask, 1], label=cat, alpha=0.6, s=50)\naxes[2].set_xlabel('UMAP 1')\naxes[2].set_ylabel('UMAP 2')\naxes[2].set_title('UMAP')\naxes[2].legend()\n\nplt.tight_layout()\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep components that explain 90% of variance\n",
    "pca_90 = PCA(n_components=0.90)\n",
    "X_reduced = pca_90.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Original features: {X_scaled.shape[1]}\")\n",
    "print(f\"Reduced features: {X_reduced.shape[1]}\")\n",
    "print(f\"Explained variance: {pca_90.explained_variance_ratio_.sum():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## How to Choose: A Decision Guide\n\nWith three methods available, how do you decide which to use? Here's a practical decision tree:\n\n### What's your goal?\n\n**1. \"I need to reduce features for a downstream model\"**\n→ Use **PCA**. It's fast, deterministic, and you can choose how much variance to keep. The reduced features are uncorrelated, which helps many ML algorithms.\n\n**2. \"I need to understand which original features matter\"**\n→ Use **PCA**. It's the only method with interpretable loadings. You can explain what each component represents.\n\n**3. \"I just want to visualize clusters in my data\"**\n→ Use **UMAP** first. It's fast and usually gives good results. Fall back to t-SNE if UMAP doesn't show clear structure.\n\n**4. \"I need to project new data points onto an existing visualization\"**\n→ Use **UMAP** or **PCA**. t-SNE cannot transform new points.\n\n**5. \"I have a huge dataset (>100,000 samples)\"**\n→ Use **PCA** or **UMAP**. t-SNE will be too slow.\n\n### A recommended workflow\n\n1. **Always start with PCA** - it's fast and gives you explained variance as a sanity check\n2. **If PCA shows good separation** - you might not need anything fancier\n3. **If PCA shows overlapping groups** - try UMAP to see if there's nonlinear structure\n4. **If you're publishing or presenting** - try multiple methods and show the comparison (as we did above)\n\n### Common pitfalls to avoid\n\n- Don't forget to scale your data (especially for PCA)\n- Don't interpret distances in t-SNE plots\n- Don't use t-SNE/UMAP coordinates as features without careful validation\n- Don't conclude \"no structure\" from a single t-SNE run with default parameters\n- Don't over-interpret small clusters - they might be artifacts of the algorithm",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "! pip install -q jupyterquiz\nfrom jupyterquiz import display_quiz\n\ndisplay_quiz(\"https://raw.githubusercontent.com/jkitchin/s26-06642/main/dsmles/05-dimensionality-reduction/quizzes/dimensionality-reduction-quiz.json\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Recommended Reading\n\nThese resources provide deeper understanding of dimensionality reduction techniques:\n\n1. **[Scikit-learn Decomposition](https://scikit-learn.org/stable/modules/decomposition.html)** - Official documentation on PCA, ICA, NMF, and other decomposition methods. Includes mathematical details and practical guidance.\n\n2. **[How to Use t-SNE Effectively (Wattenberg et al.)](https://distill.pub/2016/misread-tsne/)** - An interactive article explaining how t-SNE works and common pitfalls in interpretation. Essential reading before using t-SNE on your data.\n\n3. **[UMAP: Uniform Manifold Approximation and Projection](https://umap-learn.readthedocs.io/en/latest/)** - Official UMAP documentation with excellent explanations of how it differs from t-SNE and when to use each method.\n\n4. **[A Tutorial on Principal Component Analysis (Shlens)](https://arxiv.org/abs/1404.1100)** - Clear mathematical exposition of PCA from first principles. Helps build intuition for what principal components actually represent.\n\n5. **[Visualizing Data using t-SNE (van der Maaten & Hinton, JMLR 2008)](https://www.jmlr.org/papers/v9/vandermaaten08a.html)** - The original t-SNE paper. Readable introduction to the algorithm and its advantages over linear methods like PCA.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nIn this module, we learned three complementary approaches to dimensionality reduction, each with distinct strengths:\n\n| Method | Best For | Key Parameter | Limitation |\n|--------|----------|---------------|------------|\n| **PCA** | Preprocessing, interpretation, fast exploration | n_components (variance threshold) | Linear only |\n| **t-SNE** | Visualizing cluster structure | perplexity (5-100) | Slow, no new data projection |\n| **UMAP** | General-purpose visualization | n_neighbors, min_dist | Less interpretable than PCA |\n\n### Key takeaways\n\n1. **Scaling matters**: Always standardize features before applying these methods (unless you have a specific reason not to)\n\n2. **No free lunch**: Each method makes tradeoffs\n   - PCA: interpretable but linear\n   - t-SNE: great clusters but no global structure\n   - UMAP: good balance but still a \"black box\"\n\n3. **Start simple**: Begin with PCA to understand your data's variance structure, then try nonlinear methods if needed\n\n4. **Be skeptical**: Visualizations can mislead. Don't over-interpret clusters or distances, especially with t-SNE\n\n5. **Try multiple methods**: When presenting results, showing PCA alongside UMAP gives a more complete picture\n\n### Looking ahead\n\nDimensionality reduction connects to many topics we'll cover later:\n- **Regression** (next module): PCA can help when you have many correlated features\n- **Clustering**: UMAP/t-SNE visualizations help validate cluster assignments\n- **Model interpretability**: PCA loadings reveal which features drive predictions\n\n## Next Steps\n\nNow we'll learn how to build predictive models using linear regression."
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## The Catalyst Crisis: Chapter 5 - \"Finding the Shape of the Problem\"\n\n*A story about dimensionality reduction and discovering hidden structure*\n\n---\n\nTwo hundred sensor columns stared back from Alex's screen. Two hundred dimensions of data, far too many to visualize or understand.\n\n\"The curse of dimensionality,\" Sam said, looking over her shoulder. \"That's what the professor called it. Too many features, not enough signal.\"\n\n\"There has to be structure here,\" Alex muttered. \"Real processes don't have two hundred independent degrees of freedom.\"\n\nShe ran PCA—principal component analysis—and watched the scree plot appear. The first five components captured 85% of the variance. Everything else was noise.\n\n\"Five dimensions,\" she said aloud. \"Not two hundred. Five.\"\n\nBut what did those five dimensions mean? PC1 was dominated by temperature-related variables. PC2 by nutrient concentrations. The abstract math was collapsing two hundred measurements into a handful of underlying factors.\n\nJordan pulled up a chair. He'd been quiet this week, more than usual. \"Try t-SNE. It shows clusters better than PCA.\"\n\nThe t-SNE plot appeared on screen, and Alex felt her breath catch. The batches weren't scattered randomly—they clumped into three distinct groups. Three modes of operation that no one had documented.\n\n\"Maya, pull up the yield data.\"\n\nThey overlaid the yields on the t-SNE plot. One cluster was almost entirely good batches. One was entirely bad. The third was mixed.\n\n\"The reactor has three natural operating regimes,\" Alex said slowly. \"And we've been treating them all as one.\"\n\nSam leaned in, finally fully engaged. \"What separates them?\"\n\nAlex traced the cluster boundaries back to the original features. The answer emerged gradually, like a photograph developing. The bad cluster shared one thing in common: they all used catalyst from lots manufactured in a specific time window.\n\n\"It's the catalyst,\" she said. \"Different lots of catalyst push the reactor into different regimes. We've been blaming randomness, but it's the catalyst supply chain.\"\n\nShe added to the mystery board: **Three operating regimes. Bad batches cluster together. Common factor: catalyst lot.**\n\nThat night, Professor Pipeline found the team still in the lab, surrounded by plots and printouts.\n\n\"Making progress?\"\n\n\"We found something.\" Alex pointed to the t-SNE plot. \"The batches cluster by catalyst lot. Something about the catalyst is driving the failures.\"\n\nHe nodded slowly. \"So what's the next question?\"\n\n\"Why. What's different about the catalyst lots? What changed?\"\n\n\"Good.\" He headed for the door, then paused. \"You're thinking like investigators now. Not just analysts.\"\n\n---\n\n*Continue to the next lecture to follow Alex and the team as they build predictive models...*",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}