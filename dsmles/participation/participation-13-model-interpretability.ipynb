{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "t4w5b99mvh",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jkitchin/s26-06642/blob/main/dsmles/participation/participation-13-model-interpretability.ipynb)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 13: Model Interpretability - Participation Exercises\n",
    "\n",
    "## Exercise Types\n",
    "\n",
    "| Type | Icon | Description | Time |\n",
    "|------|------|-------------|------|\n",
    "| **Reflection** | :thinking: | Personal reflection on concepts and connections | 3-5 min |\n",
    "| **Mini-Exercise** | :wrench: | Hands-on coding or problem solving | 5-10 min |\n",
    "| **Discussion** | :speech_balloon: | Pair or group discussion with neighbors | 5-7 min |\n",
    "| **Prediction** | :crystal_ball: | Make a prediction before seeing results | 2-3 min |\n",
    "| **Critique** | :mag: | Analyze code, results, or approaches | 5-7 min |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 13.1: Discussion - Explaining to Stakeholders\n",
    "\n",
    "**Type:** :speech_balloon: Discussion (5 min)\n",
    "\n",
    "Your black-box model recommends changing a reactor setpoint. The operator asks: \"Why?\"\n",
    "\n",
    "**Discuss:**\n",
    "1. What would be a satisfying answer?\n",
    "2. How might SHAP values help?\n",
    "3. When might \"trust the model\" be an acceptable answer? When is it not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discussion notes:*\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 13.2: Mini-Exercise - SHAP Interpretation\n",
    "\n",
    "**Type:** :wrench: Mini-Exercise (7 min)\n",
    "\n",
    "Interpret a SHAP summary plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Create interpretable data\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "\n",
    "temperature = np.random.uniform(300, 500, n)\n",
    "pressure = np.random.uniform(1, 10, n)\n",
    "catalyst = np.random.uniform(1, 5, n)\n",
    "\n",
    "# yield increases with temp and pressure, decreases with too much catalyst\n",
    "y = (0.1 * temperature + \n",
    "     5 * pressure + \n",
    "     10 * catalyst - catalyst**2 +  # Optimum catalyst loading\n",
    "     np.random.randn(n) * 3)\n",
    "\n",
    "X = np.column_stack([temperature, pressure, catalyst])\n",
    "feature_names = ['temperature', 'pressure', 'catalyst_loading']\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# If SHAP is available:\n",
    "try:\n",
    "    import shap\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X[:50])  # Sample for speed\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values, X[:50], feature_names=feature_names, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"SHAP not available - interpret feature importance instead:\")\n",
    "    for name, imp in zip(feature_names, model.feature_importances_):\n",
    "        print(f\"  {name}: {imp:.3f}\")\n",
    "\n",
    "# TASK: Based on the plot (or importances):\n",
    "# 1. Which feature has the largest impact?\n",
    "# 2. Is the effect of temperature positive or negative?\n",
    "# 3. What's unusual about catalyst_loading?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your interpretation:*\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 13.3: Reflection - The Right to Explanation\n",
    "\n",
    "**Type:** :thinking: Reflection (3 min)\n",
    "\n",
    "In many contexts (medical, legal, financial), there's growing demand for \"explainable AI.\"\n",
    "\n",
    "**Reflect:**\n",
    "1. In chemical engineering applications, when is model explainability critical?\n",
    "2. When might an unexplainable model be acceptable?\n",
    "3. How does interpretability relate to trust and safety?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your reflection:*\n",
    "\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}