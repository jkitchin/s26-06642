{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "i4mgl8rbq4",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jkitchin/s26-06642/blob/main/dsmles/participation/participation-09-nonlinear-methods.ipynb)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 09: Nonlinear Methods - Participation Exercises\n",
    "\n",
    "## Exercise Types\n",
    "\n",
    "| Type | Icon | Description | Time |\n",
    "|------|------|-------------|------|\n",
    "| **Reflection** | ü§î | Personal reflection on concepts and connections | 3-5 min |\n",
    "| **Mini-Exercise** | üîß | Hands-on coding or problem solving | 5-10 min |\n",
    "| **Discussion** | üí¨ | Pair or group discussion with neighbors | 5-7 min |\n",
    "| **Prediction** | üîÆ | Make a prediction before seeing results | 2-3 min |\n",
    "| **Critique** | üîç | Analyze code, results, or approaches | 5-7 min |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9.1: Prediction - Method Selection\n",
    "\n",
    "**Type:** üîÆ Prediction (3 min)\n",
    "\n",
    "For each scenario, predict which method would work best: **Linear Regression**, **Polynomial Regression**, **Decision Tree**, or **k-Nearest Neighbors**.\n",
    "\n",
    "| Scenario | Your Choice | Reasoning |\n",
    "|----------|-------------|----------|\n",
    "| Predicting yield from temperature (Arrhenius-like) | | |\n",
    "| Classifying materials into 5 categories based on properties | | |\n",
    "| Predicting property with many step-changes/thresholds | | |\n",
    "| Predicting output from 100 features, most are noise | | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fill in the table above*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9.2: Mini-Exercise - Overfitting Visualization\n",
    "\n",
    "**Type:** üîß Mini-Exercise (7 min)\n",
    "\n",
    "Visualize overfitting with polynomial regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Generate noisy data from a simple quadratic\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 1, 15).reshape(-1, 1)\n",
    "y = 2*X.ravel()**2 - X.ravel() + 0.5 + np.random.randn(15)*0.1\n",
    "\n",
    "X_plot = np.linspace(-0.1, 1.1, 100).reshape(-1, 1)\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "# TASK: Try degrees 1, 2, and 15\n",
    "# For each, plot the fit and observe what happens\n",
    "for i, degree in enumerate([1, 2, 15]):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    \n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    plt.scatter(X, y, color='blue', label='Data')\n",
    "    plt.plot(X_plot, model.predict(X_plot), color='red', label=f'Degree {degree}')\n",
    "    plt.ylim(-0.5, 2)\n",
    "    plt.title(f'Degree {degree}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# QUESTION: Which degree is best? How do you know?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your observation:*\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9.3: Discussion - Interpretability vs Performance\n",
    "\n",
    "**Type:** üí¨ Discussion (5 min)\n",
    "\n",
    "You're presenting model results to plant operators who need to understand *why* the model makes certain predictions.\n",
    "\n",
    "**Discuss:**\n",
    "1. Rank these models from most to least interpretable: Linear Regression, Random Forest, Neural Network, Decision Tree\n",
    "2. When might you sacrifice interpretability for performance?\n",
    "3. How could you make a black-box model more interpretable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discussion notes:*\n",
    "\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}