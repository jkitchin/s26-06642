{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "g5eld0me2n",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jkitchin/s26-06642/blob/main/dsmles/participation/participation-07-classification.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0891770f",
   "metadata": {},
   "source": [
    "# Module 07: Classification - Participation Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9ed672",
   "metadata": {},
   "source": [
    "## Exercise 7.1: Discussion - Choosing Metrics\n",
    "\n",
    "**Type:** ðŸ’¬ Discussion (5 min)\n",
    "\n",
    "You're building a classifier to detect faulty batches in a chemical plant. Only 2% of batches are faulty.\n",
    "\n",
    "**Discuss with a partner:**\n",
    "1. If you predict \"good\" for every batch, what's your accuracy?\n",
    "2. Why is accuracy a bad metric here?\n",
    "3. Which metric would you use instead: precision, recall, or F1? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9bc92c",
   "metadata": {},
   "source": [
    "*Discussion notes:*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb15ec7",
   "metadata": {},
   "source": [
    "## Exercise 7.2: Mini-Exercise - Confusion Matrix Interpretation\n",
    "\n",
    "**Type:** ðŸ”§ Mini-Exercise (6 min)\n",
    "\n",
    "Calculate metrics from a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1511c00f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T07:17:56.889949Z",
     "iopub.status.busy": "2026-01-12T07:17:56.889823Z",
     "iopub.status.idle": "2026-01-12T07:17:56.893946Z",
     "shell.execute_reply": "2026-01-12T07:17:56.893224Z"
    }
   },
   "outputs": [],
   "source": [
    "# A classifier for detecting equipment failures\n",
    "# Confusion matrix:\n",
    "#                 Predicted\n",
    "#              | Normal | Failure |\n",
    "# Actual Normal |   85   |   10    |\n",
    "# Actual Failure|    3   |    2    |\n",
    "\n",
    "# True Positives (TP) = correctly predicted failures = 2\n",
    "# True Negatives (TN) = correctly predicted normal = 85\n",
    "# False Positives (FP) = normal predicted as failure = 10\n",
    "# False Negatives (FN) = failure predicted as normal = 3\n",
    "\n",
    "TP, TN, FP, FN = 2, 85, 10, 3\n",
    "\n",
    "# TASK: Calculate these metrics\n",
    "# accuracy = ???\n",
    "# precision = ???  (of predicted failures, how many were real?)\n",
    "# recall = ???  (of actual failures, how many did we catch?)\n",
    "# f1 = ???\n",
    "\n",
    "# Which metric is most concerning? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b6e884",
   "metadata": {},
   "source": [
    "*Your calculations and interpretation:*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e07d93",
   "metadata": {},
   "source": [
    "## Exercise 7.3: Prediction - ROC Curves\n",
    "\n",
    "**Type:** ðŸ”® Prediction (3 min)\n",
    "\n",
    "You have two classifiers:\n",
    "- **Model A**: High precision (0.9), low recall (0.3)\n",
    "- **Model B**: Low precision (0.5), high recall (0.9)\n",
    "\n",
    "**Predict:** Sketch (mentally or on paper) where each model's operating point would be on an ROC curve. Which model has higher AUC?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c04b76",
   "metadata": {},
   "source": [
    "*Your prediction:*\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
