{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jkitchin/s26-06642/blob/main/dsmles/03-intermediate-pandas/intermediate-pandas.ipynb)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 03: Intermediate Pandas\n",
    "\n",
    "Advanced data manipulation techniques for real-world data analysis.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Use groupby for split-apply-combine operations\n",
    "2. Merge and join DataFrames\n",
    "3. Reshape data with pivot and melt\n",
    "4. Apply custom functions efficiently\n",
    "5. Work with datetime data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample experimental data\n",
    "np.random.seed(42)\n",
    "\n",
    "experiments = pd.DataFrame({\n",
    "    'experiment_id': [f'EXP{i:03d}' for i in range(1, 51)],\n",
    "    'date': pd.date_range('2024-01-01', periods=50, freq='D'),\n",
    "    'catalyst': np.random.choice(['Pt/Al2O3', 'Pd/Al2O3', 'Ru/Al2O3'], 50),\n",
    "    'temperature': np.random.choice([300, 350, 400, 450, 500], 50),\n",
    "    'pressure': np.random.uniform(1, 5, 50).round(1),\n",
    "    'conversion': np.random.uniform(0.3, 0.95, 50).round(3),\n",
    "    'selectivity': np.random.uniform(0.7, 0.98, 50).round(3)\n",
    "})\n",
    "\n",
    "experiments['yield'] = (experiments['conversion'] * experiments['selectivity'] * 100).round(1)\n",
    "experiments.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## GroupBy: The Split-Apply-Combine Pattern\n\nGroupBy is arguably the most powerful Pandas operation. It lets you answer questions like:\n- \"What's the average yield for each catalyst?\"\n- \"What's the best-performing temperature for each reactor?\"\n- \"How does conversion vary by day of week?\"\n\n### The Mental Model: Split-Apply-Combine\n\n1. **Split**: Divide data into groups based on one or more columns\n2. **Apply**: Perform some operation on each group\n3. **Combine**: Combine results back into a DataFrame\n\nThis is the Pandas equivalent of SQL's `GROUP BY`, but more flexible.\n\n### When to Use GroupBy\n\n- Comparing performance across categories (catalysts, reactors, operators)\n- Aggregating replicate experiments\n- Computing statistics within subgroups\n- Normalizing data within groups"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic groupby: mean by catalyst\n",
    "experiments.groupby('catalyst')['yield'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple aggregations\n",
    "experiments.groupby('catalyst')['yield'].agg(['mean', 'std', 'min', 'max', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by multiple columns\n",
    "experiments.groupby(['catalyst', 'temperature'])['yield'].mean().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different aggregations for different columns\n",
    "experiments.groupby('catalyst').agg({\n",
    "    'conversion': 'mean',\n",
    "    'selectivity': 'mean',\n",
    "    'yield': ['mean', 'std'],\n",
    "    'experiment_id': 'count'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named aggregations (cleaner column names)\n",
    "summary = experiments.groupby('catalyst').agg(\n",
    "    n_experiments=('experiment_id', 'count'),\n",
    "    mean_yield=('yield', 'mean'),\n",
    "    std_yield=('yield', 'std'),\n",
    "    max_yield=('yield', 'max')\n",
    ").round(2)\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom aggregation function\n",
    "def yield_range(x):\n",
    "    return x.max() - x.min()\n",
    "\n",
    "experiments.groupby('catalyst')['yield'].agg(['mean', yield_range])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Merging DataFrames: Combining Data Sources\n\nReal analysis often requires combining data from multiple sources:\n- Experiment results + catalyst properties\n- Sensor readings + equipment specifications\n- Lab data + literature values\n\n### The Four Types of Merges\n\n| Type | What It Keeps | Use When |\n|------|---------------|----------|\n| `inner` | Only matching rows | You only want complete records |\n| `left` | All rows from left table | Your main table is on the left, reference data on right |\n| `right` | All rows from right table | Your main table is on the right |\n| `outer` | All rows from both | You want to see what's missing |\n\n### The Key Concept: Join Keys\n\nThe `on` parameter specifies which column(s) to match. Make sure:\n- The columns have the same meaning in both tables\n- Values are comparable (watch out for \"Pt\" vs \"Pt/Al2O3\")\n- Missing keys are handled appropriately (check for NaN after merge)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create catalyst properties table\n",
    "catalyst_info = pd.DataFrame({\n",
    "    'catalyst': ['Pt/Al2O3', 'Pd/Al2O3', 'Ru/Al2O3', 'Rh/Al2O3'],\n",
    "    'metal_loading': [1.0, 0.5, 2.0, 0.3],  # wt%\n",
    "    'surface_area': [250, 280, 220, 300],  # m2/g\n",
    "    'cost_per_kg': [50000, 30000, 8000, 80000]  # USD\n",
    "})\n",
    "\n",
    "catalyst_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge experiments with catalyst info\n",
    "merged = experiments.merge(catalyst_info, on='catalyst', how='left')\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different merge types\n",
    "# how='left'   - Keep all rows from left DataFrame\n",
    "# how='right'  - Keep all rows from right DataFrame\n",
    "# how='inner'  - Keep only matching rows (intersection)\n",
    "# how='outer'  - Keep all rows from both (union)\n",
    "\n",
    "# Example: inner merge (Rh/Al2O3 exists in catalyst_info but not in experiments)\n",
    "inner_merge = experiments.merge(catalyst_info, on='catalyst', how='inner')\n",
    "print(f\"Left: {len(experiments)}, Right: {len(catalyst_info)}, Inner: {len(inner_merge)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Reshaping Data: Pivot and Melt\n\nData comes in different \"shapes,\" and sometimes you need to reshape it:\n\n### Long vs Wide Format\n\n**Long format** (tidy data):\n- Each row is one observation\n- Good for analysis and plotting\n- Example: Each row = (catalyst, temperature, yield)\n\n**Wide format** (spreadsheet-style):\n- Columns represent different conditions\n- Easier for humans to read\n- Example: Rows = catalysts, Columns = temperatures, Values = yields\n\n### When to Use Which\n\n| Format | Use For |\n|--------|---------|\n| Long | Most Pandas operations, plotting, ML |\n| Wide | Display tables, Excel export, certain visualizations |\n\n### The Transformation Tools\n\n- **`pivot_table()`**: Long → Wide (aggregate if needed)\n- **`melt()`**: Wide → Long (unpivot)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot: long to wide format\n",
    "# Create summary table: catalysts as rows, temperatures as columns\n",
    "pivot_table = experiments.pivot_table(\n",
    "    values='yield',\n",
    "    index='catalyst',\n",
    "    columns='temperature',\n",
    "    aggfunc='mean'\n",
    ").round(1)\n",
    "\n",
    "pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple values in pivot\n",
    "pivot_multi = experiments.pivot_table(\n",
    "    values=['conversion', 'selectivity'],\n",
    "    index='catalyst',\n",
    "    columns='temperature',\n",
    "    aggfunc='mean'\n",
    ").round(2)\n",
    "\n",
    "pivot_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt: wide to long format\n",
    "# Useful when data is in \"spreadsheet\" format\n",
    "wide_data = pd.DataFrame({\n",
    "    'catalyst': ['Pt', 'Pd', 'Ru'],\n",
    "    'yield_300K': [45, 42, 38],\n",
    "    'yield_400K': [65, 62, 55],\n",
    "    'yield_500K': [78, 75, 68]\n",
    "})\n",
    "\n",
    "print(\"Wide format:\")\n",
    "print(wide_data)\n",
    "\n",
    "# Convert to long format\n",
    "long_data = wide_data.melt(\n",
    "    id_vars='catalyst',\n",
    "    var_name='condition',\n",
    "    value_name='yield'\n",
    ")\n",
    "\n",
    "print(\"\\nLong format:\")\n",
    "print(long_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply and Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a function to each row\n",
    "def categorize_yield(row):\n",
    "    if row['yield'] >= 70:\n",
    "        return 'high'\n",
    "    elif row['yield'] >= 50:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'low'\n",
    "\n",
    "experiments['yield_category'] = experiments.apply(categorize_yield, axis=1)\n",
    "experiments[['experiment_id', 'yield', 'yield_category']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform: apply function but keep original shape\n",
    "# Useful for normalization within groups\n",
    "\n",
    "# Z-score normalization of yield within each catalyst group\n",
    "experiments['yield_zscore'] = experiments.groupby('catalyst')['yield'].transform(\n",
    "    lambda x: (x - x.mean()) / x.std()\n",
    ")\n",
    "\n",
    "experiments[['experiment_id', 'catalyst', 'yield', 'yield_zscore']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract date components\n",
    "experiments['year'] = experiments['date'].dt.year\n",
    "experiments['month'] = experiments['date'].dt.month\n",
    "experiments['week'] = experiments['date'].dt.isocalendar().week\n",
    "experiments['day_of_week'] = experiments['date'].dt.day_name()\n",
    "\n",
    "experiments[['date', 'year', 'month', 'week', 'day_of_week']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by time period\n",
    "weekly_avg = experiments.groupby('week')['yield'].mean()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "weekly_avg.plot(kind='line', marker='o')\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Average Yield (%)')\n",
    "plt.title('Weekly Average Yield')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample time series data\n",
    "experiments_ts = experiments.set_index('date')\n",
    "\n",
    "# Weekly average\n",
    "weekly = experiments_ts['yield'].resample('W').mean()\n",
    "print(weekly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method Chaining\n",
    "\n",
    "Combine operations elegantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of step-by-step:\n",
    "# df1 = experiments[experiments['temperature'] >= 400]\n",
    "# df2 = df1.groupby('catalyst')['yield'].mean()\n",
    "# df3 = df2.sort_values(ascending=False)\n",
    "\n",
    "# Use method chaining:\n",
    "result = (\n",
    "    experiments\n",
    "    .query('temperature >= 400')  # Filter\n",
    "    .groupby('catalyst')['yield']  # Group\n",
    "    .mean()  # Aggregate\n",
    "    .sort_values(ascending=False)  # Sort\n",
    "    .round(1)  # Format\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex analysis in one chain\n",
    "analysis = (\n",
    "    experiments\n",
    "    .assign(yield_category=lambda df: pd.cut(df['yield'], \n",
    "                                              bins=[0, 50, 70, 100],\n",
    "                                              labels=['low', 'medium', 'high']))\n",
    "    .groupby(['catalyst', 'yield_category'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary: The Data Wrangling Toolkit\n\nIntermediate Pandas is about transforming data to answer complex questions. Here's your toolkit:\n\n### Core Operations\n\n| Operation | When to Use | Key Method |\n|-----------|-------------|------------|\n| **GroupBy** | Compare across categories | `df.groupby('col').agg()` |\n| **Merge** | Combine data sources | `df1.merge(df2, on='col')` |\n| **Pivot** | Long → Wide format | `df.pivot_table()` |\n| **Melt** | Wide → Long format | `df.melt()` |\n| **Transform** | Apply function, keep shape | `df.groupby().transform()` |\n\n### The Method Chaining Philosophy\n\nInstead of:\n```python\ndf1 = df[df['temp'] > 400]\ndf2 = df1.groupby('catalyst')['yield'].mean()\ndf3 = df2.sort_values(ascending=False)\n```\n\nWrite:\n```python\n(df.query('temp > 400')\n   .groupby('catalyst')['yield']\n   .mean()\n   .sort_values(ascending=False))\n```\n\nBenefits: More readable, fewer intermediate variables, easier to modify.\n\n### Key Decisions\n\n| Situation | Choice |\n|-----------|--------|\n| Need to compare groups? | Use `groupby()` |\n| Need to combine tables? | Use `merge()` with appropriate `how=` |\n| Need to aggregate duplicates? | Use `pivot_table(aggfunc=...)` |\n| Need to normalize within groups? | Use `groupby().transform()` |\n\n### Common Gotchas\n\n- `groupby()` returns a GroupBy object, not a DataFrame—add `.mean()`, `.agg()`, etc.\n- `merge()` may create more rows than expected if keys aren't unique\n- `pivot_table()` aggregates by default (use `aggfunc` to control how)\n- Watch for column names becoming tuples after complex aggregations\n\n## Next Steps\n\nNow that you can manipulate data effectively, we'll move to dimensionality reduction—techniques for exploring and visualizing high-dimensional datasets before building models."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}