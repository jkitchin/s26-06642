{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jkitchin/s26-06642/blob/main/dsmles/01-numpy/numpy.ipynb)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "! pip install -q pycse\nfrom pycse.colab import pdf",
   "metadata": {
    "tags": [
     "skip-execution",
     "remove-cell"
    ]
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} NumPy, ndarray, numerical computing\n```\n\n\n# Module 01: NumPy Fundamentals\n\nNumPy is the foundation of numerical computing in Python. Nearly every data science library builds on NumPy arrays.\n\n## Learning Objectives\n\n1. Create and manipulate NumPy arrays\n2. Use vectorized operations (no loops!)\n3. Apply broadcasting for efficient computation\n4. Perform basic linear algebra\n5. Read data from files and work with structured arrays\n6. Generate random numbers for simulations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} vectorization, Python lists vs NumPy\n```\n\n\n## Why NumPy? The Foundation of Scientific Python\n\nBefore we dive in, it's worth understanding *why* NumPy exists and what problem it solves.\n\n### The Problem with Python Lists\n\nPython is a wonderful language, but it was designed for general-purpose programming, not numerical computing. Python lists are:\n\n- **Flexible**: Can hold mixed types (`[1, \"hello\", 3.14]`)\n- **Dynamic**: Can grow and shrink easily\n- **Slow**: Each element is a full Python object with overhead\n\nThis flexibility comes at a cost. When you're doing numerical work—solving differential equations, processing spectra, training machine learning models—you need to perform the *same operation on millions of numbers*. Python's flexibility becomes a liability.\n\n### NumPy's Solution\n\nNumPy provides a new data type: the **ndarray** (n-dimensional array). NumPy arrays are:\n\n- **Homogeneous**: All elements have the same type (e.g., all 64-bit floats)\n- **Contiguous**: Stored in a single block of memory\n- **Vectorized**: Operations apply to all elements at once, implemented in C\n\nThe result? NumPy can be 10-100x faster than Python lists for numerical operations.\n\n### Why This Matters for Chemical Engineering\n\nAlmost everything we do involves arrays of numbers:\n- Sensor data from reactors (time series of temperatures, pressures, flows)\n- Concentration profiles from simulations\n- Spectroscopic data (absorbance vs wavelength)\n- Training data for machine learning models\n\nNumPy is the foundation that Pandas, scikit-learn, TensorFlow, and virtually every scientific Python library builds upon. Master NumPy, and everything else becomes easier."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speed comparison\n",
    "python_list = list(range(1000000))\n",
    "numpy_array = np.arange(1000000)\n",
    "\n",
    "# Python list\n",
    "%timeit [x**2 for x in python_list]\n",
    "\n",
    "# NumPy array\n",
    "%timeit numpy_array**2"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "The timing results speak for themselves. On my machine, the NumPy operation is roughly **50-100x faster** than the Python list comprehension. For a million elements!\n\nThis difference compounds quickly:\n- Processing 1 million data points: seconds vs minutes\n- Training a machine learning model: minutes vs hours\n- Running a Monte Carlo simulation: hours vs days\n\nThe speed difference comes from:\n1. **Memory layout**: NumPy arrays are stored contiguously, enabling cache-efficient access\n2. **Type consistency**: No type checking needed for each element\n3. **Compiled code**: Operations run in optimized C/Fortran, not interpreted Python\n\nThis is why every scientific Python library uses NumPy under the hood.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} array creation, np.array, np.zeros, np.ones, np.linspace, np.arange\n```\n\n\n## Creating Arrays: Choosing the Right Approach\n\nThere are many ways to create NumPy arrays, and the right choice depends on your situation:\n\n| Situation | Method | Example |\n|-----------|--------|---------|\n| From existing data | `np.array()` | Converting a Python list |\n| Initialize to zeros | `np.zeros()` | Pre-allocating for a loop |\n| Initialize to ones | `np.ones()` | Creating masks or weights |\n| Evenly spaced values | `np.linspace()` | Plotting a function |\n| Integer sequence | `np.arange()` | Loop indices |\n| Random values | `np.random.uniform()` | Monte Carlo simulation |\n\n**Pro tip**: Avoid growing arrays in loops. If you know the final size, pre-allocate with `np.zeros()` and fill in values. This is much faster than appending."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Python list\n",
    "concentrations = np.array([0.1, 0.2, 0.5, 1.0, 2.0])  # mol/L\n",
    "print(\"Concentrations:\", concentrations)\n",
    "print(\"Type:\", type(concentrations))\n",
    "print(\"Shape:\", concentrations.shape)\n",
    "print(\"Dtype:\", concentrations.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common array creation functions\n",
    "zeros = np.zeros(5)\n",
    "ones = np.ones(5)\n",
    "temps = np.linspace(300, 500, 5)  # 5 evenly spaced points from 300 to 500\n",
    "pressures = np.arange(1, 11, 2)   # From 1 to 11, step 2\n",
    "\n",
    "print(\"Zeros:\", zeros)\n",
    "print(\"Ones:\", ones)\n",
    "print(\"Temperatures:\", temps)\n",
    "print(\"Pressures:\", pressures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D arrays (matrices)\n",
    "# Experimental data: rows = experiments, columns = [T, P, yield]\n",
    "experiments = np.array([\n",
    "    [300, 1.0, 45.2],\n",
    "    [350, 1.5, 52.8],\n",
    "    [400, 2.0, 68.1],\n",
    "    [450, 2.5, 75.4],\n",
    "    [500, 3.0, 82.0]\n",
    "])\n",
    "\n",
    "print(\"Shape:\", experiments.shape)  # (5 rows, 3 columns)\n",
    "print(\"\\nExperiment data:\")\n",
    "print(experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} indexing, slicing, advanced indexing\n```\n\n\n## Indexing and Slicing: Accessing Your Data\n\nNumPy's indexing is one of its most powerful features. Understanding it well will save you countless hours of writing loops.\n\n### The Key Insight\n\nIn Python, you typically write loops to process data. In NumPy, you **describe what you want**, and NumPy figures out how to get it efficiently.\n\nInstead of:\n```python\nresult = []\nfor i in range(len(data)):\n    if data[i] > threshold:\n        result.append(data[i])\n```\n\nYou write:\n```python\nresult = data[data > threshold]\n```\n\nThis isn't just shorter—it's 10-100x faster because the loop runs in compiled C code.\n\n### Types of Indexing\n\n1. **Basic indexing** (integers and slices): Returns views (no copy)\n2. **Advanced indexing** (boolean masks, integer arrays): Returns copies\n\nUnderstanding this distinction matters for performance and avoiding bugs."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D indexing\n",
    "temps = np.array([300, 350, 400, 450, 500])\n",
    "print(\"First element:\", temps[0])\n",
    "print(\"Last element:\", temps[-1])\n",
    "print(\"First three:\", temps[:3])\n",
    "print(\"Every other:\", temps[::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D indexing\n",
    "print(\"First row (experiment 1):\", experiments[0])\n",
    "print(\"First column (all temperatures):\", experiments[:, 0])\n",
    "print(\"Yields (third column):\", experiments[:, 2])\n",
    "print(\"Single element [2,1]:\", experiments[2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean indexing - very powerful!\n",
    "yields = experiments[:, 2]\n",
    "temps = experiments[:, 0]\n",
    "\n",
    "# Find experiments with yield > 60%\n",
    "high_yield = yields > 60\n",
    "print(\"High yield mask:\", high_yield)\n",
    "print(\"High yield values:\", yields[high_yield])\n",
    "print(\"Temps for high yield:\", temps[high_yield])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} boolean indexing, masking\n```\n\n\n## Vectorized Operations: Thinking in Arrays\n\nThis is the most important concept in NumPy. **Vectorization** means applying an operation to an entire array at once, rather than looping through elements.\n\n### Why Vectorization Matters\n\n1. **Speed**: Operations run in optimized C code, not interpreted Python\n2. **Clarity**: `y = A * np.exp(-Ea / (R * T))` is clearer than a 5-line loop\n3. **Fewer bugs**: No off-by-one errors, no forgetting to append\n\n### The Mental Shift\n\nIf you're coming from MATLAB, vectorization will feel natural. If you're coming from traditional programming, it requires a mental shift:\n\n**Loop thinking**: \"For each element, do this operation\"\n**Array thinking**: \"Apply this operation to all elements\"\n\nThe examples below demonstrate this shift. Notice how each calculation applies to entire arrays—no loops needed!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature conversion: K to °C\n",
    "temps_K = np.array([300, 350, 400, 450, 500])\n",
    "temps_C = temps_K - 273.15\n",
    "\n",
    "print(\"Kelvin:\", temps_K)\n",
    "print(\"Celsius:\", temps_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideal gas law: PV = nRT\n",
    "# Calculate molar volume V/n = RT/P\n",
    "\n",
    "R = 8.314  # J/(mol·K)\n",
    "T = np.linspace(300, 500, 5)  # K\n",
    "P = 101325  # Pa (1 atm)\n",
    "\n",
    "V_molar = R * T / P  # m³/mol\n",
    "V_molar_L = V_molar * 1000  # L/mol\n",
    "\n",
    "print(\"Temperature (K):\", T)\n",
    "print(\"Molar volume (L/mol):\", V_molar_L)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "These Arrhenius plots illustrate a classic chemical engineering relationship:\n\n**Left plot (linear scale)**: Shows the exponential growth of k with temperature. The rate constant increases by several orders of magnitude across this temperature range—this is why temperature control is so critical in reactors!\n\n**Right plot (semi-log scale)**: The hallmark of the Arrhenius equation. When we plot ln(k) vs 1/T, we get a straight line:\n- **Slope** = -Ea/R (gives activation energy)\n- **Intercept** = ln(A) (gives pre-exponential factor)\n\nThis linearization is how we traditionally extract kinetic parameters from experimental data. The linear fit on a semi-log plot is a signature of Arrhenius behavior.\n\nNotice how we generated these plots with just a few lines—no loops! The vectorized operations calculated k for all 100 temperature points simultaneously.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrhenius equation: k = A * exp(-Ea/RT)\n",
    "A = 1e13  # 1/s\n",
    "Ea = 80000  # J/mol\n",
    "R = 8.314  # J/(mol·K)\n",
    "T = np.linspace(300, 600, 100)\n",
    "\n",
    "k = A * np.exp(-Ea / (R * T))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(T, k)\n",
    "plt.xlabel('Temperature (K)')\n",
    "plt.ylabel('Rate constant k (1/s)')\n",
    "plt.title('Arrhenius Plot (linear)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.semilogy(1000/T, k)\n",
    "plt.xlabel('1000/T (1/K)')\n",
    "plt.ylabel('Rate constant k (1/s)')\n",
    "plt.title('Arrhenius Plot (log scale)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} broadcasting\n```\n\n\n## Broadcasting: The Magic Behind NumPy's Elegance\n\nBroadcasting is NumPy's way of handling operations between arrays of different shapes. It's what allows you to write `array + 5` (adding a scalar to every element) or multiply a matrix by a vector.\n\n### How Broadcasting Works\n\nWhen NumPy sees arrays of different shapes, it tries to make them compatible by:\n1. Comparing dimensions from right to left\n2. Dimensions match if they're equal, or if one of them is 1\n3. Arrays with a dimension of 1 are \"stretched\" to match the other\n\n### A Chemical Engineering Example\n\nSuppose you want to calculate reaction rates at multiple temperatures AND multiple concentrations. Instead of writing nested loops, broadcasting does it in one line.\n\n**The key**: Reshape one array so dimensions can be broadcast. `k.reshape(-1, 1)` makes k a column vector (5×1), and `C` stays a row (1×4). NumPy broadcasts to create a 5×4 grid of all combinations."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate reaction rates at multiple T and C combinations\n",
    "T = np.array([300, 350, 400, 450, 500])  # 5 temperatures\n",
    "C = np.array([0.1, 0.5, 1.0, 2.0])  # 4 concentrations\n",
    "\n",
    "# Rate = k * C, where k depends on T\n",
    "k = A * np.exp(-Ea / (R * T))  # Shape: (5,)\n",
    "\n",
    "# We want a 5x4 array of rates\n",
    "# Reshape k to (5, 1) and C stays (4,) → broadcasts to (5, 4)\n",
    "rates = k.reshape(-1, 1) * C\n",
    "\n",
    "print(\"k shape:\", k.shape)\n",
    "print(\"C shape:\", C.shape)\n",
    "print(\"rates shape:\", rates.shape)\n",
    "print(\"\\nRates (rows=T, cols=C):\")\n",
    "print(rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} aggregation, mean, std, min, max, median\n```\n\n\n## Aggregation Functions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated experimental yields\n",
    "yields = np.array([78.2, 82.1, 79.5, 81.3, 80.0, 79.8, 83.2, 77.9, 80.5, 81.1])\n",
    "\n",
    "print(f\"Mean: {np.mean(yields):.2f}%\")\n",
    "print(f\"Std: {np.std(yields):.2f}%\")\n",
    "print(f\"Min: {np.min(yields):.2f}%\")\n",
    "print(f\"Max: {np.max(yields):.2f}%\")\n",
    "print(f\"Median: {np.median(yields):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation along axes for 2D arrays\n",
    "# Rows = different catalysts, Cols = replicate experiments\n",
    "catalyst_yields = np.array([\n",
    "    [78, 82, 79, 81, 80],  # Catalyst A\n",
    "    [65, 68, 66, 64, 67],  # Catalyst B\n",
    "    [88, 91, 89, 87, 90],  # Catalyst C\n",
    "])\n",
    "\n",
    "print(\"Mean per catalyst (across replicates):\")\n",
    "print(np.mean(catalyst_yields, axis=1))  # axis=1 means across columns\n",
    "\n",
    "print(\"\\nMean per replicate (across catalysts):\")\n",
    "print(np.mean(catalyst_yields, axis=0))  # axis=0 means across rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} linear algebra, np.linalg, matrix operations\n```\n\n\n## Linear Algebra"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solving linear systems: Ax = b\n",
    "# Material balance: 3 reactions, 3 unknowns\n",
    "\n",
    "# Stoichiometric matrix\n",
    "A = np.array([\n",
    "    [1, -1, 0],\n",
    "    [0, 1, -1],\n",
    "    [1, 0, 1]\n",
    "])\n",
    "\n",
    "# Right-hand side (inlet flows)\n",
    "b = np.array([10, 5, 20])\n",
    "\n",
    "# Solve\n",
    "x = np.linalg.solve(A, b)\n",
    "print(\"Solution x:\", x)\n",
    "\n",
    "# Verify: Ax should equal b\n",
    "print(\"Verification A @ x:\", A @ x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Matrix operations\nM = np.array([[1, 2], [3, 4]])\n\nprint(\"Determinant:\", np.linalg.det(M))\nprint(\"\\nInverse:\")\nprint(np.linalg.inv(M))\n\neigenvalues, eigenvectors = np.linalg.eig(M)\nprint(\"\\nEigenvalues:\", eigenvalues)\nprint(\"\\nEigenvectors:\")\nprint(eigenvectors)"
  },
  {
   "cell_type": "markdown",
   "source": "```{index} file I/O, loadtxt, genfromtxt, save, load\n```\n\n\n## Reading Data from Files\n\nReal data comes from files—CSV exports from instruments, text files from simulations, or datasets shared by colleagues. NumPy provides several functions for loading numerical data:\n\n| Function | Best For | Features |\n|----------|----------|----------|\n| `np.loadtxt()` | Clean, simple data | Fast, simple API |\n| `np.genfromtxt()` | Messy data with headers/missing values | Handles edge cases |\n| `np.load()` / `np.save()` | NumPy binary format | Fast, preserves precision |\n\n### When to Use What\n\n- **Clean CSV with only numbers**: `np.loadtxt()` is fastest and simplest\n- **Headers, missing values, or mixed types**: Use `np.genfromtxt()` or consider Pandas\n- **Saving/loading NumPy arrays**: Use `np.save()` and `np.load()` for efficiency\n\n**Pro tip**: For complex tabular data with mixed types and labels, Pandas (next module) is often easier. But for pure numerical data, NumPy's functions are fast and lightweight.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create data files for the examples below (needed for Google Colab)\n# In a local environment, these files would already exist\n\nreactor_csv = \"\"\"# Reactor experiment data\n# Temperature (K), Pressure (atm), Flow Rate (L/min), Conversion (%)\n350,1.0,2.5,45.2\n375,1.2,2.8,52.1\n400,1.5,3.0,61.8\n425,1.8,3.2,68.4\n450,2.0,3.5,74.9\n475,2.2,3.8,79.3\n500,2.5,4.0,83.7\n\"\"\"\n\ncatalyst_txt = \"\"\"catalyst_id temperature pressure yield selectivity\nA1 350.0 1.0 45.2 78.5\nA2 375.0 1.2 52.1 81.3\nB1 400.0 1.5 61.8 85.2\nB2 425.0 1.8 68.4 87.9\nC1 450.0 2.0 74.9 89.1\nC2 475.0 2.2 79.3 90.4\nD1 500.0 2.5 83.7 91.8\n\"\"\"\n\nwith open('reactor_data.csv', 'w') as f:\n    f.write(reactor_csv)\n\nwith open('catalyst_experiments.txt', 'w') as f:\n    f.write(catalyst_txt)\n\nprint(\"Data files created: reactor_data.csv, catalyst_experiments.txt\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Loading a simple CSV file with np.loadtxt()\n# Skip comment lines (starting with #), use comma delimiter\ndata = np.loadtxt('reactor_data.csv', delimiter=',', comments='#')\n\nprint(\"Shape:\", data.shape)\nprint(\"Data type:\", data.dtype)\nprint(\"\\nFirst 3 rows:\")\nprint(data[:3])\n\n# Access columns by index\ntemp = data[:, 0]  # Temperature\npressure = data[:, 1]  # Pressure\nflow = data[:, 2]  # Flow rate\nconversion = data[:, 3]  # Conversion\n\nprint(\"\\nTemperatures:\", temp)\nprint(\"Conversions:\", conversion)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Loading data with headers using np.genfromtxt()\n# This is more flexible than loadtxt - handles headers and missing values\ndata = np.genfromtxt('catalyst_experiments.txt', \n                     skip_header=1,  # Skip the header row\n                     usecols=(1, 2, 3, 4))  # Skip the string column, use only numeric columns\n\nprint(\"Shape:\", data.shape)\nprint(\"\\nData (T, P, yield, selectivity):\")\nprint(data)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Saving and loading NumPy arrays (binary format)\n# This is the fastest way to save/load NumPy arrays\n\n# Save an array\ntemps = np.array([300, 350, 400, 450, 500])\nnp.save('temperatures.npy', temps)\n\n# Load it back\ntemps_loaded = np.load('temperatures.npy')\nprint(\"Loaded array:\", temps_loaded)\n\n# Save multiple arrays in one file\nnp.savez('experiment.npz', temperature=temps, conversion=conversion)\n\n# Load multiple arrays\nloaded = np.load('experiment.npz')\nprint(\"\\nArrays in file:\", list(loaded.keys()))\nprint(\"Temperature:\", loaded['temperature'])\nprint(\"Conversion:\", loaded['conversion'])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "```{index} structured arrays, record arrays, dtype\n```\n\n\n## Structured Arrays (Record Arrays)\n\nSometimes you need to work with heterogeneous data—not just numbers, but mixed types with named fields. NumPy's **structured arrays** (also called record arrays) allow you to define a custom data type with named fields of different types.\n\n### When to Use Structured Arrays\n\n- **Small datasets with mixed types**: When you need names but not full Pandas overhead\n- **Interfacing with C/Fortran code**: Structured arrays map directly to C structs\n- **Memory-mapped files**: Efficient access to large datasets on disk\n- **Performance-critical code**: Faster than Pandas for simple operations\n\n### When to Use Pandas Instead\n\nFor most tabular data work, **Pandas is easier and more feature-rich**. Use structured arrays when:\n- You need maximum performance for simple operations\n- You're interfacing with low-level code\n- You want to keep dependencies minimal\n\nThink of structured arrays as \"labeled arrays\" and Pandas as \"database tables.\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Creating a structured array with a custom dtype\n# Define the data type: (field_name, field_type)\ndt = np.dtype([\n    ('temperature', 'f8'),  # 64-bit float\n    ('pressure', 'f8'),     # 64-bit float\n    ('conversion', 'f8'),   # 64-bit float\n    ('catalyst', 'U10')     # Unicode string, max 10 chars\n])\n\n# Create the structured array\nexperiments = np.array([\n    (350.0, 1.0, 45.2, 'Pt/Al2O3'),\n    (400.0, 1.5, 62.8, 'Pd/C'),\n    (450.0, 2.0, 75.4, 'Ni/SiO2'),\n    (500.0, 2.5, 83.1, 'Pt/Al2O3'),\n], dtype=dt)\n\nprint(\"Structured array:\")\nprint(experiments)\nprint(\"\\nShape:\", experiments.shape)\nprint(\"Dtype:\", experiments.dtype)\nprint(\"Field names:\", experiments.dtype.names)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Accessing fields by name - much clearer than column indices!\nprint(\"All temperatures:\", experiments['temperature'])\nprint(\"All catalysts:\", experiments['catalyst'])\n\n# Access a single record (row)\nprint(\"\\nFirst experiment:\", experiments[0])\nprint(\"First experiment temp:\", experiments[0]['temperature'])\n\n# Boolean indexing works too\nhigh_conv = experiments[experiments['conversion'] > 60]\nprint(\"\\nHigh conversion experiments:\")\nprint(high_conv)\n\n# Find experiments with Pt catalyst\npt_experiments = experiments[experiments['catalyst'] == 'Pt/Al2O3']\nprint(\"\\nPt/Al2O3 experiments:\")\nprint(pt_experiments)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Loading structured data from a file with genfromtxt\n# Define the dtype to match the file structure\ndt = np.dtype([\n    ('catalyst_id', 'U10'),\n    ('temperature', 'f8'),\n    ('pressure', 'f8'),\n    ('yield', 'f8'),\n    ('selectivity', 'f8')\n])\n\n# Load with dtype specification\ndata = np.genfromtxt('catalyst_experiments.txt', \n                     dtype=dt,\n                     skip_header=1)\n\nprint(\"Loaded structured array:\")\nprint(data)\nprint(\"\\nAccess by field name:\")\nprint(\"Catalyst IDs:\", data['catalyst_id'])\nprint(\"Mean yield:\", np.mean(data['yield']))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Common dtype codes\n\n| Code | Type | Example |\n|------|------|---------|\n| `'f4'` | 32-bit float | `np.float32` |\n| `'f8'` | 64-bit float | `np.float64` |\n| `'i4'` | 32-bit integer | `np.int32` |\n| `'i8'` | 64-bit integer | `np.int64` |\n| `'U10'` | Unicode string (10 chars) | Fixed-length string |\n| `'S10'` | Byte string (10 bytes) | ASCII only |\n| `'?'` | Boolean | True/False |\n\n**Tip**: Use `'f8'` (64-bit float) as your default for numerical data—it matches Python's `float` precision.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} random numbers, random number generator, seeding, reproducibility\n```\n\n\n## Random Numbers: Essential for Simulation\n\nRandom number generation is fundamental to many scientific applications:\n\n- **Monte Carlo simulation**: Propagating uncertainty, sampling from distributions\n- **Machine learning**: Initializing weights, shuffling data, dropout\n- **Process simulation**: Modeling noise, disturbances, variability\n- **Experimental design**: Random sampling, bootstrap resampling\n\n### NumPy's Random Number Generator\n\nModern NumPy (1.17+) uses a new random number generator API. The recommended approach is:\n\n```python\nrng = np.random.default_rng(seed=42)  # Create a generator with a seed\n```\n\n**Why use a seed?** Seeds make your results reproducible. Running the same code twice with the same seed gives identical \"random\" numbers. This is essential for:\n- Debugging (reproducing exactly what happened)\n- Sharing results (others can reproduce your analysis)\n- Testing (consistent behavior across runs)\n\n**When NOT to seed**: When you genuinely need randomness (cryptography, production sampling)"
  },
  {
   "cell_type": "markdown",
   "source": "```{index} Monte Carlo simulation, uncertainty propagation\n```\n\n\nThis Monte Carlo simulation demonstrates **uncertainty propagation**—a critical skill for engineering.\n\n**What we did**:\n1. Our temperature measurement is uncertain: T = 400 ± 5 K (about 1.25% uncertainty)\n2. We sampled 10,000 possible temperatures from this distribution\n3. We calculated k for each sampled temperature\n4. We analyzed the resulting distribution of k values\n\n**What we learned**:\n- The input (T) is normally distributed and symmetric\n- The output (k) is **not** normally distributed—it's skewed!\n- A small uncertainty in T (1.25%) creates a much larger uncertainty in k (≈20-30%)\n\nThis asymmetric amplification happens because k depends *exponentially* on T. The Arrhenius equation is highly nonlinear, so symmetric input uncertainty becomes asymmetric output uncertainty.\n\n**Engineering implication**: When designing experiments or processes, you must propagate uncertainties through your models. The final uncertainty in your prediction can be much larger (or smaller) than the measurement uncertainties you started with.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Uniform random numbers\n",
    "uniform = rng.uniform(0, 1, 5)\n",
    "print(\"Uniform [0,1):\", uniform)\n",
    "\n",
    "# Normal distribution (Gaussian)\n",
    "normal = rng.normal(loc=50, scale=5, size=5)  # mean=50, std=5\n",
    "print(\"Normal (μ=50, σ=5):\", normal)\n",
    "\n",
    "# Random integers\n",
    "integers = rng.integers(1, 100, 5)\n",
    "print(\"Random integers [1, 100):\", integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo simulation: Propagating measurement uncertainty\n",
    "# Measure temperature: T = 400 ± 5 K\n",
    "# What's the uncertainty in rate constant k?\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "n_samples = 10000\n",
    "\n",
    "# Sample temperatures from normal distribution\n",
    "T_samples = rng.normal(400, 5, n_samples)\n",
    "\n",
    "# Calculate k for each sample\n",
    "k_samples = A * np.exp(-Ea / (R * T_samples))\n",
    "\n",
    "print(f\"k = {np.mean(k_samples):.4e} ± {np.std(k_samples):.4e}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "axes[0].hist(T_samples, bins=50, edgecolor='black')\n",
    "axes[0].set_xlabel('Temperature (K)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Input: Temperature Distribution')\n",
    "\n",
    "axes[1].hist(k_samples, bins=50, edgecolor='black')\n",
    "axes[1].set_xlabel('Rate constant k (1/s)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Output: Rate Constant Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```{index} views vs copies, common pitfalls\n```\n\n\n## Common Pitfalls: Mistakes Everyone Makes\n\nNumPy has some non-obvious behaviors that trip up beginners (and sometimes experts). Understanding these will save you debugging time.\n\n### 1. Views vs Copies\n\nThis is the most common source of NumPy bugs. When you slice an array, you get a **view**, not a copy. Modifying the view modifies the original!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitfall 1: Views vs Copies\n",
    "a = np.array([1, 2, 3, 4, 5])\n",
    "b = a[1:4]  # This is a VIEW, not a copy!\n",
    "\n",
    "b[0] = 99  # This modifies 'a' too!\n",
    "print(\"a:\", a)  # [1, 99, 3, 4, 5]\n",
    "\n",
    "# Use .copy() if you need an independent copy\n",
    "a = np.array([1, 2, 3, 4, 5])\n",
    "b = a[1:4].copy()\n",
    "b[0] = 99\n",
    "print(\"a (with copy):\", a)  # [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## The Catalyst Crisis: \"The Long Loop\"\n\n*Continued from Introduction...*\n\n---\n\nAlex's code had been running for twelve minutes.\n\nShe watched the progress bar crawl across her screen, each percentage point taking an eternity. All she was doing was calculating summary statistics for the ChemCorp sensor data—10,000 rows, nothing fancy. But her nested loops were grinding through the data like a car stuck in first gear.\n\n\"That's... a lot of for loops.\"\n\nAlex turned to find Maya peering at her screen with the expression of someone watching a car accident.\n\n\"It works,\" Alex said defensively.\n\n\"It works like a horse-drawn carriage works. Technically functional. Here—\" Maya pulled up a chair. \"Can I show you something?\"\n\nShe typed a few lines, replacing Alex's twelve nested loops with something that looked almost too simple:\n\n```python\nmeans = data.mean(axis=0)\nstds = data.std(axis=0)\n```\n\nThe code finished in 0.3 seconds.\n\n\"NumPy doesn't loop through elements,\" Maya explained. \"It operates on entire arrays at once. The loop still happens, but in compiled C code, not interpreted Python.\"\n\nAlex stared at the output. Same numbers. Fraction of the time. \"I spent three hours writing that loop structure.\"\n\n\"And now you'll never write it again.\" Maya grinned. \"That's learning, right?\"\n\nLater that night, Alex sat in the graduate student lounge, reworking her analysis with vectorized operations. The ChemCorp data transformed from an unwieldy beast into something almost manageable. She could process a day's worth of sensor readings in seconds instead of minutes.\n\nBut something else caught her attention. As she explored the data, she noticed gaps—timestamps where sensors should have reported but didn't. Not random gaps. Clusters of missing values, always during the same time windows.\n\nShe opened her notebook—the physical one, old habit from industry—and sketched the pattern. Gaps at 11 PM. Gaps at 3 AM. Gaps at 7 AM.\n\nShift changes.\n\nShe didn't know what it meant yet. But she'd learned something today: the data wasn't just numbers. It was evidence. And evidence always told a story if you knew how to listen.\n\nShe added a sticky note to the mystery board: **Missing data clusters around shift changes. Why?**\n\n*To be continued...*",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitfall 2: Integer division\n",
    "a = np.array([1, 2, 3])  # Integer array\n",
    "print(\"Integer array / 2:\", a / 2)  # Fine, returns floats\n",
    "\n",
    "# But be careful with floor division\n",
    "print(\"Integer array // 2:\", a // 2)  # Integer division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitfall 3: Shape mismatches\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([1, 2])  # Different length!\n",
    "\n",
    "try:\n",
    "    c = a + b\n",
    "except ValueError as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Self-Assessment Quiz\n\nTest your understanding of NumPy fundamentals with this quiz.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "! pip install -q jupyterquiz\nfrom jupyterquiz import display_quiz\n\ndisplay_quiz(\"https://raw.githubusercontent.com/jkitchin/s26-06642/main/dsmles/01-numpy/quizzes/numpy-quiz.json\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Recommended Reading\n\nThese resources will deepen your understanding of NumPy and numerical computing:\n\n1. **[NumPy User Guide](https://numpy.org/doc/stable/user/index.html)** - The official NumPy documentation with comprehensive coverage of array creation, indexing, broadcasting, and linear algebra. The \"NumPy fundamentals\" section is particularly valuable.\n\n2. **[From Python to NumPy by Nicolas Rougier](https://www.labri.fr/perso/nrougier/from-python-to-numpy/)** - A free online book that takes you from Python basics to advanced NumPy techniques. Excellent explanations of vectorization and why it matters for performance.\n\n3. **[NumPy: the absolute basics for beginners](https://numpy.org/doc/stable/user/absolute_beginners.html)** - Official beginner's guide that covers array fundamentals, broadcasting rules, and common operations with clear examples.\n\n4. **[100 NumPy Exercises](https://github.com/rougier/numpy-100)** - A collection of exercises ranging from beginner to expert level. Great for practicing and solidifying your NumPy skills.\n\n5. **[Array Programming with NumPy (Harris et al., Nature 2020)](https://www.nature.com/articles/s41586-020-2649-2)** - The definitive paper on NumPy's design and its role in scientific computing. Provides context for why NumPy is the foundation of the Python scientific ecosystem.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary: NumPy Mindset\n\nNumPy requires a different way of thinking about computation. Here's what to remember:\n\n### Core Concepts\n\n| Concept | Key Idea | Why It Matters |\n|---------|----------|----------------|\n| **Arrays** | Homogeneous, typed containers | 10-100x faster than lists |\n| **Vectorization** | Operate on entire arrays | Eliminates slow Python loops |\n| **Broadcasting** | Automatic shape matching | Clean code, fewer bugs |\n| **Views** | Slices share memory | Fast, but be careful with modifications |\n| **File I/O** | `loadtxt`, `genfromtxt`, `save/load` | Work with real data from files |\n| **Structured Arrays** | Named fields with mixed types | Bridge between arrays and tables |\n\n### When to Use NumPy\n\n- Any numerical computation with arrays of numbers\n- Building blocks for machine learning features\n- Scientific calculations (physics, chemistry, engineering)\n- Image and signal processing\n- Loading simple numerical data from CSV/text files\n\n### When to Use Pandas Instead\n\n- Tabular data with labeled columns\n- Mixed data types (numbers, strings, dates)\n- Time series with datetime indices\n- Data cleaning and exploration\n- Complex data with many column types and labels\n\n### Key Takeaways\n\n1. **Think in arrays**: Write operations that apply to whole arrays, not individual elements\n2. **Avoid loops**: If you're writing a for loop over array elements, there's probably a better way\n3. **Scale matters**: Always scale/normalize features before combining them\n4. **Copy when needed**: Use `.copy()` if you need to modify a slice without affecting the original\n5. **Seed for reproducibility**: Always set a random seed for reproducible results\n6. **Choose the right loader**: Use `loadtxt` for clean data, `genfromtxt` for messy data, Pandas for complex tabular data\n\n## Next Steps\n\nIn the next module, we'll learn Pandas, which builds on NumPy to provide labeled data structures for tabular data. If NumPy is the engine, Pandas is the dashboard—same power, but easier to interact with for real-world data."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}