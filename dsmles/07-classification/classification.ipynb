{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ub7ibr59x1n",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jkitchin/s26-06642/blob/main/dsmles/07-classification/classification.ipynb)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "vbqdhkg22gp",
   "source": "! pip install -q pycse\nfrom pycse.colab import pdf",
   "metadata": {
    "tags": [
     "skip-execution",
     "remove-cell"
    ]
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": "```{index} classification\n```\n\n\n# Classification\n\n## Learning Objectives\n\nBy the end of this lecture, you will be able to:\n\n1. Understand the difference between regression and classification\n2. Apply logistic regression for binary and multi-class problems\n3. Evaluate classifiers using appropriate metrics (accuracy, precision, recall, F1)\n4. Interpret confusion matrices and ROC curves\n5. Handle imbalanced classes\n6. Choose the right metric for your problem"
  },
  {
   "cell_type": "markdown",
   "id": "motivation",
   "metadata": {},
   "source": [
    "## From Regression to Classification\n",
    "\n",
    "So far, we've predicted continuous values: reaction rates, yields, material properties. But many engineering problems are **classification** tasks:\n",
    "\n",
    "- **Quality control**: Does this batch pass or fail specifications?\n",
    "- **Fault detection**: Is the reactor operating normally or abnormally?\n",
    "- **Material classification**: Is this sample crystalline or amorphous?\n",
    "- **Process monitoring**: Which of 5 operating regimes are we in?\n",
    "\n",
    "The key difference:\n",
    "- **Regression**: Predict a continuous number (y \u2208 \u211d)\n",
    "- **Classification**: Predict a category (y \u2208 {A, B, C, ...})\n",
    "\n",
    "### Why Not Just Use Linear Regression?\n",
    "\n",
    "You might try encoding classes as numbers (fail=0, pass=1) and using linear regression. This has problems:\n",
    "\n",
    "1. **Predictions outside [0,1]**: Linear regression can predict 1.3 or -0.2\u2014what do those mean?\n",
    "2. **Non-constant variance**: Errors near 0 and 1 behave differently than errors near 0.5\n",
    "3. **Wrong assumptions**: Linear regression assumes normal errors; binary outcomes are Bernoulli distributed\n",
    "\n",
    "We need a model designed for classification: **logistic regression**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             confusion_matrix, ConfusionMatrixDisplay,\n",
    "                             classification_report, roc_curve, roc_auc_score)\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logistic-intro",
   "metadata": {},
   "source": "```{index} logistic regression, sigmoid function\n```\n\n\n## Logistic Regression\n\nDespite its name, logistic regression is a **classification** algorithm. It models the probability of belonging to a class.\n\n### The Logistic (Sigmoid) Function\n\nThe key idea: transform the linear combination of features through a sigmoid function:\n\n$$P(y=1|x) = \\sigma(w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$$\n\nThe sigmoid maps any real number to (0, 1)\u2014perfect for probabilities!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sigmoid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "z = np.linspace(-6, 6, 100)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Sigmoid function\n",
    "axes[0].plot(z, sigmoid(z), 'b-', linewidth=2)\n",
    "axes[0].axhline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].set_xlabel('z = w\u00b7x + b')\n",
    "axes[0].set_ylabel('P(y=1)')\n",
    "axes[0].set_title('Sigmoid Function')\n",
    "axes[0].set_ylim(-0.1, 1.1)\n",
    "\n",
    "# Linear regression vs logistic regression\n",
    "x = np.linspace(0, 10, 50)\n",
    "y_linear = 0.15 * x - 0.25\n",
    "y_logistic = sigmoid(1.5 * x - 7)\n",
    "\n",
    "axes[1].plot(x, y_linear, 'r--', label='Linear regression', linewidth=2)\n",
    "axes[1].plot(x, y_logistic, 'b-', label='Logistic regression', linewidth=2)\n",
    "axes[1].axhline(0, color='gray', alpha=0.3)\n",
    "axes[1].axhline(1, color='gray', alpha=0.3)\n",
    "axes[1].fill_between(x, 0, 1, alpha=0.1, color='green')\n",
    "axes[1].set_xlabel('Feature value')\n",
    "axes[1].set_ylabel('Predicted probability')\n",
    "axes[1].set_title('Why Logistic Regression?')\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(-0.3, 1.3)\n",
    "axes[1].annotate('Valid probability\\nrange [0, 1]', xy=(8, 0.5), fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example-intro",
   "metadata": {},
   "source": [
    "### Example: Quality Control Classification\n",
    "\n",
    "Let's classify chemical batches as pass/fail based on process measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate quality control data\n",
    "# Features: temperature deviation, pressure deviation, impurity level\n",
    "np.random.seed(42)\n",
    "n_samples = 300\n",
    "\n",
    "# Generate features\n",
    "temp_dev = np.random.normal(0, 2, n_samples)  # Temperature deviation from setpoint\n",
    "pressure_dev = np.random.normal(0, 1.5, n_samples)  # Pressure deviation\n",
    "impurity = np.random.exponential(0.5, n_samples)  # Impurity level\n",
    "\n",
    "# Quality depends on all three (with some randomness)\n",
    "quality_score = -0.3*np.abs(temp_dev) - 0.4*np.abs(pressure_dev) - 1.5*impurity + 1\n",
    "quality_prob = sigmoid(quality_score * 2)\n",
    "quality = (np.random.random(n_samples) < quality_prob).astype(int)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'temp_deviation': temp_dev,\n",
    "    'pressure_deviation': pressure_dev,\n",
    "    'impurity_level': impurity,\n",
    "    'quality': quality  # 1 = pass, 0 = fail\n",
    "})\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['quality'].value_counts())\n",
    "print(f\"\\nPass rate: {df['quality'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ni8uminao8h",
   "source": "We've created a realistic quality control dataset where:\n- **~60% pass** (quality=1) and ~40% fail (quality=0)\n- Quality depends on operating conditions in physically sensible ways:\n  - Larger temperature deviations \u2192 worse quality\n  - Larger pressure deviations \u2192 worse quality  \n  - Higher impurity levels \u2192 worse quality\n\nThis is a **balanced** dataset (roughly equal classes). Later we'll see what happens with imbalanced data, where the minority class is much harder to predict.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-logistic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = df[['temp_deviation', 'pressure_deviation', 'impurity_level']]\n",
    "y = df['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features (important for logistic regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train logistic regression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "y_prob = clf.predict_proba(X_test_scaled)[:, 1]  # Probability of class 1\n",
    "\n",
    "print(\"Model coefficients (standardized):\")\n",
    "for name, coef in zip(X.columns, clf.coef_[0]):\n",
    "    print(f\"  {name}: {coef:.3f}\")\n",
    "print(f\"  intercept: {clf.intercept_[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07woxsnuuy2",
   "source": "**Interpreting the standardized coefficients:**\n\nAll three coefficients are **negative**, which makes physical sense:\n- **Impurity level** (-1.3): Strongest negative effect\u2014impurities kill quality\n- **Pressure deviation** (-0.5): Moderate effect\u2014process variations hurt\n- **Temperature deviation** (-0.3): Weakest effect in this data\n\nThe signs tell us: increasing any of these variables decreases the log-odds of passing, which matches our physical intuition. Unlike linear regression where coefficients have simple units, logistic regression coefficients are in **log-odds**\u2014a one-unit increase in standardized impurity decreases the log-odds of passing by 1.3.\n\nThe model learned the correct ranking of importance and direction of effects from data alone!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "metrics-intro",
   "metadata": {},
   "source": [
    "## Classification Metrics: Beyond Accuracy\n",
    "\n",
    "Accuracy seems like a natural metric: what fraction did we get right?\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{\\text{Correct predictions}}{\\text{Total predictions}}$$\n",
    "\n",
    "But accuracy can be **misleading**, especially with imbalanced classes.\n",
    "\n",
    "### The Imbalanced Class Problem\n",
    "\n",
    "Imagine a fault detection system where faults occur 1% of the time. A model that always predicts \"no fault\" achieves 99% accuracy\u2014but catches zero faults!\n",
    "\n",
    "We need metrics that consider the **types of errors**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confusion-matrix-intro",
   "metadata": {},
   "source": "```{index} confusion matrix\n```\n\n\n### The Confusion Matrix\n\nA confusion matrix breaks down predictions by actual vs predicted class:\n\n|  | Predicted Negative | Predicted Positive |\n|--|-------------------|--------------------|\n| Actual Negative | True Negative (TN) | False Positive (FP) |\n| Actual Positive | False Negative (FN) | True Positive (TP) |\n\n- **True Positive (TP)**: Correctly predicted positive\n- **True Negative (TN)**: Correctly predicted negative\n- **False Positive (FP)**: Predicted positive, actually negative (Type I error)\n- **False Negative (FN)**: Predicted negative, actually positive (Type II error)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=['Fail', 'Pass'])\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "ax.set_title('Confusion Matrix for Quality Control')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"True Negatives (correctly predicted fails): {tn}\")\n",
    "print(f\"False Positives (fails predicted as pass): {fp}\")\n",
    "print(f\"False Negatives (passes predicted as fail): {fn}\")\n",
    "print(f\"True Positives (correctly predicted passes): {tp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ggdwdqlbxst",
   "source": "**Reading the confusion matrix:**\n\nThe matrix shows all four outcomes:\n- **True Negatives (top-left)**: Correctly predicted failures\n- **False Positives (top-right)**: We said \"pass\" but it actually failed\u2014**escaped defects!**\n- **False Negatives (bottom-left)**: We said \"fail\" but it actually passed\u2014**wasted good product**\n- **True Positives (bottom-right)**: Correctly predicted passes\n\n**In quality control, FP and FN have different costs:**\n- **False Positives** (predicting pass when it's fail): Bad product ships to customer! Warranty claims, reputation damage.\n- **False Negatives** (predicting fail when it's pass): Good product gets scrapped. Lost revenue, but no customer impact.\n\nMost companies would rather have more FN than FP\u2014it's better to be overly cautious. This asymmetry is why accuracy alone isn't enough to evaluate a classifier.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "precision-recall",
   "metadata": {},
   "source": "```{index} precision, recall\n```\n\n\n### Precision and Recall\n\nFrom the confusion matrix, we derive more informative metrics:\n\n**Precision**: Of all predicted positives, how many were actually positive?\n$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n\n**Recall (Sensitivity)**: Of all actual positives, how many did we catch?\n$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n\n### The Precision-Recall Tradeoff\n\nYou usually can't maximize both:\n- High precision (few false positives) often means low recall (miss some positives)\n- High recall (catch most positives) often means low precision (more false alarms)\n\n**Which matters more depends on your problem:**\n\n| Scenario | Prioritize | Why |\n|----------|------------|-----|\n| Cancer screening | Recall | Don't miss any cancers, even if some false positives |\n| Spam filter | Precision | Don't lose important emails to spam folder |\n| Fault detection | Recall | Catch all faults, even if some false alarms |\n| Expensive inspection | Precision | Only trigger when likely true to save costs |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metrics-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Classification Metrics:\")\n",
    "print(f\"  Accuracy:  {accuracy:.3f}\")\n",
    "print(f\"  Precision: {precision:.3f}\")\n",
    "print(f\"  Recall:    {recall:.3f}\")\n",
    "print(f\"  F1 Score:  {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1-explanation",
   "metadata": {},
   "source": "```{index} F1 score\n```\n\n\n### F1 Score: Balancing Precision and Recall\n\nThe F1 score is the harmonic mean of precision and recall:\n\n$$F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n\nWhy harmonic mean? It penalizes extreme imbalances. If precision = 0.9 and recall = 0.1:\n- Arithmetic mean: (0.9 + 0.1) / 2 = 0.5\n- Harmonic mean: 2 \u00d7 0.9 \u00d7 0.1 / (0.9 + 0.1) = 0.18\n\nF1 is only high when **both** precision and recall are reasonable."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classification-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete classification report\n",
    "print(\"\\nComplete Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Fail', 'Pass']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roc-intro",
   "metadata": {},
   "source": "```{index} ROC curve, AUC\n```\n\n\n## ROC Curves and AUC\n\nSo far, we've used a threshold of 0.5: predict class 1 if P(y=1) > 0.5.\n\nBut this threshold is adjustable! Lower threshold \u2192 higher recall, lower precision.\n\nThe **ROC curve** (Receiver Operating Characteristic) shows the tradeoff across all thresholds:\n- X-axis: False Positive Rate = FP / (FP + TN)\n- Y-axis: True Positive Rate = TP / (TP + FN) = Recall\n\n**AUC** (Area Under the Curve) summarizes model performance:\n- AUC = 1.0: Perfect classifier\n- AUC = 0.5: Random guessing (diagonal line)\n- AUC > 0.8: Generally considered good"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roc-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# ROC curve\n",
    "axes[0].plot(fpr, tpr, 'b-', linewidth=2, label=f'Logistic Regression (AUC = {auc:.3f})')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.5)')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate (Recall)')\n",
    "axes[0].set_title('ROC Curve')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].set_xlim(-0.02, 1.02)\n",
    "axes[0].set_ylim(-0.02, 1.02)\n",
    "\n",
    "# Threshold selection\n",
    "axes[1].plot(thresholds, fpr, 'r-', label='False Positive Rate', linewidth=2)\n",
    "axes[1].plot(thresholds, tpr, 'b-', label='True Positive Rate', linewidth=2)\n",
    "axes[1].axvline(0.5, color='gray', linestyle='--', alpha=0.5, label='Default threshold')\n",
    "axes[1].set_xlabel('Threshold')\n",
    "axes[1].set_ylabel('Rate')\n",
    "axes[1].set_title('Effect of Threshold on Rates')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threshold-selection",
   "metadata": {},
   "source": [
    "### Choosing a Threshold\n",
    "\n",
    "The default 0.5 threshold isn't always optimal:\n",
    "\n",
    "- **Safety-critical applications**: Lower threshold (catch more positives, accept more false alarms)\n",
    "- **Cost-sensitive applications**: Adjust based on the cost of each error type\n",
    "\n",
    "Example: If a false negative (missing a fault) costs \\$100,000 and a false positive (false alarm) costs \\$1,000, you want to lower the threshold to catch more faults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threshold-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different thresholds\n",
    "thresholds_to_try = [0.3, 0.5, 0.7]\n",
    "\n",
    "print(\"Effect of threshold on metrics:\")\n",
    "print(f\"{'Threshold':<12} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1':<10}\")\n",
    "print(\"-\" * 52)\n",
    "\n",
    "for thresh in thresholds_to_try:\n",
    "    y_pred_thresh = (y_prob >= thresh).astype(int)\n",
    "    acc = accuracy_score(y_test, y_pred_thresh)\n",
    "    prec = precision_score(y_test, y_pred_thresh, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred_thresh)\n",
    "    f1_thresh = f1_score(y_test, y_pred_thresh)\n",
    "    print(f\"{thresh:<12.1f} {acc:<10.3f} {prec:<10.3f} {rec:<10.3f} {f1_thresh:<10.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiclass-intro",
   "metadata": {},
   "source": [
    "## Multi-Class Classification\n",
    "\n",
    "Many problems have more than two classes:\n",
    "- Classify material phase: solid, liquid, gas\n",
    "- Identify catalyst type: Pt, Pd, Ni, Cu\n",
    "- Determine operating regime: startup, steady-state, shutdown, fault\n",
    "\n",
    "Logistic regression extends to multi-class via:\n",
    "- **One-vs-Rest (OvR)**: Train K binary classifiers, each separating one class from the rest\n",
    "- **Multinomial**: Directly model probabilities for all K classes (softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiclass-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-class dataset: classifying reactor operating regimes\n",
    "# 0 = Normal, 1 = High Temperature, 2 = High Pressure, 3 = Fault\n",
    "\n",
    "X_multi, y_multi = make_classification(\n",
    "    n_samples=500, n_features=4, n_informative=3, n_redundant=1,\n",
    "    n_classes=4, n_clusters_per_class=1, random_state=42\n",
    ")\n",
    "\n",
    "regime_names = ['Normal', 'High Temp', 'High Pressure', 'Fault']\n",
    "print(\"Class distribution:\")\n",
    "unique, counts = np.unique(y_multi, return_counts=True)\n",
    "for cls, cnt in zip(unique, counts):\n",
    "    print(f\"  {regime_names[cls]}: {cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiclass-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multi-class logistic regression\n",
    "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(\n",
    "    X_multi, y_multi, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler_m = StandardScaler()\n",
    "X_train_m_scaled = scaler_m.fit_transform(X_train_m)\n",
    "X_test_m_scaled = scaler_m.transform(X_test_m)\n",
    "\n",
    "clf_multi = LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "clf_multi.fit(X_train_m_scaled, y_train_m)\n",
    "\n",
    "y_pred_m = clf_multi.predict(X_test_m_scaled)\n",
    "\n",
    "print(\"Multi-class Classification Report:\")\n",
    "print(classification_report(y_test_m, y_pred_m, target_names=regime_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiclass-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-class confusion matrix\n",
    "cm_multi = confusion_matrix(y_test_m, y_pred_m)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(cm_multi, display_labels=regime_names)\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "ax.set_title('Multi-class Confusion Matrix: Reactor Operating Regimes')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imbalanced-intro",
   "metadata": {},
   "source": "```{index} imbalanced classes, class weight\n```\n\n\n## Handling Imbalanced Classes\n\nIn many real-world problems, classes are imbalanced:\n- Fault detection: 99% normal, 1% fault\n- Quality control: 95% pass, 5% fail\n- Rare event prediction: 99.9% non-events\n\nStandard classifiers tend to ignore the minority class. Solutions:\n\n| Approach | Description | When to Use |\n|----------|-------------|-------------|\n| Class weights | Penalize errors on minority class more | Simple, often effective |\n| Oversampling | Duplicate minority samples (e.g., SMOTE) | Small datasets |\n| Undersampling | Remove majority samples | Very large datasets |\n| Threshold adjustment | Lower threshold for minority class | When you control deployment |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imbalanced-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create imbalanced dataset (90% normal, 10% fault)\n",
    "X_imb, y_imb = make_classification(\n",
    "    n_samples=1000, n_features=4, n_informative=3, n_redundant=1,\n",
    "    n_classes=2, weights=[0.9, 0.1], random_state=42\n",
    ")\n",
    "\n",
    "print(\"Imbalanced class distribution:\")\n",
    "print(f\"  Normal: {(y_imb == 0).sum()}\")\n",
    "print(f\"  Fault:  {(y_imb == 1).sum()}\")\n",
    "print(f\"  Fault rate: {y_imb.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imbalanced-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare balanced vs unbalanced classifiers\n",
    "X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(\n",
    "    X_imb, y_imb, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standard classifier\n",
    "clf_standard = LogisticRegression()\n",
    "clf_standard.fit(X_train_i, y_train_i)\n",
    "y_pred_standard = clf_standard.predict(X_test_i)\n",
    "\n",
    "# Balanced classifier (class_weight='balanced')\n",
    "clf_balanced = LogisticRegression(class_weight='balanced')\n",
    "clf_balanced.fit(X_train_i, y_train_i)\n",
    "y_pred_balanced = clf_balanced.predict(X_test_i)\n",
    "\n",
    "print(\"Standard Logistic Regression:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_test_i, y_pred_standard):.3f}\")\n",
    "print(f\"  Recall (Fault): {recall_score(y_test_i, y_pred_standard):.3f}\")\n",
    "print(f\"  F1 (Fault): {f1_score(y_test_i, y_pred_standard):.3f}\")\n",
    "\n",
    "print(\"\\nBalanced Logistic Regression:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_test_i, y_pred_balanced):.3f}\")\n",
    "print(f\"  Recall (Fault): {recall_score(y_test_i, y_pred_balanced):.3f}\")\n",
    "print(f\"  F1 (Fault): {f1_score(y_test_i, y_pred_balanced):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s63hxn04yjf",
   "source": "**The balanced classifier tradeoff is clear:**\n\n| Metric | Standard | Balanced |\n|--------|----------|----------|\n| Accuracy | Higher | Lower |\n| Recall (Faults) | ~50% | ~80% |\n| F1 (Faults) | Lower | Higher |\n\nThe standard classifier optimizes for overall accuracy, essentially ignoring the rare fault class. The balanced classifier sacrifices some overall accuracy to catch more faults.\n\n**What \"balanced\" does**: It upweights errors on the minority class. Mathematically, it's like replicating minority samples until classes are equal. The result: the model pays more attention to the rare class.\n\n**The 90% vs 95% accuracy paradox**: In imbalanced data, a model with 90% accuracy that catches 80% of faults is often *more useful* than a model with 95% accuracy that catches only 50% of faults. Always look beyond accuracy!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "imbalanced-lesson",
   "metadata": {},
   "source": [
    "### Key Insight\n",
    "\n",
    "The balanced classifier has lower accuracy but **much higher recall** for faults. In fault detection, catching faults is more important than overall accuracy!\n",
    "\n",
    "Always ask: \"What is the cost of each type of error?\" Let that guide your metric choice and class weighting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choosing-metrics",
   "metadata": {},
   "source": [
    "## Choosing the Right Metric\n",
    "\n",
    "| Scenario | Recommended Metric | Reasoning |\n",
    "|----------|-------------------|------------|\n",
    "| Balanced classes, equal error costs | Accuracy or F1 | Both give reasonable picture |\n",
    "| Imbalanced classes | F1 or AUC | Accuracy misleading |\n",
    "| Missing positives is costly | Recall | Prioritize catching all positives |\n",
    "| False alarms are costly | Precision | Prioritize being right when positive |\n",
    "| Ranking matters (not just yes/no) | AUC | Evaluates probability calibration |\n",
    "\n",
    "### A Decision Framework\n",
    "\n",
    "1. **Understand the costs**: What happens if you miss a positive? What happens if you false alarm?\n",
    "2. **Check class balance**: If imbalanced, avoid accuracy as primary metric\n",
    "3. **Consider the use case**: Are you making binary decisions or ranking candidates?\n",
    "4. **Report multiple metrics**: No single metric tells the whole story"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decision-guide",
   "metadata": {},
   "source": [
    "## Classification vs Regression: How to Choose\n",
    "\n",
    "Sometimes the boundary is fuzzy:\n",
    "\n",
    "| Problem | Natural Framing | Alternative |\n",
    "|---------|-----------------|-------------|\n",
    "| Product quality | Classification (pass/fail) | Regression (quality score) |\n",
    "| Equipment failure | Classification (fail/ok) | Regression (time to failure) |\n",
    "| Customer churn | Classification (leave/stay) | Regression (probability of leaving) |\n",
    "\n",
    "**Guidelines**:\n",
    "- If the outcome is naturally categorical, use classification\n",
    "- If you need probabilities, logistic regression gives you both\n",
    "- If there's a natural ordering or you care about degree, consider regression\n",
    "- You can always discretize regression outputs if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stl20000e8h",
   "source": "## Quiz\n\nTest your understanding of classification concepts.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "lc8nvv9cb2r",
   "source": "! pip install -q jupyterquiz\nfrom jupyterquiz import display_quiz\n\ndisplay_quiz(\"https://raw.githubusercontent.com/jkitchin/s26-06642/main/dsmles/07-classification/quizzes/classification-quiz.json\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ogkceuhw9",
   "source": "## Recommended Reading\n\nThese resources explore classification methods and evaluation metrics:\n\n1. **[Scikit-learn Classification Guide](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)** - Official documentation on logistic regression including multi-class strategies, regularization options, and solver selection.\n\n2. **[An Introduction to Statistical Learning, Chapter 4](https://www.statlearning.com/)** - Covers logistic regression, LDA, and classification concepts. Clear explanations of the math behind classification.\n\n3. **[The Precision-Recall Tradeoff (Google Developers)](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall)** - Interactive tutorial on classification metrics with visualizations of how threshold changes affect precision and recall.\n\n4. **[ROC Curves and AUC Explained](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)** - Clear explanation of ROC curves with interactive examples. Helps build intuition for what AUC actually measures.\n\n5. **[Learning from Imbalanced Data (He & Garcia, IEEE TKDE 2009)](https://ieeexplore.ieee.org/document/5128907)** - Survey paper on handling class imbalance. Covers sampling methods, cost-sensitive learning, and evaluation strategies for imbalanced datasets.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Classification predicts categories, not numbers**: Use when outcomes are discrete classes\n",
    "\n",
    "2. **Logistic regression is the workhorse**: Simple, interpretable, gives probabilities\n",
    "\n",
    "3. **Accuracy can be misleading**: Especially with imbalanced classes\n",
    "\n",
    "4. **Know your metrics**:\n",
    "   - Precision: How many predicted positives are correct?\n",
    "   - Recall: How many actual positives did we find?\n",
    "   - F1: Harmonic mean of precision and recall\n",
    "   - AUC: Overall ranking quality across all thresholds\n",
    "\n",
    "5. **The confusion matrix is your friend**: Visualizes all error types\n",
    "\n",
    "6. **Handle imbalanced classes**: Use class weights or adjust thresholds\n",
    "\n",
    "7. **Choose metrics based on costs**: What's the cost of missing a positive vs false alarm?\n",
    "\n",
    "### What's Next\n",
    "\n",
    "In the next module, we'll explore **regularization**\u2014techniques to prevent overfitting by constraining model complexity. These apply to both regression and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69zyekspxk",
   "source": "---\n\n## The Catalyst Crisis: Chapter 7 - \"Accuracy Isn't Everything\"\n\n*A story about classification metrics and real-world tradeoffs*\n\n---\n\n\"Ninety-four percent accuracy,\" Sam announced proudly. \"Our classifier can predict batch failures before they happen.\"\n\nThe team had pivoted from regression to classification\u2014instead of predicting exact yield, they were now predicting pass/fail. ChemCorp could use this to catch bad batches early, maybe even prevent them.\n\nFrank Morrison was on the video call, skeptical as always. \"Ninety-four percent sounds good. What's the catch?\"\n\nAlex had been digging through the confusion matrix. She found the catch.\n\n\"We're catching 62% of the failures,\" she said quietly.\n\nFrank frowned. \"Sixty-two? You said ninety-four.\"\n\n\"Ninety-four percent overall accuracy. But that's because most batches pass. When we predict 'pass,' we're usually right. But when a batch is actually going to fail, we only catch it 62% of the time.\"\n\nMaya pulled up the numbers. \"So about 40% of the bad batches slip through.\"\n\n\"That's not acceptable.\" Frank's voice was hard. \"A bad batch that ships costs us $200,000. I don't care about overall accuracy\u2014I care about catching failures.\"\n\nSam looked deflated. \"So our model is useless?\"\n\n\"No,\" Alex said. \"It's optimizing for the wrong thing.\" She turned to the screen. \"We can adjust the threshold. Accept more false alarms in exchange for catching more real failures. It's a trade-off.\"\n\nShe adjusted the classification threshold, watching the metrics shift. Recall\u2014the percentage of actual failures caught\u2014climbed to 89%. But precision dropped. More false alarms.\n\n\"So now we're stopping good batches unnecessarily?\" Frank asked.\n\n\"Some. But we're catching almost all the bad ones.\" Alex pulled up a cost analysis. \"False alarms cost you the time to investigate\u2014maybe $5,000 per batch. Missed failures cost you $200,000. What's the right trade-off?\"\n\nThe room was quiet. This was the reality of applied ML\u2014not just building models, but making decisions about what errors you could live with.\n\n\"Give me the sensitive version,\" Frank said finally. \"I'd rather investigate ten batches than ship one bad one.\"\n\nAfter the call, Jordan found Alex at the mystery board. \"That was uncomfortable.\"\n\n\"Real decisions usually are.\" She added a note: **Precision vs. recall trade-off. ChemCorp values catching failures over avoiding false alarms.**\n\n\"You handled Frank well.\"\n\nAlex shrugged. \"Seven years of dealing with operations managers. They don't want perfect\u2014they want useful.\"\n\n---\n\n*Continue to the next lecture to learn about regularization and model selection...*",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}